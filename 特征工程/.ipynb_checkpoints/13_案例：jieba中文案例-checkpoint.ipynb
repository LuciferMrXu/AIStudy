{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ibf\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.751 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "jieba.add_word('郑秋冬')\n",
    "jieba.add_word('离境天')\n",
    "jieba.suggest_freq('离境天', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#第一个文档分词#\n",
    "with open('./datas/nlp_1.txt',encoding='utf-8') as f:\n",
    "    document = f.read()\n",
    "    document_cut = jieba.cut(document)\n",
    "    #print  ' '.join(jieba_cut)\n",
    "    result = ' '.join(document_cut)\n",
    "    with open('./datas/nlp_1_cut.txt', 'w',encoding='utf-8') as f2:\n",
    "        f2.write(result)\n",
    "#第二个文档分词#\n",
    "with open('./datas/nlp_2.txt',encoding='utf-8') as f:\n",
    "    document = f.read()\n",
    "    document_cut = jieba.cut(document)\n",
    "    #print  ' '.join(jieba_cut)\n",
    "    result = ' '.join(document_cut)\n",
    "    with open('./datas/nlp_2_cut.txt', 'w',encoding='utf-8') as f2:\n",
    "        f2.write(result)\n",
    "#第三个文档分词#\n",
    "with open('./datas/nlp_3.txt',encoding='utf-8') as f:\n",
    "    document = f.read()\n",
    "    jieba.load_userdict(\"./datas/01mydict.txt\")\n",
    "    document_cut = jieba.cut(document)\n",
    "    #print  ' '.join(jieba_cut)\n",
    "    result = ' '.join(document_cut)\n",
    "    with open('./datas/nlp_3_cut.txt', 'w',encoding='utf-8') as f2:\n",
    "        f2.write(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('./datas/nlp_1_cut.txt',encoding='utf-8') as f3:\n",
    "    cut1 = f3.read()\n",
    "# print(res1)\n",
    "with open('./datas/nlp_2_cut.txt',encoding='utf-8') as f4:\n",
    "    cut2 = f4.read()\n",
    "# print(res2)\n",
    "with open('./datas/nlp_3_cut.txt',encoding='utf-8') as f5:\n",
    "    cut3 = f5.read()\n",
    "# print(res3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#从文件导入停用词表\n",
    "stpwrdpath = \"./datas/stop_words.txt\"\n",
    "stpwrd_dic = open(stpwrdpath, 'rb')\n",
    "stpwrd_content = stpwrd_dic.read()\n",
    "#将停用词表转换为list  \n",
    "stpwrdlst = stpwrd_content.splitlines()\n",
    "stpwrd_dic.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [1 1 2 ... 3 1 1]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "corpus = [cut1,cut2,cut3]\n",
    "CV = CountVectorizer(stop_words=stpwrdlst)\n",
    "CTF = CV.fit_transform(corpus)\n",
    "print(CTF.toarray())\n",
    "words=CV.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_topics=2,learning_method='batch',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "lda_result = lda.fit_transform(CTF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.57302175e-03 9.97426978e-01]\n",
      " [9.99099666e-01 9.00334145e-04]\n",
      " [1.73681106e-03 9.98263189e-01]]\n",
      "[[1.49996064 1.49996064 2.49995958 ... 3.49995936 1.49996064 1.49996064]\n",
      " [0.50003936 0.50003936 0.50004042 ... 0.50004064 0.50003936 0.50003936]]\n"
     ]
    }
   ],
   "source": [
    "print(lda_result)\n",
    "print(lda.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
