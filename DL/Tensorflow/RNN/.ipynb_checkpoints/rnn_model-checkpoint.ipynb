{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN_Model(object):\n",
    "\n",
    "    def __init__(self,config,is_training=True):\n",
    "        # drop保留\n",
    "        self.keep_prob=config.keep_prob\n",
    "        # 训练数据量\n",
    "        self.batch_size=tf.Variable(0,dtype=tf.int32,trainable=False)\n",
    "        # 数据步进\n",
    "        num_step=config.num_step\n",
    "        #输入数据\n",
    "        #input_shape=[batch_size,num_step]\n",
    "        self.input_data=tf.placeholder(tf.int32,[None,num_step])\n",
    "        #目标\n",
    "        self.target = tf.placeholder(tf.int64,[None])\n",
    "        #输出形状\n",
    "        self.mask_x = tf.placeholder(tf.float32,[num_step,None])\n",
    "        # 分类数\n",
    "        class_num=config.class_num\n",
    "        # 隐神经的大小\n",
    "        hidden_neural_size=config.hidden_neural_size\n",
    "        # 词汇量的大小\n",
    "        vocabulary_size=config.vocabulary_size\n",
    "        #嵌入形状\n",
    "        \n",
    "        embed_dim=config.embed_dim\n",
    "        # 隐层数量\n",
    "        hidden_layer_num=config.hidden_layer_num\n",
    "        # 新的数据维度\n",
    "        self.new_batch_size = tf.placeholder(tf.int32,shape=[],name=\"new_batch_size\")\n",
    "        # 数据维度更新\n",
    "        self._batch_size_update = tf.assign(self.batch_size,self.new_batch_size)\n",
    "\n",
    "        #build LSTM network\n",
    "        # 调用默认的LSTM\n",
    "        lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_neural_size,forget_bias=0.0,state_is_tuple=True)\n",
    "        if self.keep_prob<1:\n",
    "            lstm_cell =  tf.nn.rnn_cell.DropoutWrapper(\n",
    "                lstm_cell,output_keep_prob=self.keep_prob\n",
    "            )\n",
    "        #MultiRNNCell \n",
    "        #第一个参数RNN的实例形成的列表\n",
    "        #第二个参数细胞状态是否以元祖形式存储\n",
    "        cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell]*hidden_layer_num,state_is_tuple=True)\n",
    "\n",
    "        self._initial_state = cell.zero_state(self.batch_size,dtype=tf.float32)\n",
    "\n",
    "        #embedding layer\n",
    "        with tf.device(\"/cpu:0\"),tf.name_scope(\"embedding_layer\"):\n",
    "            embedding = tf.get_variable(\"embedding\",[vocabulary_size,embed_dim],dtype=tf.float32)\n",
    "            inputs=tf.nn.embedding_lookup(embedding,self.input_data)\n",
    "\n",
    "        if self.keep_prob<1:\n",
    "            inputs = tf.nn.dropout(inputs,self.keep_prob)\n",
    "\n",
    "        out_put=[]\n",
    "        state=self._initial_state\n",
    "        # 按照时间把当前层的RNN结构展开\n",
    "        with tf.variable_scope(\"LSTM_layer\"):\n",
    "            for time_step in range(num_step):\n",
    "                if time_step>0: tf.get_variable_scope().reuse_variables()\n",
    "                (cell_output,state)=cell(inputs[:,time_step,:],state)\n",
    "                out_put.append(cell_output)\n",
    "\n",
    "        out_put=out_put*self.mask_x[:,:,None]\n",
    "\n",
    "        with tf.name_scope(\"mean_pooling_layer\"):\n",
    "\n",
    "            out_put=tf.reduce_sum(out_put,0)/(tf.reduce_sum(self.mask_x,0)[:,None])\n",
    "\n",
    "        with tf.name_scope(\"Softmax_layer_and_output\"):\n",
    "            softmax_w = tf.get_variable(\"softmax_w\",[hidden_neural_size,class_num],dtype=tf.float32)\n",
    "            softmax_b = tf.get_variable(\"softmax_b\",[class_num],dtype=tf.float32)\n",
    "            self.logits = tf.matmul(out_put,softmax_w)+softmax_b\n",
    "\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            self.loss = tf.nn.sparse_softmax_cross_entropy_with_logits(self.logits+1e-10,self.target)\n",
    "            self.cost = tf.reduce_mean(self.loss)\n",
    "\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            self.prediction = tf.argmax(self.logits,1)\n",
    "            correct_prediction = tf.equal(self.prediction,self.target)\n",
    "            self.correct_num=tf.reduce_sum(tf.cast(correct_prediction,tf.float32))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32),name=\"accuracy\")\n",
    "\n",
    "        #add summary\n",
    "        loss_summary = tf.scalar_summary(\"loss\",self.cost)\n",
    "        #add summary\n",
    "        accuracy_summary=tf.scalar_summary(\"accuracy_summary\",self.accuracy)\n",
    "\n",
    "        if not is_training:\n",
    "            return\n",
    "\n",
    "        self.globle_step = tf.Variable(0,name=\"globle_step\",trainable=False)\n",
    "        self.lr = tf.Variable(0.0,trainable=False)\n",
    "\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars),\n",
    "                                      config.max_grad_norm)\n",
    "\n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in zip(grads, tvars):\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.histogram_summary(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.scalar_summary(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        self.grad_summaries_merged = tf.merge_summary(grad_summaries)\n",
    "\n",
    "        self.summary =tf.merge_summary([loss_summary,accuracy_summary,self.grad_summaries_merged])\n",
    "\n",
    "        optimizer = tf.train.GradientDescentOptimizer(self.lr)\n",
    "        optimizer.apply_gradients(zip(grads, tvars))\n",
    "        self.train_op=optimizer.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "        self.new_lr = tf.placeholder(tf.float32,shape=[],name=\"new_learning_rate\")\n",
    "        self._lr_update = tf.assign(self.lr,self.new_lr)\n",
    "\n",
    "    def assign_new_lr(self,session,lr_value):\n",
    "        session.run(self._lr_update,feed_dict={self.new_lr:lr_value})\n",
    "    def assign_new_batch_size(self,session,batch_size_value):\n",
    "        session.run(self._batch_size_update,feed_dict={self.new_batch_size:batch_size_value})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
