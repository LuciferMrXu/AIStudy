{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 数据生成及预处理\n",
    "def gen_data(size = 1000000):\n",
    "    \"\"\"生成数据\n",
    "    输入数据X：在时间t，Xt的值有50%的概率为1，50%的概率为0；\n",
    "    输出数据Y：在实践t，Yt的值有50%的概率为1，50%的概率为0，除此之外，如果`Xt-3 == 1`，Yt为1的概率增加50%， 如果`Xt-8 == 1`，则Yt为1的概率减少25%， 如果上述两个条件同时满足，则Yt为1的概率为75%。\n",
    "    \"\"\"\n",
    "    X = np.array(np.random.choice(2,size=(size,)))\n",
    "    Y = []\n",
    "    for i in range(size):\n",
    "        threshold = 0.5\n",
    "        if X[i-3] == 1:\n",
    "            threshold += 0.5\n",
    "        if X[i-8] == 1:\n",
    "            threshold -= 0.25\n",
    "        if np.random.rand() > threshold:\n",
    "            Y.append(0)\n",
    "        else:\n",
    "            Y.append(1)\n",
    "    return X,np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 将产生的数据集按照参数进行切分，主要参数是batch_size和num_steps，batch_size 指将数据分成多少块，而num_steps指输入rnn_cell中的窗口的大小，即下图中的n的大小\n",
    "def gen_batch(raw_data, batch_size, num_steps):\n",
    "    #raw_data是使用gen_data()函数生成的数据，分别是X和Y\n",
    "    raw_x, raw_y = raw_data\n",
    "    data_length = len(raw_x)\n",
    "\n",
    "    # 首先将数据切分成batch_size份，0-batch_size，batch_size-2*batch_size。。。\n",
    "    batch_partition_length = data_length // batch_size\n",
    "    data_x = np.zeros([batch_size, batch_partition_length], dtype=np.int32)\n",
    "    data_y = np.zeros([batch_size, batch_partition_length], dtype=np.int32)\n",
    "    for i in range(batch_size):\n",
    "        data_x[i] = raw_x[batch_partition_length * i:batch_partition_length * (i + 1)]\n",
    "        data_y[i] = raw_y[batch_partition_length * i:batch_partition_length * (i + 1)]\n",
    "\n",
    "    #因为RNN模型一次只处理num_steps个数据，所以将每个batch_size在进行切分成epoch_size份，每份num_steps个数据。\n",
    "    #注意这里的epoch_size和模型训练过程中的epoch不同。 \n",
    "    epoch_size = batch_partition_length // num_steps\n",
    "\n",
    "    #x是0-num_steps， batch_partition_length -batch_partition_length +num_steps。。。共batch_size个\n",
    "    for i in range(epoch_size):\n",
    "        x = data_x[:, i * num_steps:(i + 1) * num_steps]\n",
    "        y = data_y[:, i * num_steps:(i + 1) * num_steps]\n",
    "        yield (x, y)\n",
    "\n",
    "#这里的n就是训练过程中用的epoch，即在样本规模上循环的次数\n",
    "def gen_epochs(n, num_steps):\n",
    "    for i in range(n):\n",
    "        yield gen_batch(gen_data(), batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 模型构建\n",
    "batch_size = 3\n",
    "num_classes = 2\n",
    "state_size = 4\n",
    "num_steps = 10\n",
    "learning_rate = 0.2\n",
    "\n",
    "x = tf.placeholder(tf.int32, [batch_size, num_steps], name='input_placeholder')\n",
    "y = tf.placeholder(tf.int32, [batch_size, num_steps], name='labels_placeholder')\n",
    "#RNN的初始化状态，全设为零。注意state是与input保持一致，接下来会有concat操作，所以这里要有batch的维度。即每个样本都要有隐层状态\n",
    "init_state = tf.zeros([batch_size, state_size])\n",
    "\n",
    "#将输入转化为one-hot编码，两个类别。[batch_size, num_steps, num_classes]\n",
    "x_one_hot = tf.one_hot(x, num_classes)\n",
    "#将输入unstack，即在num_steps上解绑，方便给每个循环单元输入。\n",
    "#这里可以看出RNN每个cell都处理一个batch的输入（即batch个二进制样本输入）\n",
    "rnn_inputs = tf.unstack(x_one_hot, axis=1)\n",
    "\n",
    "#定义rnn_cell的权重参数，\n",
    "with tf.variable_scope('rnn_cell'):\n",
    "    \"\"\"由于tf.Variable() 每次都在创建新对象，所有reuse=True 和它并没有什么关系。对于get_variable()，来说，如果已经创建的变量对象，就把那个对象返回，如果没有创建变量对象的话，就创建一个新的。\"\"\"\n",
    "    W = tf.get_variable('W', [num_classes + state_size, state_size])\n",
    "    b = tf.get_variable('b', [state_size], initializer=tf.constant_initializer(0.0))\n",
    "#使之定义为reuse模式，循环使用，保持参数相同\n",
    "def rnn_cell(rnn_input, state):\n",
    "    with tf.variable_scope('rnn_cell', reuse=True):\n",
    "        W = tf.get_variable('W', [num_classes + state_size, state_size])\n",
    "        b = tf.get_variable('b', [state_size], initializer=tf.constant_initializer(0.0))\n",
    "    #定义rnn_cell具体的操作，这里使用的是最简单的rnn，不是LSTM\n",
    "    return tf.tanh(tf.matmul(tf.concat([rnn_input, state], 1), W) + b)\n",
    "\n",
    "state = init_state\n",
    "rnn_outputs = []\n",
    "#循环num_steps次，即将一个序列输入RNN模型\n",
    "for rnn_input in rnn_inputs:\n",
    "    state = rnn_cell(rnn_input, state)\n",
    "    rnn_outputs.append(state)\n",
    "final_state = rnn_outputs[-1]\n",
    "\n",
    "#定义softmax层\n",
    "with tf.variable_scope('softmax'):\n",
    "    W = tf.get_variable('W', [state_size, num_classes])\n",
    "    b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "#注意，这里要将num_steps个输出全部分别进行计算其输出，然后使用softmax预测\n",
    "logits = [tf.matmul(rnn_output, W) + b for rnn_output in rnn_outputs]\n",
    "predictions = [tf.nn.softmax(logit) for logit in logits]\n",
    "\n",
    "# Turn our y placeholder into a list of labels\n",
    "y_as_list = tf.unstack(y, num=num_steps, axis=1)\n",
    "\n",
    "#losses and train_step\n",
    "losses = [tf.nn.sparse_softmax_cross_entropy_with_logits(labels=label, logits=logit) for \\\n",
    "          logit, label in zip(logits, y_as_list)]\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 模型训练\n",
    "def train_network(num_epochs, num_steps, state_size=4, verbose=True):\n",
    "    if 'sess' in locals() and sess is not None:\n",
    "        print('Close interactive session')\n",
    "        sess.close()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        training_losses = []\n",
    "        #得到数据，因为num_epochs==5，所以外循环只执行五次\n",
    "        for idx, epoch in enumerate(gen_epochs(num_epochs, num_steps)):\n",
    "            training_loss = 0\n",
    "            #保存每次执行后的最后状态，然后赋给下一次执行\n",
    "            training_state = np.zeros((batch_size, state_size))\n",
    "            if verbose:\n",
    "                print(\"\\nEPOCH\", idx)\n",
    "            #这是具体获得数据的部分\n",
    "            for step, (X, Y) in enumerate(epoch):\n",
    "                tr_losses, training_loss_, training_state, _ = \\\n",
    "                    sess.run([losses,\n",
    "                              total_loss,\n",
    "                              final_state,\n",
    "                              train_step],\n",
    "                                  feed_dict={x:X, y:Y, init_state:training_state})\n",
    "                training_loss += training_loss_\n",
    "                if step % 100 == 0 and step > 0:\n",
    "                    if verbose:\n",
    "                        print(\"Average loss at step\", step,\n",
    "                              \"for last 100 steps:\", training_loss/100)\n",
    "                    training_losses.append(training_loss/100)\n",
    "                    training_loss = 0\n",
    "    return training_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 0\n",
      "Average loss at step 100 for last 100 steps: 0.574424563646\n",
      "Average loss at step 200 for last 100 steps: 0.520560624897\n",
      "Average loss at step 300 for last 100 steps: 0.542779410481\n",
      "Average loss at step 400 for last 100 steps: 0.533602103591\n",
      "Average loss at step 500 for last 100 steps: 0.518892859519\n",
      "Average loss at step 600 for last 100 steps: 0.513623861969\n",
      "Average loss at step 700 for last 100 steps: 0.517258970141\n",
      "Average loss at step 800 for last 100 steps: 0.533239878118\n",
      "Average loss at step 900 for last 100 steps: 0.521015333235\n",
      "Average loss at step 1000 for last 100 steps: 0.526021636128\n",
      "Average loss at step 1100 for last 100 steps: 0.519838831425\n",
      "Average loss at step 1200 for last 100 steps: 0.528989198506\n",
      "Average loss at step 1300 for last 100 steps: 0.506580399871\n",
      "Average loss at step 1400 for last 100 steps: 0.531964007318\n",
      "Average loss at step 1500 for last 100 steps: 0.513062051237\n",
      "Average loss at step 1600 for last 100 steps: 0.524740172327\n",
      "Average loss at step 1700 for last 100 steps: 0.521755679548\n",
      "Average loss at step 1800 for last 100 steps: 0.529212580323\n",
      "Average loss at step 1900 for last 100 steps: 0.528630821109\n",
      "Average loss at step 2000 for last 100 steps: 0.519217717648\n",
      "Average loss at step 2100 for last 100 steps: 0.541419133246\n",
      "Average loss at step 2200 for last 100 steps: 0.531329222918\n",
      "Average loss at step 2300 for last 100 steps: 0.512081038058\n",
      "Average loss at step 2400 for last 100 steps: 0.524127689004\n",
      "Average loss at step 2500 for last 100 steps: 0.50705065459\n",
      "Average loss at step 2600 for last 100 steps: 0.517761597633\n",
      "Average loss at step 2700 for last 100 steps: 0.523870942593\n",
      "Average loss at step 2800 for last 100 steps: 0.517031112611\n",
      "Average loss at step 2900 for last 100 steps: 0.513632456958\n",
      "Average loss at step 3000 for last 100 steps: 0.529995363355\n",
      "Average loss at step 3100 for last 100 steps: 0.524043317735\n",
      "Average loss at step 3200 for last 100 steps: 0.530343750715\n",
      "Average loss at step 3300 for last 100 steps: 0.493315313756\n",
      "Average loss at step 3400 for last 100 steps: 0.522120725513\n",
      "Average loss at step 3500 for last 100 steps: 0.511718851328\n",
      "Average loss at step 3600 for last 100 steps: 0.518335148692\n",
      "Average loss at step 3700 for last 100 steps: 0.524862095416\n",
      "Average loss at step 3800 for last 100 steps: 0.526337237358\n",
      "Average loss at step 3900 for last 100 steps: 0.521366126537\n",
      "Average loss at step 4000 for last 100 steps: 0.505287994146\n",
      "Average loss at step 4100 for last 100 steps: 0.511427875459\n",
      "Average loss at step 4200 for last 100 steps: 0.515011733174\n",
      "Average loss at step 4300 for last 100 steps: 0.515202144086\n",
      "Average loss at step 4400 for last 100 steps: 0.522575148642\n",
      "Average loss at step 4500 for last 100 steps: 0.515792147815\n",
      "Average loss at step 4600 for last 100 steps: 0.527870099247\n",
      "Average loss at step 4700 for last 100 steps: 0.513538845181\n",
      "Average loss at step 4800 for last 100 steps: 0.520446359515\n",
      "Average loss at step 4900 for last 100 steps: 0.507210764885\n",
      "Average loss at step 5000 for last 100 steps: 0.523858231306\n",
      "Average loss at step 5100 for last 100 steps: 0.513686310947\n",
      "Average loss at step 5200 for last 100 steps: 0.52098308295\n",
      "Average loss at step 5300 for last 100 steps: 0.529793902636\n",
      "Average loss at step 5400 for last 100 steps: 0.515719545782\n",
      "Average loss at step 5500 for last 100 steps: 0.529271179438\n",
      "Average loss at step 5600 for last 100 steps: 0.509466073811\n",
      "Average loss at step 5700 for last 100 steps: 0.530133807659\n",
      "Average loss at step 5800 for last 100 steps: 0.522749283016\n",
      "Average loss at step 5900 for last 100 steps: 0.520447562933\n",
      "Average loss at step 6000 for last 100 steps: 0.526686914861\n",
      "Average loss at step 6100 for last 100 steps: 0.523494848609\n",
      "Average loss at step 6200 for last 100 steps: 0.516080173254\n",
      "Average loss at step 6300 for last 100 steps: 0.518586204052\n",
      "Average loss at step 6400 for last 100 steps: 0.519148472548\n",
      "Average loss at step 6500 for last 100 steps: 0.500053819418\n",
      "Average loss at step 6600 for last 100 steps: 0.510464675128\n",
      "Average loss at step 6700 for last 100 steps: 0.53607884109\n",
      "Average loss at step 6800 for last 100 steps: 0.507926214337\n",
      "Average loss at step 6900 for last 100 steps: 0.530854372978\n",
      "Average loss at step 7000 for last 100 steps: 0.518488392234\n",
      "Average loss at step 7100 for last 100 steps: 0.527386489213\n",
      "Average loss at step 7200 for last 100 steps: 0.515897332132\n",
      "Average loss at step 7300 for last 100 steps: 0.529333388507\n",
      "Average loss at step 7400 for last 100 steps: 0.529418787956\n",
      "Average loss at step 7500 for last 100 steps: 0.510745155811\n",
      "Average loss at step 7600 for last 100 steps: 0.520114175677\n",
      "Average loss at step 7700 for last 100 steps: 0.514796284735\n",
      "Average loss at step 7800 for last 100 steps: 0.515854305029\n",
      "Average loss at step 7900 for last 100 steps: 0.517986344993\n",
      "Average loss at step 8000 for last 100 steps: 0.5138857463\n",
      "Average loss at step 8100 for last 100 steps: 0.514518973827\n",
      "Average loss at step 8200 for last 100 steps: 0.52470639497\n",
      "Average loss at step 8300 for last 100 steps: 0.522177482545\n",
      "Average loss at step 8400 for last 100 steps: 0.521670609117\n",
      "Average loss at step 8500 for last 100 steps: 0.517619910538\n",
      "Average loss at step 8600 for last 100 steps: 0.522484621704\n",
      "Average loss at step 8700 for last 100 steps: 0.538611094654\n",
      "Average loss at step 8800 for last 100 steps: 0.511734151244\n",
      "Average loss at step 8900 for last 100 steps: 0.517615251839\n",
      "Average loss at step 9000 for last 100 steps: 0.519627016187\n",
      "Average loss at step 9100 for last 100 steps: 0.503052100539\n",
      "Average loss at step 9200 for last 100 steps: 0.526017234027\n",
      "Average loss at step 9300 for last 100 steps: 0.510933578908\n",
      "Average loss at step 9400 for last 100 steps: 0.526303888559\n",
      "Average loss at step 9500 for last 100 steps: 0.523599983454\n",
      "Average loss at step 9600 for last 100 steps: 0.524743380249\n",
      "Average loss at step 9700 for last 100 steps: 0.529420283735\n",
      "Average loss at step 9800 for last 100 steps: 0.524476201534\n",
      "Average loss at step 9900 for last 100 steps: 0.529530751407\n",
      "Average loss at step 10000 for last 100 steps: 0.520236704946\n",
      "Average loss at step 10100 for last 100 steps: 0.50505030334\n",
      "Average loss at step 10200 for last 100 steps: 0.52138774693\n",
      "Average loss at step 10300 for last 100 steps: 0.511781124771\n",
      "Average loss at step 10400 for last 100 steps: 0.519125158191\n",
      "Average loss at step 10500 for last 100 steps: 0.499353895783\n",
      "Average loss at step 10600 for last 100 steps: 0.5225798437\n",
      "Average loss at step 10700 for last 100 steps: 0.509927668571\n",
      "Average loss at step 10800 for last 100 steps: 0.528118314147\n",
      "Average loss at step 10900 for last 100 steps: 0.507264800072\n",
      "Average loss at step 11000 for last 100 steps: 0.523849752247\n",
      "Average loss at step 11100 for last 100 steps: 0.513764604628\n",
      "Average loss at step 11200 for last 100 steps: 0.512055864632\n",
      "Average loss at step 11300 for last 100 steps: 0.508582169414\n",
      "Average loss at step 11400 for last 100 steps: 0.528380000293\n",
      "Average loss at step 11500 for last 100 steps: 0.520974521041\n",
      "Average loss at step 11600 for last 100 steps: 0.501288483143\n",
      "Average loss at step 11700 for last 100 steps: 0.512499553561\n",
      "Average loss at step 11800 for last 100 steps: 0.526344366074\n",
      "Average loss at step 11900 for last 100 steps: 0.533457198441\n",
      "Average loss at step 12000 for last 100 steps: 0.509509578645\n",
      "Average loss at step 12100 for last 100 steps: 0.527598085403\n",
      "Average loss at step 12200 for last 100 steps: 0.529756416082\n",
      "Average loss at step 12300 for last 100 steps: 0.521570795774\n",
      "Average loss at step 12400 for last 100 steps: 0.519951191247\n",
      "Average loss at step 12500 for last 100 steps: 0.515111069679\n",
      "Average loss at step 12600 for last 100 steps: 0.531562457085\n",
      "Average loss at step 12700 for last 100 steps: 0.50365311414\n",
      "Average loss at step 12800 for last 100 steps: 0.530863189399\n",
      "Average loss at step 12900 for last 100 steps: 0.499966683388\n",
      "Average loss at step 13000 for last 100 steps: 0.521679325998\n",
      "Average loss at step 13100 for last 100 steps: 0.528055019379\n",
      "Average loss at step 13200 for last 100 steps: 0.506575901955\n",
      "Average loss at step 13300 for last 100 steps: 0.505536834002\n",
      "Average loss at step 13400 for last 100 steps: 0.527898829877\n",
      "Average loss at step 13500 for last 100 steps: 0.525189377964\n",
      "Average loss at step 13600 for last 100 steps: 0.515100529194\n",
      "Average loss at step 13700 for last 100 steps: 0.526974549294\n",
      "Average loss at step 13800 for last 100 steps: 0.530206235349\n",
      "Average loss at step 13900 for last 100 steps: 0.524732197523\n",
      "Average loss at step 14000 for last 100 steps: 0.529577250481\n",
      "Average loss at step 14100 for last 100 steps: 0.505862572789\n",
      "Average loss at step 14200 for last 100 steps: 0.518618044257\n",
      "Average loss at step 14300 for last 100 steps: 0.532083645165\n",
      "Average loss at step 14400 for last 100 steps: 0.521406024396\n",
      "Average loss at step 14500 for last 100 steps: 0.503697510064\n",
      "Average loss at step 14600 for last 100 steps: 0.530407799482\n",
      "Average loss at step 14700 for last 100 steps: 0.535677894652\n",
      "Average loss at step 14800 for last 100 steps: 0.525390827656\n",
      "Average loss at step 14900 for last 100 steps: 0.524954444468\n",
      "Average loss at step 15000 for last 100 steps: 0.53164788872\n",
      "Average loss at step 15100 for last 100 steps: 0.527697951198\n",
      "Average loss at step 15200 for last 100 steps: 0.514287663698\n",
      "Average loss at step 15300 for last 100 steps: 0.511719717383\n",
      "Average loss at step 15400 for last 100 steps: 0.527587333322\n",
      "Average loss at step 15500 for last 100 steps: 0.513054479957\n",
      "Average loss at step 15600 for last 100 steps: 0.53748416841\n",
      "Average loss at step 15700 for last 100 steps: 0.511600590646\n",
      "Average loss at step 15800 for last 100 steps: 0.52491181761\n",
      "Average loss at step 15900 for last 100 steps: 0.52405829221\n",
      "Average loss at step 16000 for last 100 steps: 0.520583240092\n",
      "Average loss at step 16100 for last 100 steps: 0.522862729728\n",
      "Average loss at step 16200 for last 100 steps: 0.515807005763\n",
      "Average loss at step 16300 for last 100 steps: 0.509848623872\n",
      "Average loss at step 16400 for last 100 steps: 0.505810504556\n",
      "Average loss at step 16500 for last 100 steps: 0.519663267732\n",
      "Average loss at step 16600 for last 100 steps: 0.524576759934\n",
      "Average loss at step 16700 for last 100 steps: 0.532401198745\n",
      "Average loss at step 16800 for last 100 steps: 0.508222613633\n",
      "Average loss at step 16900 for last 100 steps: 0.524689022303\n",
      "Average loss at step 17000 for last 100 steps: 0.504430532455\n",
      "Average loss at step 17100 for last 100 steps: 0.517665313482\n",
      "Average loss at step 17200 for last 100 steps: 0.515463372469\n",
      "Average loss at step 17300 for last 100 steps: 0.500242031813\n",
      "Average loss at step 17400 for last 100 steps: 0.533044342995\n",
      "Average loss at step 17500 for last 100 steps: 0.507917639315\n",
      "Average loss at step 17600 for last 100 steps: 0.528142237365\n",
      "Average loss at step 17700 for last 100 steps: 0.537772873342\n",
      "Average loss at step 17800 for last 100 steps: 0.514039815366\n",
      "Average loss at step 17900 for last 100 steps: 0.515598740876\n",
      "Average loss at step 18000 for last 100 steps: 0.509958392382\n",
      "Average loss at step 18100 for last 100 steps: 0.514428921938\n",
      "Average loss at step 18200 for last 100 steps: 0.518213971853\n",
      "Average loss at step 18300 for last 100 steps: 0.511402462721\n",
      "Average loss at step 18400 for last 100 steps: 0.527208028436\n",
      "Average loss at step 18500 for last 100 steps: 0.518480306268\n",
      "Average loss at step 18600 for last 100 steps: 0.522548574507\n",
      "Average loss at step 18700 for last 100 steps: 0.511948714852\n",
      "Average loss at step 18800 for last 100 steps: 0.521004535854\n",
      "Average loss at step 18900 for last 100 steps: 0.510741634071\n",
      "Average loss at step 19000 for last 100 steps: 0.52139230758\n",
      "Average loss at step 19100 for last 100 steps: 0.522912293077\n",
      "Average loss at step 19200 for last 100 steps: 0.528421873748\n",
      "Average loss at step 19300 for last 100 steps: 0.534242864251\n",
      "Average loss at step 19400 for last 100 steps: 0.528335458934\n",
      "Average loss at step 19500 for last 100 steps: 0.532107786536\n",
      "Average loss at step 19600 for last 100 steps: 0.509143587053\n",
      "Average loss at step 19700 for last 100 steps: 0.515985170007\n",
      "Average loss at step 19800 for last 100 steps: 0.509636595249\n",
      "Average loss at step 19900 for last 100 steps: 0.518669986129\n",
      "Average loss at step 20000 for last 100 steps: 0.521655047238\n",
      "Average loss at step 20100 for last 100 steps: 0.534665772319\n",
      "Average loss at step 20200 for last 100 steps: 0.51943868041\n",
      "Average loss at step 20300 for last 100 steps: 0.518905113935\n",
      "Average loss at step 20400 for last 100 steps: 0.511532928646\n",
      "Average loss at step 20500 for last 100 steps: 0.5196285218\n",
      "Average loss at step 20600 for last 100 steps: 0.522766260207\n",
      "Average loss at step 20700 for last 100 steps: 0.510303645432\n",
      "Average loss at step 20800 for last 100 steps: 0.522656781971\n",
      "Average loss at step 20900 for last 100 steps: 0.50659153372\n",
      "Average loss at step 21000 for last 100 steps: 0.521157745123\n",
      "Average loss at step 21100 for last 100 steps: 0.512534170747\n",
      "Average loss at step 21200 for last 100 steps: 0.517881108522\n",
      "Average loss at step 21300 for last 100 steps: 0.528460534215\n",
      "Average loss at step 21400 for last 100 steps: 0.515370035768\n",
      "Average loss at step 21500 for last 100 steps: 0.500335801542\n",
      "Average loss at step 21600 for last 100 steps: 0.538306490779\n",
      "Average loss at step 21700 for last 100 steps: 0.521190686226\n",
      "Average loss at step 21800 for last 100 steps: 0.50749166131\n",
      "Average loss at step 21900 for last 100 steps: 0.514829838276\n",
      "Average loss at step 22000 for last 100 steps: 0.529884134829\n",
      "Average loss at step 22100 for last 100 steps: 0.512841089666\n",
      "Average loss at step 22200 for last 100 steps: 0.522232601941\n",
      "Average loss at step 22300 for last 100 steps: 0.515628778636\n",
      "Average loss at step 22400 for last 100 steps: 0.519170485735\n",
      "Average loss at step 22500 for last 100 steps: 0.531080182195\n",
      "Average loss at step 22600 for last 100 steps: 0.518211520016\n",
      "Average loss at step 22700 for last 100 steps: 0.521208171248\n",
      "Average loss at step 22800 for last 100 steps: 0.525051833689\n",
      "Average loss at step 22900 for last 100 steps: 0.530582505763\n",
      "Average loss at step 23000 for last 100 steps: 0.507542694211\n",
      "Average loss at step 23100 for last 100 steps: 0.516190561652\n",
      "Average loss at step 23200 for last 100 steps: 0.522267296314\n",
      "Average loss at step 23300 for last 100 steps: 0.511397745907\n",
      "Average loss at step 23400 for last 100 steps: 0.524573542476\n",
      "Average loss at step 23500 for last 100 steps: 0.511115147769\n",
      "Average loss at step 23600 for last 100 steps: 0.516887193918\n",
      "Average loss at step 23700 for last 100 steps: 0.526710490882\n",
      "Average loss at step 23800 for last 100 steps: 0.519657648802\n",
      "Average loss at step 23900 for last 100 steps: 0.514242289364\n",
      "Average loss at step 24000 for last 100 steps: 0.514022113681\n",
      "Average loss at step 24100 for last 100 steps: 0.532263324261\n",
      "Average loss at step 24200 for last 100 steps: 0.514949330389\n",
      "Average loss at step 24300 for last 100 steps: 0.536299642622\n",
      "Average loss at step 24400 for last 100 steps: 0.528802322745\n",
      "Average loss at step 24500 for last 100 steps: 0.517302495837\n",
      "Average loss at step 24600 for last 100 steps: 0.521187262833\n",
      "Average loss at step 24700 for last 100 steps: 0.527198708951\n",
      "Average loss at step 24800 for last 100 steps: 0.517982706726\n",
      "Average loss at step 24900 for last 100 steps: 0.517217667401\n",
      "Average loss at step 25000 for last 100 steps: 0.525748590827\n",
      "Average loss at step 25100 for last 100 steps: 0.506614377201\n",
      "Average loss at step 25200 for last 100 steps: 0.53217173636\n",
      "Average loss at step 25300 for last 100 steps: 0.512366989851\n",
      "Average loss at step 25400 for last 100 steps: 0.539956172407\n",
      "Average loss at step 25500 for last 100 steps: 0.536340573728\n",
      "Average loss at step 25600 for last 100 steps: 0.502883245051\n",
      "Average loss at step 25700 for last 100 steps: 0.512288019955\n",
      "Average loss at step 25800 for last 100 steps: 0.515266030431\n",
      "Average loss at step 25900 for last 100 steps: 0.516841392517\n",
      "Average loss at step 26000 for last 100 steps: 0.518498029113\n",
      "Average loss at step 26100 for last 100 steps: 0.52601169616\n",
      "Average loss at step 26200 for last 100 steps: 0.520497390032\n",
      "Average loss at step 26300 for last 100 steps: 0.532559582889\n",
      "Average loss at step 26400 for last 100 steps: 0.512202418149\n",
      "Average loss at step 26500 for last 100 steps: 0.521177170575\n",
      "Average loss at step 26600 for last 100 steps: 0.517464219034\n",
      "Average loss at step 26700 for last 100 steps: 0.527494655848\n",
      "Average loss at step 26800 for last 100 steps: 0.522097054422\n",
      "Average loss at step 26900 for last 100 steps: 0.515704089403\n",
      "Average loss at step 27000 for last 100 steps: 0.519483994246\n",
      "Average loss at step 27100 for last 100 steps: 0.520724616349\n",
      "Average loss at step 27200 for last 100 steps: 0.51974768877\n",
      "Average loss at step 27300 for last 100 steps: 0.531393710971\n",
      "Average loss at step 27400 for last 100 steps: 0.533412724435\n",
      "Average loss at step 27500 for last 100 steps: 0.513449122012\n",
      "Average loss at step 27600 for last 100 steps: 0.523226747811\n",
      "Average loss at step 27700 for last 100 steps: 0.520438675582\n",
      "Average loss at step 27800 for last 100 steps: 0.51538762182\n",
      "Average loss at step 27900 for last 100 steps: 0.522435675859\n",
      "Average loss at step 28000 for last 100 steps: 0.504488230497\n",
      "Average loss at step 28100 for last 100 steps: 0.522173754275\n",
      "Average loss at step 28200 for last 100 steps: 0.507230483294\n",
      "Average loss at step 28300 for last 100 steps: 0.520544211566\n",
      "Average loss at step 28400 for last 100 steps: 0.521693178117\n",
      "Average loss at step 28500 for last 100 steps: 0.52740749985\n",
      "Average loss at step 28600 for last 100 steps: 0.509878916442\n",
      "Average loss at step 28700 for last 100 steps: 0.522880052328\n",
      "Average loss at step 28800 for last 100 steps: 0.520230617821\n",
      "Average loss at step 28900 for last 100 steps: 0.510590506196\n",
      "Average loss at step 29000 for last 100 steps: 0.534580805004\n",
      "Average loss at step 29100 for last 100 steps: 0.530665419102\n",
      "Average loss at step 29200 for last 100 steps: 0.51382609278\n",
      "Average loss at step 29300 for last 100 steps: 0.53274433732\n",
      "Average loss at step 29400 for last 100 steps: 0.516257386208\n",
      "Average loss at step 29500 for last 100 steps: 0.504259163439\n",
      "Average loss at step 29600 for last 100 steps: 0.512965694964\n",
      "Average loss at step 29700 for last 100 steps: 0.530131569803\n",
      "Average loss at step 29800 for last 100 steps: 0.520489853621\n",
      "Average loss at step 29900 for last 100 steps: 0.517029942274\n",
      "Average loss at step 30000 for last 100 steps: 0.521317492127\n",
      "Average loss at step 30100 for last 100 steps: 0.511638910472\n",
      "Average loss at step 30200 for last 100 steps: 0.540088343918\n",
      "Average loss at step 30300 for last 100 steps: 0.531514538229\n",
      "Average loss at step 30400 for last 100 steps: 0.514621840715\n",
      "Average loss at step 30500 for last 100 steps: 0.518775105476\n",
      "Average loss at step 30600 for last 100 steps: 0.525352163911\n",
      "Average loss at step 30700 for last 100 steps: 0.505469529629\n",
      "Average loss at step 30800 for last 100 steps: 0.516239897311\n",
      "Average loss at step 30900 for last 100 steps: 0.506241779923\n",
      "Average loss at step 31000 for last 100 steps: 0.516485011578\n",
      "Average loss at step 31100 for last 100 steps: 0.521944744587\n",
      "Average loss at step 31200 for last 100 steps: 0.501921231747\n",
      "Average loss at step 31300 for last 100 steps: 0.507041465044\n",
      "Average loss at step 31400 for last 100 steps: 0.524768327177\n",
      "Average loss at step 31500 for last 100 steps: 0.520413393974\n",
      "Average loss at step 31600 for last 100 steps: 0.516540311575\n",
      "Average loss at step 31700 for last 100 steps: 0.502734309137\n",
      "Average loss at step 31800 for last 100 steps: 0.529571590424\n",
      "Average loss at step 31900 for last 100 steps: 0.516299719214\n",
      "Average loss at step 32000 for last 100 steps: 0.506569483876\n",
      "Average loss at step 32100 for last 100 steps: 0.521596563458\n",
      "Average loss at step 32200 for last 100 steps: 0.542770393193\n",
      "Average loss at step 32300 for last 100 steps: 0.516424063444\n",
      "Average loss at step 32400 for last 100 steps: 0.513058696687\n",
      "Average loss at step 32500 for last 100 steps: 0.510136438906\n",
      "Average loss at step 32600 for last 100 steps: 0.54238214612\n",
      "Average loss at step 32700 for last 100 steps: 0.530280297101\n",
      "Average loss at step 32800 for last 100 steps: 0.504702105224\n",
      "Average loss at step 32900 for last 100 steps: 0.528566794395\n",
      "Average loss at step 33000 for last 100 steps: 0.487605126798\n",
      "Average loss at step 33100 for last 100 steps: 0.516787324548\n",
      "Average loss at step 33200 for last 100 steps: 0.523196170926\n",
      "Average loss at step 33300 for last 100 steps: 0.528365646601\n",
      "\n",
      "EPOCH 1\n",
      "Average loss at step 100 for last 100 steps: 0.530609915257\n",
      "Average loss at step 200 for last 100 steps: 0.518268354535\n",
      "Average loss at step 300 for last 100 steps: 0.528233252466\n",
      "Average loss at step 400 for last 100 steps: 0.521540038288\n",
      "Average loss at step 500 for last 100 steps: 0.512317174375\n",
      "Average loss at step 600 for last 100 steps: 0.534068685472\n",
      "Average loss at step 700 for last 100 steps: 0.518514384031\n",
      "Average loss at step 800 for last 100 steps: 0.525726410449\n",
      "Average loss at step 900 for last 100 steps: 0.518951930404\n",
      "Average loss at step 1000 for last 100 steps: 0.509701904953\n",
      "Average loss at step 1100 for last 100 steps: 0.517628323436\n",
      "Average loss at step 1200 for last 100 steps: 0.519347730577\n",
      "Average loss at step 1300 for last 100 steps: 0.51665803194\n",
      "Average loss at step 1400 for last 100 steps: 0.510890874863\n",
      "Average loss at step 1500 for last 100 steps: 0.519816854596\n",
      "Average loss at step 1600 for last 100 steps: 0.519645042121\n",
      "Average loss at step 1700 for last 100 steps: 0.515496550202\n",
      "Average loss at step 1800 for last 100 steps: 0.522706640065\n",
      "Average loss at step 1900 for last 100 steps: 0.51817653358\n",
      "Average loss at step 2000 for last 100 steps: 0.526622511148\n",
      "Average loss at step 2100 for last 100 steps: 0.52339481771\n",
      "Average loss at step 2200 for last 100 steps: 0.507367942929\n",
      "Average loss at step 2300 for last 100 steps: 0.514552670121\n",
      "Average loss at step 2400 for last 100 steps: 0.518865105808\n",
      "Average loss at step 2500 for last 100 steps: 0.520076240599\n",
      "Average loss at step 2600 for last 100 steps: 0.523271211386\n",
      "Average loss at step 2700 for last 100 steps: 0.512775523961\n",
      "Average loss at step 2800 for last 100 steps: 0.525236749947\n",
      "Average loss at step 2900 for last 100 steps: 0.530610799193\n",
      "Average loss at step 3000 for last 100 steps: 0.51181717962\n",
      "Average loss at step 3100 for last 100 steps: 0.525678293109\n",
      "Average loss at step 3200 for last 100 steps: 0.517702045739\n",
      "Average loss at step 3300 for last 100 steps: 0.51845305115\n",
      "Average loss at step 3400 for last 100 steps: 0.519212180674\n",
      "Average loss at step 3500 for last 100 steps: 0.527695559859\n",
      "Average loss at step 3600 for last 100 steps: 0.523814859986\n",
      "Average loss at step 3700 for last 100 steps: 0.525556038022\n",
      "Average loss at step 3800 for last 100 steps: 0.515301365852\n",
      "Average loss at step 3900 for last 100 steps: 0.514299644232\n",
      "Average loss at step 4000 for last 100 steps: 0.531868925393\n",
      "Average loss at step 4100 for last 100 steps: 0.519758505821\n",
      "Average loss at step 4200 for last 100 steps: 0.533514250815\n",
      "Average loss at step 4300 for last 100 steps: 0.514420186877\n",
      "Average loss at step 4400 for last 100 steps: 0.525464369953\n",
      "Average loss at step 4500 for last 100 steps: 0.535573816299\n",
      "Average loss at step 4600 for last 100 steps: 0.534102145731\n",
      "Average loss at step 4700 for last 100 steps: 0.516733843684\n",
      "Average loss at step 4800 for last 100 steps: 0.519344629943\n",
      "Average loss at step 4900 for last 100 steps: 0.529413125813\n",
      "Average loss at step 5000 for last 100 steps: 0.533350993097\n",
      "Average loss at step 5100 for last 100 steps: 0.506959456205\n",
      "Average loss at step 5200 for last 100 steps: 0.518455984294\n",
      "Average loss at step 5300 for last 100 steps: 0.509096137881\n",
      "Average loss at step 5400 for last 100 steps: 0.528435130119\n",
      "Average loss at step 5500 for last 100 steps: 0.508726514578\n",
      "Average loss at step 5600 for last 100 steps: 0.529739157259\n",
      "Average loss at step 5700 for last 100 steps: 0.525083151758\n",
      "Average loss at step 5800 for last 100 steps: 0.524463242888\n",
      "Average loss at step 5900 for last 100 steps: 0.52149435848\n",
      "Average loss at step 6000 for last 100 steps: 0.526800960898\n",
      "Average loss at step 6100 for last 100 steps: 0.529666719437\n",
      "Average loss at step 6200 for last 100 steps: 0.524458858073\n",
      "Average loss at step 6300 for last 100 steps: 0.520314503014\n",
      "Average loss at step 6400 for last 100 steps: 0.524387490749\n",
      "Average loss at step 6500 for last 100 steps: 0.508387958407\n",
      "Average loss at step 6600 for last 100 steps: 0.525737543106\n",
      "Average loss at step 6700 for last 100 steps: 0.536894990802\n",
      "Average loss at step 6800 for last 100 steps: 0.526847132146\n",
      "Average loss at step 6900 for last 100 steps: 0.511100469232\n",
      "Average loss at step 7000 for last 100 steps: 0.512801893651\n",
      "Average loss at step 7100 for last 100 steps: 0.536761699319\n",
      "Average loss at step 7200 for last 100 steps: 0.517495018989\n",
      "Average loss at step 7300 for last 100 steps: 0.516376861334\n",
      "Average loss at step 7400 for last 100 steps: 0.510052915215\n",
      "Average loss at step 7500 for last 100 steps: 0.527917522192\n",
      "Average loss at step 7600 for last 100 steps: 0.519579916596\n",
      "Average loss at step 7700 for last 100 steps: 0.502148829997\n",
      "Average loss at step 7800 for last 100 steps: 0.506155846119\n",
      "Average loss at step 7900 for last 100 steps: 0.539868719876\n",
      "Average loss at step 8000 for last 100 steps: 0.529533725977\n",
      "Average loss at step 8100 for last 100 steps: 0.510229931474\n",
      "Average loss at step 8200 for last 100 steps: 0.521355547607\n",
      "Average loss at step 8300 for last 100 steps: 0.52020807445\n",
      "Average loss at step 8400 for last 100 steps: 0.520245292783\n",
      "Average loss at step 8500 for last 100 steps: 0.543035304248\n",
      "Average loss at step 8600 for last 100 steps: 0.515593136251\n",
      "Average loss at step 8700 for last 100 steps: 0.534073438048\n",
      "Average loss at step 8800 for last 100 steps: 0.527224439979\n",
      "Average loss at step 8900 for last 100 steps: 0.51741620332\n",
      "Average loss at step 9000 for last 100 steps: 0.537115417123\n",
      "Average loss at step 9100 for last 100 steps: 0.523367674649\n",
      "Average loss at step 9200 for last 100 steps: 0.516134571433\n",
      "Average loss at step 9300 for last 100 steps: 0.529311685264\n",
      "Average loss at step 9400 for last 100 steps: 0.514657689333\n",
      "Average loss at step 9500 for last 100 steps: 0.523236701488\n",
      "Average loss at step 9600 for last 100 steps: 0.544439198673\n",
      "Average loss at step 9700 for last 100 steps: 0.510977890491\n",
      "Average loss at step 9800 for last 100 steps: 0.521021212637\n",
      "Average loss at step 9900 for last 100 steps: 0.525880383253\n",
      "Average loss at step 10000 for last 100 steps: 0.527451046705\n",
      "Average loss at step 10100 for last 100 steps: 0.515913907886\n",
      "Average loss at step 10200 for last 100 steps: 0.512340894341\n",
      "Average loss at step 10300 for last 100 steps: 0.525488185585\n",
      "Average loss at step 10400 for last 100 steps: 0.528478356302\n",
      "Average loss at step 10500 for last 100 steps: 0.524788739681\n",
      "Average loss at step 10600 for last 100 steps: 0.523695167601\n",
      "Average loss at step 10700 for last 100 steps: 0.52148681432\n",
      "Average loss at step 10800 for last 100 steps: 0.517127438188\n",
      "Average loss at step 10900 for last 100 steps: 0.529004667401\n",
      "Average loss at step 11000 for last 100 steps: 0.521047512591\n",
      "Average loss at step 11100 for last 100 steps: 0.514375319481\n",
      "Average loss at step 11200 for last 100 steps: 0.520761066973\n",
      "Average loss at step 11300 for last 100 steps: 0.507572375238\n",
      "Average loss at step 11400 for last 100 steps: 0.515166553557\n",
      "Average loss at step 11500 for last 100 steps: 0.527838714421\n",
      "Average loss at step 11600 for last 100 steps: 0.514287548661\n",
      "Average loss at step 11700 for last 100 steps: 0.523620528579\n",
      "Average loss at step 11800 for last 100 steps: 0.521369821727\n",
      "Average loss at step 11900 for last 100 steps: 0.530517144799\n",
      "Average loss at step 12000 for last 100 steps: 0.510172359347\n",
      "Average loss at step 12100 for last 100 steps: 0.506970780492\n",
      "Average loss at step 12200 for last 100 steps: 0.503530660272\n",
      "Average loss at step 12300 for last 100 steps: 0.513948789239\n",
      "Average loss at step 12400 for last 100 steps: 0.526014447212\n",
      "Average loss at step 12500 for last 100 steps: 0.527447645664\n",
      "Average loss at step 12600 for last 100 steps: 0.518954979777\n",
      "Average loss at step 12700 for last 100 steps: 0.527832023799\n",
      "Average loss at step 12800 for last 100 steps: 0.524949146807\n",
      "Average loss at step 12900 for last 100 steps: 0.526782409549\n",
      "Average loss at step 13000 for last 100 steps: 0.507828684151\n",
      "Average loss at step 13100 for last 100 steps: 0.52392467618\n",
      "Average loss at step 13200 for last 100 steps: 0.515636153817\n",
      "Average loss at step 13300 for last 100 steps: 0.500974138379\n",
      "Average loss at step 13400 for last 100 steps: 0.522657846808\n",
      "Average loss at step 13500 for last 100 steps: 0.512750098109\n",
      "Average loss at step 13600 for last 100 steps: 0.525440992117\n",
      "Average loss at step 13700 for last 100 steps: 0.522446849644\n",
      "Average loss at step 13800 for last 100 steps: 0.5249343732\n",
      "Average loss at step 13900 for last 100 steps: 0.529079333246\n",
      "Average loss at step 14000 for last 100 steps: 0.520765480399\n",
      "Average loss at step 14100 for last 100 steps: 0.531823713481\n",
      "Average loss at step 14200 for last 100 steps: 0.515272674263\n",
      "Average loss at step 14300 for last 100 steps: 0.505602145493\n",
      "Average loss at step 14400 for last 100 steps: 0.528844854534\n",
      "Average loss at step 14500 for last 100 steps: 0.511071472466\n",
      "Average loss at step 14600 for last 100 steps: 0.52508323729\n",
      "Average loss at step 14700 for last 100 steps: 0.539819287062\n",
      "Average loss at step 14800 for last 100 steps: 0.508800510764\n",
      "Average loss at step 14900 for last 100 steps: 0.514081460238\n",
      "Average loss at step 15000 for last 100 steps: 0.513853187263\n",
      "Average loss at step 15100 for last 100 steps: 0.51321036756\n",
      "Average loss at step 15200 for last 100 steps: 0.512364623249\n",
      "Average loss at step 15300 for last 100 steps: 0.519002259672\n",
      "Average loss at step 15400 for last 100 steps: 0.529022671878\n",
      "Average loss at step 15500 for last 100 steps: 0.511722916365\n",
      "Average loss at step 15600 for last 100 steps: 0.508042200208\n",
      "Average loss at step 15700 for last 100 steps: 0.507062070072\n",
      "Average loss at step 15800 for last 100 steps: 0.523655515015\n",
      "Average loss at step 15900 for last 100 steps: 0.498870981336\n",
      "Average loss at step 16000 for last 100 steps: 0.525252410173\n",
      "Average loss at step 16100 for last 100 steps: 0.524099198282\n",
      "Average loss at step 16200 for last 100 steps: 0.521942309737\n",
      "Average loss at step 16300 for last 100 steps: 0.517590909004\n",
      "Average loss at step 16400 for last 100 steps: 0.518565329313\n",
      "Average loss at step 16500 for last 100 steps: 0.523451791704\n",
      "Average loss at step 16600 for last 100 steps: 0.518240939081\n",
      "Average loss at step 16700 for last 100 steps: 0.53410020858\n",
      "Average loss at step 16800 for last 100 steps: 0.514725369811\n",
      "Average loss at step 16900 for last 100 steps: 0.50972504288\n",
      "Average loss at step 17000 for last 100 steps: 0.527607437968\n",
      "Average loss at step 17100 for last 100 steps: 0.52097517401\n",
      "Average loss at step 17200 for last 100 steps: 0.538704391122\n",
      "Average loss at step 17300 for last 100 steps: 0.512694509029\n",
      "Average loss at step 17400 for last 100 steps: 0.525784060359\n",
      "Average loss at step 17500 for last 100 steps: 0.529783800244\n",
      "Average loss at step 17600 for last 100 steps: 0.515676705837\n",
      "Average loss at step 17700 for last 100 steps: 0.508444948494\n",
      "Average loss at step 17800 for last 100 steps: 0.526814667583\n",
      "Average loss at step 17900 for last 100 steps: 0.530610770285\n",
      "Average loss at step 18000 for last 100 steps: 0.531946441829\n",
      "Average loss at step 18100 for last 100 steps: 0.537253304422\n",
      "Average loss at step 18200 for last 100 steps: 0.511357223094\n",
      "Average loss at step 18300 for last 100 steps: 0.52718377769\n",
      "Average loss at step 18400 for last 100 steps: 0.525974619091\n",
      "Average loss at step 18500 for last 100 steps: 0.518077528179\n",
      "Average loss at step 18600 for last 100 steps: 0.525389683545\n",
      "Average loss at step 18700 for last 100 steps: 0.509870417714\n",
      "Average loss at step 18800 for last 100 steps: 0.513060664535\n",
      "Average loss at step 18900 for last 100 steps: 0.51382206589\n",
      "Average loss at step 19000 for last 100 steps: 0.524623630345\n",
      "Average loss at step 19100 for last 100 steps: 0.510682115257\n",
      "Average loss at step 19200 for last 100 steps: 0.508533019722\n",
      "Average loss at step 19300 for last 100 steps: 0.512340345681\n",
      "Average loss at step 19400 for last 100 steps: 0.530059384108\n",
      "Average loss at step 19500 for last 100 steps: 0.528230424523\n",
      "Average loss at step 19600 for last 100 steps: 0.513232108355\n",
      "Average loss at step 19700 for last 100 steps: 0.521394383311\n",
      "Average loss at step 19800 for last 100 steps: 0.524967991412\n",
      "Average loss at step 19900 for last 100 steps: 0.5294998312\n",
      "Average loss at step 20000 for last 100 steps: 0.524087750316\n",
      "Average loss at step 20100 for last 100 steps: 0.52293609798\n",
      "Average loss at step 20200 for last 100 steps: 0.524666873217\n",
      "Average loss at step 20300 for last 100 steps: 0.528195938766\n",
      "Average loss at step 20400 for last 100 steps: 0.505562287569\n",
      "Average loss at step 20500 for last 100 steps: 0.527375362217\n",
      "Average loss at step 20600 for last 100 steps: 0.517856498361\n",
      "Average loss at step 20700 for last 100 steps: 0.512290573716\n",
      "Average loss at step 20800 for last 100 steps: 0.536380411983\n",
      "Average loss at step 20900 for last 100 steps: 0.532088440061\n",
      "Average loss at step 21000 for last 100 steps: 0.532382903397\n",
      "Average loss at step 21100 for last 100 steps: 0.505227379799\n",
      "Average loss at step 21200 for last 100 steps: 0.522481701076\n",
      "Average loss at step 21300 for last 100 steps: 0.523414796591\n",
      "Average loss at step 21400 for last 100 steps: 0.523814163208\n",
      "Average loss at step 21500 for last 100 steps: 0.502044588029\n",
      "Average loss at step 21600 for last 100 steps: 0.516771766841\n",
      "Average loss at step 21700 for last 100 steps: 0.517604975998\n",
      "Average loss at step 21800 for last 100 steps: 0.520903238356\n",
      "Average loss at step 21900 for last 100 steps: 0.507983231843\n",
      "Average loss at step 22000 for last 100 steps: 0.512375209332\n",
      "Average loss at step 22100 for last 100 steps: 0.517205648869\n",
      "Average loss at step 22200 for last 100 steps: 0.499897979498\n",
      "Average loss at step 22300 for last 100 steps: 0.519396959841\n",
      "Average loss at step 22400 for last 100 steps: 0.509004990458\n",
      "Average loss at step 22500 for last 100 steps: 0.519099099338\n",
      "Average loss at step 22600 for last 100 steps: 0.515007505715\n",
      "Average loss at step 22700 for last 100 steps: 0.507991556823\n",
      "Average loss at step 22800 for last 100 steps: 0.521435522437\n",
      "Average loss at step 22900 for last 100 steps: 0.514655081332\n",
      "Average loss at step 23000 for last 100 steps: 0.527041657567\n",
      "Average loss at step 23100 for last 100 steps: 0.524900594652\n",
      "Average loss at step 23200 for last 100 steps: 0.513283090293\n",
      "Average loss at step 23300 for last 100 steps: 0.520496379137\n",
      "Average loss at step 23400 for last 100 steps: 0.524601455033\n",
      "Average loss at step 23500 for last 100 steps: 0.52540630579\n",
      "Average loss at step 23600 for last 100 steps: 0.533207378089\n",
      "Average loss at step 23700 for last 100 steps: 0.526666019857\n",
      "Average loss at step 23800 for last 100 steps: 0.514775538743\n",
      "Average loss at step 23900 for last 100 steps: 0.518838023245\n",
      "Average loss at step 24000 for last 100 steps: 0.51840780437\n",
      "Average loss at step 24100 for last 100 steps: 0.516891405582\n",
      "Average loss at step 24200 for last 100 steps: 0.516156570613\n",
      "Average loss at step 24300 for last 100 steps: 0.513491350114\n",
      "Average loss at step 24400 for last 100 steps: 0.51394110471\n",
      "Average loss at step 24500 for last 100 steps: 0.518838199973\n",
      "Average loss at step 24600 for last 100 steps: 0.499873738289\n",
      "Average loss at step 24700 for last 100 steps: 0.515999771357\n",
      "Average loss at step 24800 for last 100 steps: 0.509960004389\n",
      "Average loss at step 24900 for last 100 steps: 0.528008570969\n",
      "Average loss at step 25000 for last 100 steps: 0.507734036446\n",
      "Average loss at step 25100 for last 100 steps: 0.526060228348\n",
      "Average loss at step 25200 for last 100 steps: 0.523581205904\n",
      "Average loss at step 25300 for last 100 steps: 0.528560912609\n",
      "Average loss at step 25400 for last 100 steps: 0.518161887825\n",
      "Average loss at step 25500 for last 100 steps: 0.533565745652\n",
      "Average loss at step 25600 for last 100 steps: 0.510265557468\n",
      "Average loss at step 25700 for last 100 steps: 0.531837208569\n",
      "Average loss at step 25800 for last 100 steps: 0.504467723668\n",
      "Average loss at step 25900 for last 100 steps: 0.519115065038\n",
      "Average loss at step 26000 for last 100 steps: 0.531538769603\n",
      "Average loss at step 26100 for last 100 steps: 0.534177343845\n",
      "Average loss at step 26200 for last 100 steps: 0.520732908547\n",
      "Average loss at step 26300 for last 100 steps: 0.532112604082\n",
      "Average loss at step 26400 for last 100 steps: 0.52933329761\n",
      "Average loss at step 26500 for last 100 steps: 0.520747362673\n",
      "Average loss at step 26600 for last 100 steps: 0.533345538974\n",
      "Average loss at step 26700 for last 100 steps: 0.52312102288\n",
      "Average loss at step 26800 for last 100 steps: 0.517239847779\n",
      "Average loss at step 26900 for last 100 steps: 0.511408913136\n",
      "Average loss at step 27000 for last 100 steps: 0.527644830048\n",
      "Average loss at step 27100 for last 100 steps: 0.512170567811\n",
      "Average loss at step 27200 for last 100 steps: 0.504178241491\n",
      "Average loss at step 27300 for last 100 steps: 0.530588295758\n",
      "Average loss at step 27400 for last 100 steps: 0.513684119582\n",
      "Average loss at step 27500 for last 100 steps: 0.522288451195\n",
      "Average loss at step 27600 for last 100 steps: 0.537666736245\n",
      "Average loss at step 27700 for last 100 steps: 0.525598965585\n",
      "Average loss at step 27800 for last 100 steps: 0.517539156973\n",
      "Average loss at step 27900 for last 100 steps: 0.524989772439\n",
      "Average loss at step 28000 for last 100 steps: 0.522212311029\n",
      "Average loss at step 28100 for last 100 steps: 0.528388106823\n",
      "Average loss at step 28200 for last 100 steps: 0.512440350354\n",
      "Average loss at step 28300 for last 100 steps: 0.51071574837\n",
      "Average loss at step 28400 for last 100 steps: 0.524040009677\n",
      "Average loss at step 28500 for last 100 steps: 0.516589497924\n",
      "Average loss at step 28600 for last 100 steps: 0.523219960332\n",
      "Average loss at step 28700 for last 100 steps: 0.515371984243\n",
      "Average loss at step 28800 for last 100 steps: 0.527788859904\n",
      "Average loss at step 28900 for last 100 steps: 0.527441760898\n",
      "Average loss at step 29000 for last 100 steps: 0.518100302219\n",
      "Average loss at step 29100 for last 100 steps: 0.51291652441\n",
      "Average loss at step 29200 for last 100 steps: 0.495351541638\n",
      "Average loss at step 29300 for last 100 steps: 0.523934069574\n",
      "Average loss at step 29400 for last 100 steps: 0.521654022038\n",
      "Average loss at step 29500 for last 100 steps: 0.519706431031\n",
      "Average loss at step 29600 for last 100 steps: 0.513456766009\n",
      "Average loss at step 29700 for last 100 steps: 0.518323357105\n",
      "Average loss at step 29800 for last 100 steps: 0.527468418479\n",
      "Average loss at step 29900 for last 100 steps: 0.507254537046\n",
      "Average loss at step 30000 for last 100 steps: 0.510762970746\n",
      "Average loss at step 30100 for last 100 steps: 0.50148452729\n",
      "Average loss at step 30200 for last 100 steps: 0.522918427885\n",
      "Average loss at step 30300 for last 100 steps: 0.506756641567\n",
      "Average loss at step 30400 for last 100 steps: 0.534502507448\n",
      "Average loss at step 30500 for last 100 steps: 0.510089596212\n",
      "Average loss at step 30600 for last 100 steps: 0.519530715048\n",
      "Average loss at step 30700 for last 100 steps: 0.519590270519\n",
      "Average loss at step 30800 for last 100 steps: 0.525421521664\n",
      "Average loss at step 30900 for last 100 steps: 0.537165931165\n",
      "Average loss at step 31000 for last 100 steps: 0.502473785579\n",
      "Average loss at step 31100 for last 100 steps: 0.519975061417\n",
      "Average loss at step 31200 for last 100 steps: 0.525161017478\n",
      "Average loss at step 31300 for last 100 steps: 0.519110537171\n",
      "Average loss at step 31400 for last 100 steps: 0.505791837573\n",
      "Average loss at step 31500 for last 100 steps: 0.501663224995\n",
      "Average loss at step 31600 for last 100 steps: 0.528884147704\n",
      "Average loss at step 31700 for last 100 steps: 0.520497214198\n",
      "Average loss at step 31800 for last 100 steps: 0.525944306552\n",
      "Average loss at step 31900 for last 100 steps: 0.528928021789\n",
      "Average loss at step 32000 for last 100 steps: 0.516286794543\n",
      "Average loss at step 32100 for last 100 steps: 0.521876685917\n",
      "Average loss at step 32200 for last 100 steps: 0.526134595871\n",
      "Average loss at step 32300 for last 100 steps: 0.513251172602\n",
      "Average loss at step 32400 for last 100 steps: 0.514196880162\n",
      "Average loss at step 32500 for last 100 steps: 0.521494723856\n",
      "Average loss at step 32600 for last 100 steps: 0.518108962774\n",
      "Average loss at step 32700 for last 100 steps: 0.529784818292\n",
      "Average loss at step 32800 for last 100 steps: 0.522612563372\n",
      "Average loss at step 32900 for last 100 steps: 0.50894671917\n",
      "Average loss at step 33000 for last 100 steps: 0.513480833471\n",
      "Average loss at step 33100 for last 100 steps: 0.516195079088\n",
      "Average loss at step 33200 for last 100 steps: 0.510699447393\n",
      "Average loss at step 33300 for last 100 steps: 0.517513136864\n",
      "\n",
      "EPOCH 2\n",
      "Average loss at step 100 for last 100 steps: 0.536422060728\n",
      "Average loss at step 200 for last 100 steps: 0.513550291061\n",
      "Average loss at step 300 for last 100 steps: 0.510126843154\n",
      "Average loss at step 400 for last 100 steps: 0.510465904772\n",
      "Average loss at step 500 for last 100 steps: 0.523262793422\n",
      "Average loss at step 600 for last 100 steps: 0.522055264711\n",
      "Average loss at step 700 for last 100 steps: 0.520523290336\n",
      "Average loss at step 800 for last 100 steps: 0.519592994452\n",
      "Average loss at step 900 for last 100 steps: 0.531263357401\n",
      "Average loss at step 1000 for last 100 steps: 0.52654427737\n",
      "Average loss at step 1100 for last 100 steps: 0.510193247199\n",
      "Average loss at step 1200 for last 100 steps: 0.519160658419\n",
      "Average loss at step 1300 for last 100 steps: 0.506329419315\n",
      "Average loss at step 1400 for last 100 steps: 0.517386784554\n",
      "Average loss at step 1500 for last 100 steps: 0.527387975454\n",
      "Average loss at step 1600 for last 100 steps: 0.525584578514\n",
      "Average loss at step 1700 for last 100 steps: 0.510892471075\n",
      "Average loss at step 1800 for last 100 steps: 0.519231159389\n",
      "Average loss at step 1900 for last 100 steps: 0.52523763746\n",
      "Average loss at step 2000 for last 100 steps: 0.537229346931\n",
      "Average loss at step 2100 for last 100 steps: 0.515406177342\n",
      "Average loss at step 2200 for last 100 steps: 0.522101090252\n",
      "Average loss at step 2300 for last 100 steps: 0.517496411502\n",
      "Average loss at step 2400 for last 100 steps: 0.526027225852\n",
      "Average loss at step 2500 for last 100 steps: 0.519303474128\n",
      "Average loss at step 2600 for last 100 steps: 0.520391550362\n",
      "Average loss at step 2700 for last 100 steps: 0.507914731801\n",
      "Average loss at step 2800 for last 100 steps: 0.510972347856\n",
      "Average loss at step 2900 for last 100 steps: 0.508034395278\n",
      "Average loss at step 3000 for last 100 steps: 0.509963386953\n",
      "Average loss at step 3100 for last 100 steps: 0.519504452944\n",
      "Average loss at step 3200 for last 100 steps: 0.507949441671\n",
      "Average loss at step 3300 for last 100 steps: 0.513012234271\n",
      "Average loss at step 3400 for last 100 steps: 0.514133654237\n",
      "Average loss at step 3500 for last 100 steps: 0.508138907552\n",
      "Average loss at step 3600 for last 100 steps: 0.50838653326\n",
      "Average loss at step 3700 for last 100 steps: 0.514402849972\n",
      "Average loss at step 3800 for last 100 steps: 0.508659796715\n",
      "Average loss at step 3900 for last 100 steps: 0.531308827996\n",
      "Average loss at step 4000 for last 100 steps: 0.51038395673\n",
      "Average loss at step 4100 for last 100 steps: 0.521460724771\n",
      "Average loss at step 4200 for last 100 steps: 0.504607918561\n",
      "Average loss at step 4300 for last 100 steps: 0.501666743755\n",
      "Average loss at step 4400 for last 100 steps: 0.509906838238\n",
      "Average loss at step 4500 for last 100 steps: 0.51009457022\n",
      "Average loss at step 4600 for last 100 steps: 0.537631532252\n",
      "Average loss at step 4700 for last 100 steps: 0.52910736084\n",
      "Average loss at step 4800 for last 100 steps: 0.51601523906\n",
      "Average loss at step 4900 for last 100 steps: 0.532305249274\n",
      "Average loss at step 5000 for last 100 steps: 0.516956314743\n",
      "Average loss at step 5100 for last 100 steps: 0.524394428134\n",
      "Average loss at step 5200 for last 100 steps: 0.517937994301\n",
      "Average loss at step 5300 for last 100 steps: 0.514054031074\n",
      "Average loss at step 5400 for last 100 steps: 0.521965538561\n",
      "Average loss at step 5500 for last 100 steps: 0.507217128575\n",
      "Average loss at step 5600 for last 100 steps: 0.519049041569\n",
      "Average loss at step 5700 for last 100 steps: 0.51491776228\n",
      "Average loss at step 5800 for last 100 steps: 0.529223809242\n",
      "Average loss at step 5900 for last 100 steps: 0.516252047122\n",
      "Average loss at step 6000 for last 100 steps: 0.516049935222\n",
      "Average loss at step 6100 for last 100 steps: 0.529193619788\n",
      "Average loss at step 6200 for last 100 steps: 0.524904419482\n",
      "Average loss at step 6300 for last 100 steps: 0.501825265288\n",
      "Average loss at step 6400 for last 100 steps: 0.504466837049\n",
      "Average loss at step 6500 for last 100 steps: 0.512292562425\n",
      "Average loss at step 6600 for last 100 steps: 0.515999983847\n",
      "Average loss at step 6700 for last 100 steps: 0.523941587806\n",
      "Average loss at step 6800 for last 100 steps: 0.523520864248\n",
      "Average loss at step 6900 for last 100 steps: 0.527759955823\n",
      "Average loss at step 7000 for last 100 steps: 0.528967098296\n",
      "Average loss at step 7100 for last 100 steps: 0.506440520287\n",
      "Average loss at step 7200 for last 100 steps: 0.531659303904\n",
      "Average loss at step 7300 for last 100 steps: 0.506662813723\n",
      "Average loss at step 7400 for last 100 steps: 0.524861365259\n",
      "Average loss at step 7500 for last 100 steps: 0.520646273196\n",
      "Average loss at step 7600 for last 100 steps: 0.532983370423\n",
      "Average loss at step 7700 for last 100 steps: 0.514340496361\n",
      "Average loss at step 7800 for last 100 steps: 0.530896229446\n",
      "Average loss at step 7900 for last 100 steps: 0.525495918989\n",
      "Average loss at step 8000 for last 100 steps: 0.50686981678\n",
      "Average loss at step 8100 for last 100 steps: 0.525492078662\n",
      "Average loss at step 8200 for last 100 steps: 0.526133596301\n",
      "Average loss at step 8300 for last 100 steps: 0.512950333655\n",
      "Average loss at step 8400 for last 100 steps: 0.52037442416\n",
      "Average loss at step 8500 for last 100 steps: 0.521260102689\n",
      "Average loss at step 8600 for last 100 steps: 0.521573307216\n",
      "Average loss at step 8700 for last 100 steps: 0.511121022105\n",
      "Average loss at step 8800 for last 100 steps: 0.525429961383\n",
      "Average loss at step 8900 for last 100 steps: 0.514850126207\n",
      "Average loss at step 9000 for last 100 steps: 0.516863097548\n",
      "Average loss at step 9100 for last 100 steps: 0.512857333124\n",
      "Average loss at step 9200 for last 100 steps: 0.525879409313\n",
      "Average loss at step 9300 for last 100 steps: 0.520383306742\n",
      "Average loss at step 9400 for last 100 steps: 0.521958918273\n",
      "Average loss at step 9500 for last 100 steps: 0.508754757047\n",
      "Average loss at step 9600 for last 100 steps: 0.529518989623\n",
      "Average loss at step 9700 for last 100 steps: 0.530787474811\n",
      "Average loss at step 9800 for last 100 steps: 0.519039908648\n",
      "Average loss at step 9900 for last 100 steps: 0.51835475564\n",
      "Average loss at step 10000 for last 100 steps: 0.509452428818\n",
      "Average loss at step 10100 for last 100 steps: 0.509216604531\n",
      "Average loss at step 10200 for last 100 steps: 0.526012818217\n",
      "Average loss at step 10300 for last 100 steps: 0.508202341199\n",
      "Average loss at step 10400 for last 100 steps: 0.506301561594\n",
      "Average loss at step 10500 for last 100 steps: 0.516373102069\n",
      "Average loss at step 10600 for last 100 steps: 0.521197579801\n",
      "Average loss at step 10700 for last 100 steps: 0.519192754328\n",
      "Average loss at step 10800 for last 100 steps: 0.508106164634\n",
      "Average loss at step 10900 for last 100 steps: 0.516469722092\n",
      "Average loss at step 11000 for last 100 steps: 0.514050886035\n",
      "Average loss at step 11100 for last 100 steps: 0.516923614144\n",
      "Average loss at step 11200 for last 100 steps: 0.523181423843\n",
      "Average loss at step 11300 for last 100 steps: 0.521045701802\n",
      "Average loss at step 11400 for last 100 steps: 0.513247225285\n",
      "Average loss at step 11500 for last 100 steps: 0.50427426815\n",
      "Average loss at step 11600 for last 100 steps: 0.505102539957\n",
      "Average loss at step 11700 for last 100 steps: 0.505201945305\n",
      "Average loss at step 11800 for last 100 steps: 0.512019605637\n",
      "Average loss at step 11900 for last 100 steps: 0.524315075278\n",
      "Average loss at step 12000 for last 100 steps: 0.515141657591\n",
      "Average loss at step 12100 for last 100 steps: 0.488975916803\n",
      "Average loss at step 12200 for last 100 steps: 0.530471702516\n",
      "Average loss at step 12300 for last 100 steps: 0.523636798859\n",
      "Average loss at step 12400 for last 100 steps: 0.511603682637\n",
      "Average loss at step 12500 for last 100 steps: 0.510929738283\n",
      "Average loss at step 12600 for last 100 steps: 0.513264075816\n",
      "Average loss at step 12700 for last 100 steps: 0.515467133522\n",
      "Average loss at step 12800 for last 100 steps: 0.511186497211\n",
      "Average loss at step 12900 for last 100 steps: 0.522400043309\n",
      "Average loss at step 13000 for last 100 steps: 0.526006861329\n",
      "Average loss at step 13100 for last 100 steps: 0.520860006213\n",
      "Average loss at step 13200 for last 100 steps: 0.518148052394\n",
      "Average loss at step 13300 for last 100 steps: 0.531326567531\n",
      "Average loss at step 13400 for last 100 steps: 0.521351320148\n",
      "Average loss at step 13500 for last 100 steps: 0.517748713493\n",
      "Average loss at step 13600 for last 100 steps: 0.520473490655\n",
      "Average loss at step 13700 for last 100 steps: 0.519828626812\n",
      "Average loss at step 13800 for last 100 steps: 0.526423459053\n",
      "Average loss at step 13900 for last 100 steps: 0.540790925324\n",
      "Average loss at step 14000 for last 100 steps: 0.514030946791\n",
      "Average loss at step 14100 for last 100 steps: 0.515848462284\n",
      "Average loss at step 14200 for last 100 steps: 0.520511466265\n",
      "Average loss at step 14300 for last 100 steps: 0.528019032478\n",
      "Average loss at step 14400 for last 100 steps: 0.517127446532\n",
      "Average loss at step 14500 for last 100 steps: 0.51716509819\n",
      "Average loss at step 14600 for last 100 steps: 0.52642650485\n",
      "Average loss at step 14700 for last 100 steps: 0.526304994226\n",
      "Average loss at step 14800 for last 100 steps: 0.517984112501\n",
      "Average loss at step 14900 for last 100 steps: 0.515161544085\n",
      "Average loss at step 15000 for last 100 steps: 0.536840113699\n",
      "Average loss at step 15100 for last 100 steps: 0.526673420966\n",
      "Average loss at step 15200 for last 100 steps: 0.534268053174\n",
      "Average loss at step 15300 for last 100 steps: 0.508901844323\n",
      "Average loss at step 15400 for last 100 steps: 0.531303117871\n",
      "Average loss at step 15500 for last 100 steps: 0.510200251043\n",
      "Average loss at step 15600 for last 100 steps: 0.538457394242\n",
      "Average loss at step 15700 for last 100 steps: 0.529065286815\n",
      "Average loss at step 15800 for last 100 steps: 0.510872384906\n",
      "Average loss at step 15900 for last 100 steps: 0.525174404979\n",
      "Average loss at step 16000 for last 100 steps: 0.510515037179\n",
      "Average loss at step 16100 for last 100 steps: 0.525850930214\n",
      "Average loss at step 16200 for last 100 steps: 0.505676759183\n",
      "Average loss at step 16300 for last 100 steps: 0.530320752561\n",
      "Average loss at step 16400 for last 100 steps: 0.528579298258\n",
      "Average loss at step 16500 for last 100 steps: 0.503452159166\n",
      "Average loss at step 16600 for last 100 steps: 0.514273649454\n",
      "Average loss at step 16700 for last 100 steps: 0.507426719964\n",
      "Average loss at step 16800 for last 100 steps: 0.520260836482\n",
      "Average loss at step 16900 for last 100 steps: 0.525690065026\n",
      "Average loss at step 17000 for last 100 steps: 0.511202118993\n",
      "Average loss at step 17100 for last 100 steps: 0.520556606948\n",
      "Average loss at step 17200 for last 100 steps: 0.522271646261\n",
      "Average loss at step 17300 for last 100 steps: 0.51628170222\n",
      "Average loss at step 17400 for last 100 steps: 0.518435554504\n",
      "Average loss at step 17500 for last 100 steps: 0.523069840968\n",
      "Average loss at step 17600 for last 100 steps: 0.524748054445\n",
      "Average loss at step 17700 for last 100 steps: 0.530614889264\n",
      "Average loss at step 17800 for last 100 steps: 0.505102936327\n",
      "Average loss at step 17900 for last 100 steps: 0.522810093164\n",
      "Average loss at step 18000 for last 100 steps: 0.513771735728\n",
      "Average loss at step 18100 for last 100 steps: 0.52776004374\n",
      "Average loss at step 18200 for last 100 steps: 0.519800110459\n",
      "Average loss at step 18300 for last 100 steps: 0.516307936609\n",
      "Average loss at step 18400 for last 100 steps: 0.533829525113\n",
      "Average loss at step 18500 for last 100 steps: 0.514945752919\n",
      "Average loss at step 18600 for last 100 steps: 0.511530720294\n",
      "Average loss at step 18700 for last 100 steps: 0.523298339546\n",
      "Average loss at step 18800 for last 100 steps: 0.514459897578\n",
      "Average loss at step 18900 for last 100 steps: 0.514964917302\n",
      "Average loss at step 19000 for last 100 steps: 0.515143799484\n",
      "Average loss at step 19100 for last 100 steps: 0.524356107712\n",
      "Average loss at step 19200 for last 100 steps: 0.518552847803\n",
      "Average loss at step 19300 for last 100 steps: 0.51822211504\n",
      "Average loss at step 19400 for last 100 steps: 0.534761265218\n",
      "Average loss at step 19500 for last 100 steps: 0.52422618717\n",
      "Average loss at step 19600 for last 100 steps: 0.505919975936\n",
      "Average loss at step 19700 for last 100 steps: 0.511151034832\n",
      "Average loss at step 19800 for last 100 steps: 0.533167763054\n",
      "Average loss at step 19900 for last 100 steps: 0.526025697291\n",
      "Average loss at step 20000 for last 100 steps: 0.533669573069\n",
      "Average loss at step 20100 for last 100 steps: 0.511183882952\n",
      "Average loss at step 20200 for last 100 steps: 0.519252391756\n",
      "Average loss at step 20300 for last 100 steps: 0.508587280512\n",
      "Average loss at step 20400 for last 100 steps: 0.510948494673\n",
      "Average loss at step 20500 for last 100 steps: 0.514295716286\n",
      "Average loss at step 20600 for last 100 steps: 0.506842482984\n",
      "Average loss at step 20700 for last 100 steps: 0.523969066143\n",
      "Average loss at step 20800 for last 100 steps: 0.52562011987\n",
      "Average loss at step 20900 for last 100 steps: 0.507893835008\n",
      "Average loss at step 21000 for last 100 steps: 0.535704883635\n",
      "Average loss at step 21100 for last 100 steps: 0.528015917242\n",
      "Average loss at step 21200 for last 100 steps: 0.526557362676\n",
      "Average loss at step 21300 for last 100 steps: 0.527949736118\n",
      "Average loss at step 21400 for last 100 steps: 0.515297324955\n",
      "Average loss at step 21500 for last 100 steps: 0.51207146585\n",
      "Average loss at step 21600 for last 100 steps: 0.524324315488\n",
      "Average loss at step 21700 for last 100 steps: 0.524696622193\n",
      "Average loss at step 21800 for last 100 steps: 0.513093741536\n",
      "Average loss at step 21900 for last 100 steps: 0.532677785456\n",
      "Average loss at step 22000 for last 100 steps: 0.531210196316\n",
      "Average loss at step 22100 for last 100 steps: 0.505133533478\n",
      "Average loss at step 22200 for last 100 steps: 0.502464050651\n",
      "Average loss at step 22300 for last 100 steps: 0.532638305426\n",
      "Average loss at step 22400 for last 100 steps: 0.523311760724\n",
      "Average loss at step 22500 for last 100 steps: 0.523479689956\n",
      "Average loss at step 22600 for last 100 steps: 0.518905234039\n",
      "Average loss at step 22700 for last 100 steps: 0.531035723388\n",
      "Average loss at step 22800 for last 100 steps: 0.531337539554\n",
      "Average loss at step 22900 for last 100 steps: 0.532149689496\n",
      "Average loss at step 23000 for last 100 steps: 0.514494683445\n",
      "Average loss at step 23100 for last 100 steps: 0.50891883105\n",
      "Average loss at step 23200 for last 100 steps: 0.521093594432\n",
      "Average loss at step 23300 for last 100 steps: 0.520682007372\n",
      "Average loss at step 23400 for last 100 steps: 0.513558820784\n",
      "Average loss at step 23500 for last 100 steps: 0.513466905057\n",
      "Average loss at step 23600 for last 100 steps: 0.527683549821\n",
      "Average loss at step 23700 for last 100 steps: 0.514468910992\n",
      "Average loss at step 23800 for last 100 steps: 0.517811962962\n",
      "Average loss at step 23900 for last 100 steps: 0.532171429396\n",
      "Average loss at step 24000 for last 100 steps: 0.511039580703\n",
      "Average loss at step 24100 for last 100 steps: 0.531303257346\n",
      "Average loss at step 24200 for last 100 steps: 0.523999782801\n",
      "Average loss at step 24300 for last 100 steps: 0.53791767925\n",
      "Average loss at step 24400 for last 100 steps: 0.504279900193\n",
      "Average loss at step 24500 for last 100 steps: 0.522814454436\n",
      "Average loss at step 24600 for last 100 steps: 0.525501256883\n",
      "Average loss at step 24700 for last 100 steps: 0.530395952165\n",
      "Average loss at step 24800 for last 100 steps: 0.523783653378\n",
      "Average loss at step 24900 for last 100 steps: 0.519579102993\n",
      "Average loss at step 25000 for last 100 steps: 0.521856354475\n",
      "Average loss at step 25100 for last 100 steps: 0.533253111839\n",
      "Average loss at step 25200 for last 100 steps: 0.524891071022\n",
      "Average loss at step 25300 for last 100 steps: 0.52226084888\n",
      "Average loss at step 25400 for last 100 steps: 0.508335282505\n",
      "Average loss at step 25500 for last 100 steps: 0.526789430678\n",
      "Average loss at step 25600 for last 100 steps: 0.512375057638\n",
      "Average loss at step 25700 for last 100 steps: 0.519501024187\n",
      "Average loss at step 25800 for last 100 steps: 0.534857195914\n",
      "Average loss at step 25900 for last 100 steps: 0.509886428714\n",
      "Average loss at step 26000 for last 100 steps: 0.512004002929\n",
      "Average loss at step 26100 for last 100 steps: 0.52139941901\n",
      "Average loss at step 26200 for last 100 steps: 0.530609808564\n",
      "Average loss at step 26300 for last 100 steps: 0.519134411812\n",
      "Average loss at step 26400 for last 100 steps: 0.521922720373\n",
      "Average loss at step 26500 for last 100 steps: 0.517188743353\n",
      "Average loss at step 26600 for last 100 steps: 0.525493891537\n",
      "Average loss at step 26700 for last 100 steps: 0.528636051416\n",
      "Average loss at step 26800 for last 100 steps: 0.51044721961\n",
      "Average loss at step 26900 for last 100 steps: 0.526421425045\n",
      "Average loss at step 27000 for last 100 steps: 0.51403231144\n",
      "Average loss at step 27100 for last 100 steps: 0.515578089654\n",
      "Average loss at step 27200 for last 100 steps: 0.499092662632\n",
      "Average loss at step 27300 for last 100 steps: 0.523442041576\n",
      "Average loss at step 27400 for last 100 steps: 0.522419927716\n",
      "Average loss at step 27500 for last 100 steps: 0.535492752492\n",
      "Average loss at step 27600 for last 100 steps: 0.499416761398\n",
      "Average loss at step 27700 for last 100 steps: 0.535770824552\n",
      "Average loss at step 27800 for last 100 steps: 0.508568458855\n",
      "Average loss at step 27900 for last 100 steps: 0.520491205752\n",
      "Average loss at step 28000 for last 100 steps: 0.516211728156\n",
      "Average loss at step 28100 for last 100 steps: 0.508031285703\n",
      "Average loss at step 28200 for last 100 steps: 0.526005069911\n",
      "Average loss at step 28300 for last 100 steps: 0.525071282983\n",
      "Average loss at step 28400 for last 100 steps: 0.508272545934\n",
      "Average loss at step 28500 for last 100 steps: 0.514220011234\n",
      "Average loss at step 28600 for last 100 steps: 0.522033253014\n",
      "Average loss at step 28700 for last 100 steps: 0.513571439683\n",
      "Average loss at step 28800 for last 100 steps: 0.497937500179\n",
      "Average loss at step 28900 for last 100 steps: 0.527007602453\n",
      "Average loss at step 29000 for last 100 steps: 0.50803929925\n",
      "Average loss at step 29100 for last 100 steps: 0.527688415647\n",
      "Average loss at step 29200 for last 100 steps: 0.520220342875\n",
      "Average loss at step 29300 for last 100 steps: 0.522671173811\n",
      "Average loss at step 29400 for last 100 steps: 0.514600031078\n",
      "Average loss at step 29500 for last 100 steps: 0.521361727118\n",
      "Average loss at step 29600 for last 100 steps: 0.504261862338\n",
      "Average loss at step 29700 for last 100 steps: 0.510640742779\n",
      "Average loss at step 29800 for last 100 steps: 0.513537335694\n",
      "Average loss at step 29900 for last 100 steps: 0.504325166345\n",
      "Average loss at step 30000 for last 100 steps: 0.525627246201\n",
      "Average loss at step 30100 for last 100 steps: 0.513271737695\n",
      "Average loss at step 30200 for last 100 steps: 0.512614734471\n",
      "Average loss at step 30300 for last 100 steps: 0.530896039307\n",
      "Average loss at step 30400 for last 100 steps: 0.513208386004\n",
      "Average loss at step 30500 for last 100 steps: 0.535509181619\n",
      "Average loss at step 30600 for last 100 steps: 0.531013044119\n",
      "Average loss at step 30700 for last 100 steps: 0.527754875124\n",
      "Average loss at step 30800 for last 100 steps: 0.521260844767\n",
      "Average loss at step 30900 for last 100 steps: 0.502511195838\n",
      "Average loss at step 31000 for last 100 steps: 0.50726013869\n",
      "Average loss at step 31100 for last 100 steps: 0.515342002809\n",
      "Average loss at step 31200 for last 100 steps: 0.517328231633\n",
      "Average loss at step 31300 for last 100 steps: 0.513369760811\n",
      "Average loss at step 31400 for last 100 steps: 0.517746180892\n",
      "Average loss at step 31500 for last 100 steps: 0.509582116008\n",
      "Average loss at step 31600 for last 100 steps: 0.530936140418\n",
      "Average loss at step 31700 for last 100 steps: 0.543136042655\n",
      "Average loss at step 31800 for last 100 steps: 0.518647586405\n",
      "Average loss at step 31900 for last 100 steps: 0.536403642297\n",
      "Average loss at step 32000 for last 100 steps: 0.529088060558\n",
      "Average loss at step 32100 for last 100 steps: 0.508311170638\n",
      "Average loss at step 32200 for last 100 steps: 0.536363118589\n",
      "Average loss at step 32300 for last 100 steps: 0.527885265648\n",
      "Average loss at step 32400 for last 100 steps: 0.524894445539\n",
      "Average loss at step 32500 for last 100 steps: 0.518507098854\n",
      "Average loss at step 32600 for last 100 steps: 0.518554143012\n",
      "Average loss at step 32700 for last 100 steps: 0.519965047836\n",
      "Average loss at step 32800 for last 100 steps: 0.512695692778\n",
      "Average loss at step 32900 for last 100 steps: 0.517775295377\n",
      "Average loss at step 33000 for last 100 steps: 0.522677409053\n",
      "Average loss at step 33100 for last 100 steps: 0.527270676792\n",
      "Average loss at step 33200 for last 100 steps: 0.516231898963\n",
      "Average loss at step 33300 for last 100 steps: 0.526526250243\n",
      "\n",
      "EPOCH 3\n",
      "Average loss at step 100 for last 100 steps: 0.541795795858\n",
      "Average loss at step 200 for last 100 steps: 0.521597742736\n",
      "Average loss at step 300 for last 100 steps: 0.519794943035\n",
      "Average loss at step 400 for last 100 steps: 0.518129504323\n",
      "Average loss at step 500 for last 100 steps: 0.530466053486\n",
      "Average loss at step 600 for last 100 steps: 0.505711514652\n",
      "Average loss at step 700 for last 100 steps: 0.519724913836\n",
      "Average loss at step 800 for last 100 steps: 0.516939392686\n",
      "Average loss at step 900 for last 100 steps: 0.496107884943\n",
      "Average loss at step 1000 for last 100 steps: 0.50916366607\n",
      "Average loss at step 1100 for last 100 steps: 0.518411892951\n",
      "Average loss at step 1200 for last 100 steps: 0.516022027731\n",
      "Average loss at step 1300 for last 100 steps: 0.522385112941\n",
      "Average loss at step 1400 for last 100 steps: 0.517486026883\n",
      "Average loss at step 1500 for last 100 steps: 0.51758910954\n",
      "Average loss at step 1600 for last 100 steps: 0.527678553164\n",
      "Average loss at step 1700 for last 100 steps: 0.525124300122\n",
      "Average loss at step 1800 for last 100 steps: 0.512240160406\n",
      "Average loss at step 1900 for last 100 steps: 0.510219552219\n",
      "Average loss at step 2000 for last 100 steps: 0.506101168394\n",
      "Average loss at step 2100 for last 100 steps: 0.508061469793\n",
      "Average loss at step 2200 for last 100 steps: 0.516424464881\n",
      "Average loss at step 2300 for last 100 steps: 0.517936826944\n",
      "Average loss at step 2400 for last 100 steps: 0.51764377743\n",
      "Average loss at step 2500 for last 100 steps: 0.535087376237\n",
      "Average loss at step 2600 for last 100 steps: 0.517685399354\n",
      "Average loss at step 2700 for last 100 steps: 0.523777714968\n",
      "Average loss at step 2800 for last 100 steps: 0.520003954768\n",
      "Average loss at step 2900 for last 100 steps: 0.515847687125\n",
      "Average loss at step 3000 for last 100 steps: 0.52798002243\n",
      "Average loss at step 3100 for last 100 steps: 0.5279016608\n",
      "Average loss at step 3200 for last 100 steps: 0.518001857698\n",
      "Average loss at step 3300 for last 100 steps: 0.510330076218\n",
      "Average loss at step 3400 for last 100 steps: 0.516416360438\n",
      "Average loss at step 3500 for last 100 steps: 0.516316161752\n",
      "Average loss at step 3600 for last 100 steps: 0.519144974947\n",
      "Average loss at step 3700 for last 100 steps: 0.508540788591\n",
      "Average loss at step 3800 for last 100 steps: 0.514440064728\n",
      "Average loss at step 3900 for last 100 steps: 0.532519755363\n",
      "Average loss at step 4000 for last 100 steps: 0.52915746361\n",
      "Average loss at step 4100 for last 100 steps: 0.523581491709\n",
      "Average loss at step 4200 for last 100 steps: 0.527914259434\n",
      "Average loss at step 4300 for last 100 steps: 0.521519481838\n",
      "Average loss at step 4400 for last 100 steps: 0.524295540154\n",
      "Average loss at step 4500 for last 100 steps: 0.529956977665\n",
      "Average loss at step 4600 for last 100 steps: 0.51328361541\n",
      "Average loss at step 4700 for last 100 steps: 0.516321704686\n",
      "Average loss at step 4800 for last 100 steps: 0.504091813266\n",
      "Average loss at step 4900 for last 100 steps: 0.512150252759\n",
      "Average loss at step 5000 for last 100 steps: 0.534634074867\n",
      "Average loss at step 5100 for last 100 steps: 0.505598393679\n",
      "Average loss at step 5200 for last 100 steps: 0.513386606872\n",
      "Average loss at step 5300 for last 100 steps: 0.525464535356\n",
      "Average loss at step 5400 for last 100 steps: 0.525141424537\n",
      "Average loss at step 5500 for last 100 steps: 0.521289715767\n",
      "Average loss at step 5600 for last 100 steps: 0.513481624424\n",
      "Average loss at step 5700 for last 100 steps: 0.53083461225\n",
      "Average loss at step 5800 for last 100 steps: 0.528440754116\n",
      "Average loss at step 5900 for last 100 steps: 0.535467661321\n",
      "Average loss at step 6000 for last 100 steps: 0.523559263647\n",
      "Average loss at step 6100 for last 100 steps: 0.517699441314\n",
      "Average loss at step 6200 for last 100 steps: 0.519656822979\n",
      "Average loss at step 6300 for last 100 steps: 0.515518483222\n",
      "Average loss at step 6400 for last 100 steps: 0.519694640636\n",
      "Average loss at step 6500 for last 100 steps: 0.509327012002\n",
      "Average loss at step 6600 for last 100 steps: 0.512012320161\n",
      "Average loss at step 6700 for last 100 steps: 0.522905319035\n",
      "Average loss at step 6800 for last 100 steps: 0.528251656592\n",
      "Average loss at step 6900 for last 100 steps: 0.502843672335\n",
      "Average loss at step 7000 for last 100 steps: 0.518700304031\n",
      "Average loss at step 7100 for last 100 steps: 0.542134620547\n",
      "Average loss at step 7200 for last 100 steps: 0.508970969617\n",
      "Average loss at step 7300 for last 100 steps: 0.535179249942\n",
      "Average loss at step 7400 for last 100 steps: 0.496269095838\n",
      "Average loss at step 7500 for last 100 steps: 0.522351085544\n",
      "Average loss at step 7600 for last 100 steps: 0.515118882358\n",
      "Average loss at step 7700 for last 100 steps: 0.531081071496\n",
      "Average loss at step 7800 for last 100 steps: 0.51142925024\n",
      "Average loss at step 7900 for last 100 steps: 0.520550270081\n",
      "Average loss at step 8000 for last 100 steps: 0.537377517521\n",
      "Average loss at step 8100 for last 100 steps: 0.533074729443\n",
      "Average loss at step 8200 for last 100 steps: 0.5183666417\n",
      "Average loss at step 8300 for last 100 steps: 0.524840297699\n",
      "Average loss at step 8400 for last 100 steps: 0.521007580757\n",
      "Average loss at step 8500 for last 100 steps: 0.536257359684\n",
      "Average loss at step 8600 for last 100 steps: 0.529838015735\n",
      "Average loss at step 8700 for last 100 steps: 0.509763616025\n",
      "Average loss at step 8800 for last 100 steps: 0.517469930053\n",
      "Average loss at step 8900 for last 100 steps: 0.521517736614\n",
      "Average loss at step 9000 for last 100 steps: 0.533087334037\n",
      "Average loss at step 9100 for last 100 steps: 0.521333588958\n",
      "Average loss at step 9200 for last 100 steps: 0.508856478333\n",
      "Average loss at step 9300 for last 100 steps: 0.516339752972\n",
      "Average loss at step 9400 for last 100 steps: 0.513935258687\n",
      "Average loss at step 9500 for last 100 steps: 0.526416845024\n",
      "Average loss at step 9600 for last 100 steps: 0.517679302096\n",
      "Average loss at step 9700 for last 100 steps: 0.524055610895\n",
      "Average loss at step 9800 for last 100 steps: 0.535036490262\n",
      "Average loss at step 9900 for last 100 steps: 0.509421394765\n",
      "Average loss at step 10000 for last 100 steps: 0.511970238686\n",
      "Average loss at step 10100 for last 100 steps: 0.51961564213\n",
      "Average loss at step 10200 for last 100 steps: 0.511358098388\n",
      "Average loss at step 10300 for last 100 steps: 0.510734607577\n",
      "Average loss at step 10400 for last 100 steps: 0.524287958443\n",
      "Average loss at step 10500 for last 100 steps: 0.524570274055\n",
      "Average loss at step 10600 for last 100 steps: 0.514380207658\n",
      "Average loss at step 10700 for last 100 steps: 0.51638329953\n",
      "Average loss at step 10800 for last 100 steps: 0.510683753192\n",
      "Average loss at step 10900 for last 100 steps: 0.516952157319\n",
      "Average loss at step 11000 for last 100 steps: 0.531428349912\n",
      "Average loss at step 11100 for last 100 steps: 0.512969842851\n",
      "Average loss at step 11200 for last 100 steps: 0.52458779186\n",
      "Average loss at step 11300 for last 100 steps: 0.512912097275\n",
      "Average loss at step 11400 for last 100 steps: 0.512530555427\n",
      "Average loss at step 11500 for last 100 steps: 0.529322004616\n",
      "Average loss at step 11600 for last 100 steps: 0.522306610346\n",
      "Average loss at step 11700 for last 100 steps: 0.530698092878\n",
      "Average loss at step 11800 for last 100 steps: 0.518879234791\n",
      "Average loss at step 11900 for last 100 steps: 0.532935528755\n",
      "Average loss at step 12000 for last 100 steps: 0.524657160938\n",
      "Average loss at step 12100 for last 100 steps: 0.522577489018\n",
      "Average loss at step 12200 for last 100 steps: 0.521494361162\n",
      "Average loss at step 12300 for last 100 steps: 0.512709131837\n",
      "Average loss at step 12400 for last 100 steps: 0.52032586962\n",
      "Average loss at step 12500 for last 100 steps: 0.517657896876\n",
      "Average loss at step 12600 for last 100 steps: 0.518944376409\n",
      "Average loss at step 12700 for last 100 steps: 0.539690796733\n",
      "Average loss at step 12800 for last 100 steps: 0.514123883247\n",
      "Average loss at step 12900 for last 100 steps: 0.528237437606\n",
      "Average loss at step 13000 for last 100 steps: 0.52019446969\n",
      "Average loss at step 13100 for last 100 steps: 0.532006778419\n",
      "Average loss at step 13200 for last 100 steps: 0.523083199859\n",
      "Average loss at step 13300 for last 100 steps: 0.524399860203\n",
      "Average loss at step 13400 for last 100 steps: 0.506054716408\n",
      "Average loss at step 13500 for last 100 steps: 0.520743575096\n",
      "Average loss at step 13600 for last 100 steps: 0.5287370193\n",
      "Average loss at step 13700 for last 100 steps: 0.528094526529\n",
      "Average loss at step 13800 for last 100 steps: 0.516804110706\n",
      "Average loss at step 13900 for last 100 steps: 0.52062214613\n",
      "Average loss at step 14000 for last 100 steps: 0.533450094461\n",
      "Average loss at step 14100 for last 100 steps: 0.514242040813\n",
      "Average loss at step 14200 for last 100 steps: 0.524418096542\n",
      "Average loss at step 14300 for last 100 steps: 0.539740306735\n",
      "Average loss at step 14400 for last 100 steps: 0.517541573644\n",
      "Average loss at step 14500 for last 100 steps: 0.510496222675\n",
      "Average loss at step 14600 for last 100 steps: 0.523728359342\n",
      "Average loss at step 14700 for last 100 steps: 0.519603659511\n",
      "Average loss at step 14800 for last 100 steps: 0.510709830523\n",
      "Average loss at step 14900 for last 100 steps: 0.515784907937\n",
      "Average loss at step 15000 for last 100 steps: 0.511228616834\n",
      "Average loss at step 15100 for last 100 steps: 0.513640800118\n",
      "Average loss at step 15200 for last 100 steps: 0.520873266459\n",
      "Average loss at step 15300 for last 100 steps: 0.532136040926\n",
      "Average loss at step 15400 for last 100 steps: 0.50447553128\n",
      "Average loss at step 15500 for last 100 steps: 0.529265339077\n",
      "Average loss at step 15600 for last 100 steps: 0.523577696085\n",
      "Average loss at step 15700 for last 100 steps: 0.517763508856\n",
      "Average loss at step 15800 for last 100 steps: 0.532613418996\n",
      "Average loss at step 15900 for last 100 steps: 0.512989114225\n",
      "Average loss at step 16000 for last 100 steps: 0.526236723065\n",
      "Average loss at step 16100 for last 100 steps: 0.518950349391\n",
      "Average loss at step 16200 for last 100 steps: 0.524452140927\n",
      "Average loss at step 16300 for last 100 steps: 0.513404767215\n",
      "Average loss at step 16400 for last 100 steps: 0.519528655708\n",
      "Average loss at step 16500 for last 100 steps: 0.504809529781\n",
      "Average loss at step 16600 for last 100 steps: 0.530317548811\n",
      "Average loss at step 16700 for last 100 steps: 0.505832749605\n",
      "Average loss at step 16800 for last 100 steps: 0.516691993177\n",
      "Average loss at step 16900 for last 100 steps: 0.523984619677\n",
      "Average loss at step 17000 for last 100 steps: 0.53114231199\n",
      "Average loss at step 17100 for last 100 steps: 0.515135346353\n",
      "Average loss at step 17200 for last 100 steps: 0.51413672328\n",
      "Average loss at step 17300 for last 100 steps: 0.520200555921\n",
      "Average loss at step 17400 for last 100 steps: 0.524807904363\n",
      "Average loss at step 17500 for last 100 steps: 0.513356055915\n",
      "Average loss at step 17600 for last 100 steps: 0.518155350983\n",
      "Average loss at step 17700 for last 100 steps: 0.507925854921\n",
      "Average loss at step 17800 for last 100 steps: 0.516604999304\n",
      "Average loss at step 17900 for last 100 steps: 0.527694371641\n",
      "Average loss at step 18000 for last 100 steps: 0.519039663076\n",
      "Average loss at step 18100 for last 100 steps: 0.521363011301\n",
      "Average loss at step 18200 for last 100 steps: 0.518981630504\n",
      "Average loss at step 18300 for last 100 steps: 0.511319313049\n",
      "Average loss at step 18400 for last 100 steps: 0.505958923399\n",
      "Average loss at step 18500 for last 100 steps: 0.508980775177\n",
      "Average loss at step 18600 for last 100 steps: 0.513621103168\n",
      "Average loss at step 18700 for last 100 steps: 0.537308397293\n",
      "Average loss at step 18800 for last 100 steps: 0.512659624219\n",
      "Average loss at step 18900 for last 100 steps: 0.515356188118\n",
      "Average loss at step 19000 for last 100 steps: 0.536082471013\n",
      "Average loss at step 19100 for last 100 steps: 0.508796907067\n",
      "Average loss at step 19200 for last 100 steps: 0.504580923319\n",
      "Average loss at step 19300 for last 100 steps: 0.526462545991\n",
      "Average loss at step 19400 for last 100 steps: 0.534046297073\n",
      "Average loss at step 19500 for last 100 steps: 0.524333259165\n",
      "Average loss at step 19600 for last 100 steps: 0.501586101949\n",
      "Average loss at step 19700 for last 100 steps: 0.513167507648\n",
      "Average loss at step 19800 for last 100 steps: 0.510861672759\n",
      "Average loss at step 19900 for last 100 steps: 0.509870955348\n",
      "Average loss at step 20000 for last 100 steps: 0.528904606104\n",
      "Average loss at step 20100 for last 100 steps: 0.511142737865\n",
      "Average loss at step 20200 for last 100 steps: 0.525417469144\n",
      "Average loss at step 20300 for last 100 steps: 0.525303693712\n",
      "Average loss at step 20400 for last 100 steps: 0.532364436388\n",
      "Average loss at step 20500 for last 100 steps: 0.517763280272\n",
      "Average loss at step 20600 for last 100 steps: 0.51337434411\n",
      "Average loss at step 20700 for last 100 steps: 0.508642858565\n",
      "Average loss at step 20800 for last 100 steps: 0.521456792951\n",
      "Average loss at step 20900 for last 100 steps: 0.520040232539\n",
      "Average loss at step 21000 for last 100 steps: 0.52586877346\n",
      "Average loss at step 21100 for last 100 steps: 0.525820353627\n",
      "Average loss at step 21200 for last 100 steps: 0.533226921856\n",
      "Average loss at step 21300 for last 100 steps: 0.518726819456\n",
      "Average loss at step 21400 for last 100 steps: 0.527413969338\n",
      "Average loss at step 21500 for last 100 steps: 0.520883174241\n",
      "Average loss at step 21600 for last 100 steps: 0.512415973246\n",
      "Average loss at step 21700 for last 100 steps: 0.531674031019\n",
      "Average loss at step 21800 for last 100 steps: 0.504452852309\n",
      "Average loss at step 21900 for last 100 steps: 0.509518229067\n",
      "Average loss at step 22000 for last 100 steps: 0.521011082232\n",
      "Average loss at step 22100 for last 100 steps: 0.508273400664\n",
      "Average loss at step 22200 for last 100 steps: 0.516529191136\n",
      "Average loss at step 22300 for last 100 steps: 0.500340565443\n",
      "Average loss at step 22400 for last 100 steps: 0.521945121884\n",
      "Average loss at step 22500 for last 100 steps: 0.514889684021\n",
      "Average loss at step 22600 for last 100 steps: 0.527695433795\n",
      "Average loss at step 22700 for last 100 steps: 0.512351833284\n",
      "Average loss at step 22800 for last 100 steps: 0.521035247296\n",
      "Average loss at step 22900 for last 100 steps: 0.53016438365\n",
      "Average loss at step 23000 for last 100 steps: 0.519109429121\n",
      "Average loss at step 23100 for last 100 steps: 0.535314500034\n",
      "Average loss at step 23200 for last 100 steps: 0.511632868946\n",
      "Average loss at step 23300 for last 100 steps: 0.518483180404\n",
      "Average loss at step 23400 for last 100 steps: 0.495924256444\n",
      "Average loss at step 23500 for last 100 steps: 0.51283754617\n",
      "Average loss at step 23600 for last 100 steps: 0.521287740767\n",
      "Average loss at step 23700 for last 100 steps: 0.509500159621\n",
      "Average loss at step 23800 for last 100 steps: 0.511025517881\n",
      "Average loss at step 23900 for last 100 steps: 0.507981924415\n",
      "Average loss at step 24000 for last 100 steps: 0.50990329355\n",
      "Average loss at step 24100 for last 100 steps: 0.519647420049\n",
      "Average loss at step 24200 for last 100 steps: 0.513497231305\n",
      "Average loss at step 24300 for last 100 steps: 0.4990952003\n",
      "Average loss at step 24400 for last 100 steps: 0.516074458957\n",
      "Average loss at step 24500 for last 100 steps: 0.516316973567\n",
      "Average loss at step 24600 for last 100 steps: 0.494921044409\n",
      "Average loss at step 24700 for last 100 steps: 0.513501718938\n",
      "Average loss at step 24800 for last 100 steps: 0.503514614701\n",
      "Average loss at step 24900 for last 100 steps: 0.514747336507\n",
      "Average loss at step 25000 for last 100 steps: 0.496736907959\n",
      "Average loss at step 25100 for last 100 steps: 0.522118443251\n",
      "Average loss at step 25200 for last 100 steps: 0.529615393579\n",
      "Average loss at step 25300 for last 100 steps: 0.523988075256\n",
      "Average loss at step 25400 for last 100 steps: 0.534800929129\n",
      "Average loss at step 25500 for last 100 steps: 0.519046551585\n",
      "Average loss at step 25600 for last 100 steps: 0.509673272669\n",
      "Average loss at step 25700 for last 100 steps: 0.517832641006\n",
      "Average loss at step 25800 for last 100 steps: 0.514906918108\n",
      "Average loss at step 25900 for last 100 steps: 0.521670867503\n",
      "Average loss at step 26000 for last 100 steps: 0.510081934035\n",
      "Average loss at step 26100 for last 100 steps: 0.522736772597\n",
      "Average loss at step 26200 for last 100 steps: 0.521289513409\n",
      "Average loss at step 26300 for last 100 steps: 0.495791225433\n",
      "Average loss at step 26400 for last 100 steps: 0.519094795585\n",
      "Average loss at step 26500 for last 100 steps: 0.530381326973\n",
      "Average loss at step 26600 for last 100 steps: 0.503658716977\n",
      "Average loss at step 26700 for last 100 steps: 0.50786131531\n",
      "Average loss at step 26800 for last 100 steps: 0.509441373348\n",
      "Average loss at step 26900 for last 100 steps: 0.506497243941\n",
      "Average loss at step 27000 for last 100 steps: 0.509562736452\n",
      "Average loss at step 27100 for last 100 steps: 0.505311479568\n",
      "Average loss at step 27200 for last 100 steps: 0.526696928144\n",
      "Average loss at step 27300 for last 100 steps: 0.514909530878\n",
      "Average loss at step 27400 for last 100 steps: 0.511341632307\n",
      "Average loss at step 27500 for last 100 steps: 0.51190269202\n",
      "Average loss at step 27600 for last 100 steps: 0.533698247373\n",
      "Average loss at step 27700 for last 100 steps: 0.519312889576\n",
      "Average loss at step 27800 for last 100 steps: 0.518214156628\n",
      "Average loss at step 27900 for last 100 steps: 0.506326372027\n",
      "Average loss at step 28000 for last 100 steps: 0.524637928009\n",
      "Average loss at step 28100 for last 100 steps: 0.512510788441\n",
      "Average loss at step 28200 for last 100 steps: 0.505454752743\n",
      "Average loss at step 28300 for last 100 steps: 0.505965849757\n",
      "Average loss at step 28400 for last 100 steps: 0.514806883633\n",
      "Average loss at step 28500 for last 100 steps: 0.519818101823\n",
      "Average loss at step 28600 for last 100 steps: 0.517556512654\n",
      "Average loss at step 28700 for last 100 steps: 0.518408540785\n",
      "Average loss at step 28800 for last 100 steps: 0.527913864851\n",
      "Average loss at step 28900 for last 100 steps: 0.519194884896\n",
      "Average loss at step 29000 for last 100 steps: 0.511915511191\n",
      "Average loss at step 29100 for last 100 steps: 0.535741504431\n",
      "Average loss at step 29200 for last 100 steps: 0.515078034699\n",
      "Average loss at step 29300 for last 100 steps: 0.523517899811\n",
      "Average loss at step 29400 for last 100 steps: 0.517459624708\n",
      "Average loss at step 29500 for last 100 steps: 0.51306866765\n",
      "Average loss at step 29600 for last 100 steps: 0.53036813885\n",
      "Average loss at step 29700 for last 100 steps: 0.504440870285\n",
      "Average loss at step 29800 for last 100 steps: 0.510624540746\n",
      "Average loss at step 29900 for last 100 steps: 0.520365368724\n",
      "Average loss at step 30000 for last 100 steps: 0.501658556759\n",
      "Average loss at step 30100 for last 100 steps: 0.518270822167\n",
      "Average loss at step 30200 for last 100 steps: 0.516303417385\n",
      "Average loss at step 30300 for last 100 steps: 0.535080962479\n",
      "Average loss at step 30400 for last 100 steps: 0.519515805244\n",
      "Average loss at step 30500 for last 100 steps: 0.545409847796\n",
      "Average loss at step 30600 for last 100 steps: 0.519541009963\n",
      "Average loss at step 30700 for last 100 steps: 0.510842966735\n",
      "Average loss at step 30800 for last 100 steps: 0.523583973944\n",
      "Average loss at step 30900 for last 100 steps: 0.525541612208\n",
      "Average loss at step 31000 for last 100 steps: 0.521634870768\n",
      "Average loss at step 31100 for last 100 steps: 0.521796168089\n",
      "Average loss at step 31200 for last 100 steps: 0.522742545605\n",
      "Average loss at step 31300 for last 100 steps: 0.497851483822\n",
      "Average loss at step 31400 for last 100 steps: 0.532107650042\n",
      "Average loss at step 31500 for last 100 steps: 0.512292937636\n",
      "Average loss at step 31600 for last 100 steps: 0.514641019702\n",
      "Average loss at step 31700 for last 100 steps: 0.513494845033\n",
      "Average loss at step 31800 for last 100 steps: 0.504659847617\n",
      "Average loss at step 31900 for last 100 steps: 0.511202975512\n",
      "Average loss at step 32000 for last 100 steps: 0.520634402037\n",
      "Average loss at step 32100 for last 100 steps: 0.515354364514\n",
      "Average loss at step 32200 for last 100 steps: 0.516294778883\n",
      "Average loss at step 32300 for last 100 steps: 0.512531516254\n",
      "Average loss at step 32400 for last 100 steps: 0.507485010326\n",
      "Average loss at step 32500 for last 100 steps: 0.514532738626\n",
      "Average loss at step 32600 for last 100 steps: 0.533237829208\n",
      "Average loss at step 32700 for last 100 steps: 0.519365959764\n",
      "Average loss at step 32800 for last 100 steps: 0.520032691658\n",
      "Average loss at step 32900 for last 100 steps: 0.516552511156\n",
      "Average loss at step 33000 for last 100 steps: 0.515562322736\n",
      "Average loss at step 33100 for last 100 steps: 0.512157664001\n",
      "Average loss at step 33200 for last 100 steps: 0.521068894863\n",
      "Average loss at step 33300 for last 100 steps: 0.526258351803\n",
      "\n",
      "EPOCH 4\n",
      "Average loss at step 100 for last 100 steps: 0.520047302246\n",
      "Average loss at step 200 for last 100 steps: 0.523255420327\n",
      "Average loss at step 300 for last 100 steps: 0.514205850363\n",
      "Average loss at step 400 for last 100 steps: 0.514184891284\n",
      "Average loss at step 500 for last 100 steps: 0.514632629752\n",
      "Average loss at step 600 for last 100 steps: 0.525599583387\n",
      "Average loss at step 700 for last 100 steps: 0.510354215801\n",
      "Average loss at step 800 for last 100 steps: 0.507003243268\n",
      "Average loss at step 900 for last 100 steps: 0.508233503997\n",
      "Average loss at step 1000 for last 100 steps: 0.522916321456\n",
      "Average loss at step 1100 for last 100 steps: 0.536020965874\n",
      "Average loss at step 1200 for last 100 steps: 0.523782484233\n",
      "Average loss at step 1300 for last 100 steps: 0.511721845865\n",
      "Average loss at step 1400 for last 100 steps: 0.527452583909\n",
      "Average loss at step 1500 for last 100 steps: 0.533408804834\n",
      "Average loss at step 1600 for last 100 steps: 0.531899659038\n",
      "Average loss at step 1700 for last 100 steps: 0.516510581374\n",
      "Average loss at step 1800 for last 100 steps: 0.50990018487\n",
      "Average loss at step 1900 for last 100 steps: 0.511352601051\n",
      "Average loss at step 2000 for last 100 steps: 0.504904432595\n",
      "Average loss at step 2100 for last 100 steps: 0.524076176584\n",
      "Average loss at step 2200 for last 100 steps: 0.522294482589\n",
      "Average loss at step 2300 for last 100 steps: 0.529826094806\n",
      "Average loss at step 2400 for last 100 steps: 0.515653821826\n",
      "Average loss at step 2500 for last 100 steps: 0.505939148962\n",
      "Average loss at step 2600 for last 100 steps: 0.514356305003\n",
      "Average loss at step 2700 for last 100 steps: 0.518620241582\n",
      "Average loss at step 2800 for last 100 steps: 0.521796922684\n",
      "Average loss at step 2900 for last 100 steps: 0.512284832001\n",
      "Average loss at step 3000 for last 100 steps: 0.509479460418\n",
      "Average loss at step 3100 for last 100 steps: 0.51140825808\n",
      "Average loss at step 3200 for last 100 steps: 0.514474652112\n",
      "Average loss at step 3300 for last 100 steps: 0.523328637183\n",
      "Average loss at step 3400 for last 100 steps: 0.514402605891\n",
      "Average loss at step 3500 for last 100 steps: 0.513496486545\n",
      "Average loss at step 3600 for last 100 steps: 0.525368149281\n",
      "Average loss at step 3700 for last 100 steps: 0.504151692986\n",
      "Average loss at step 3800 for last 100 steps: 0.511728853583\n",
      "Average loss at step 3900 for last 100 steps: 0.491676173508\n",
      "Average loss at step 4000 for last 100 steps: 0.509758530259\n",
      "Average loss at step 4100 for last 100 steps: 0.528026729524\n",
      "Average loss at step 4200 for last 100 steps: 0.512643676698\n",
      "Average loss at step 4300 for last 100 steps: 0.510110220015\n",
      "Average loss at step 4400 for last 100 steps: 0.516551533639\n",
      "Average loss at step 4500 for last 100 steps: 0.502884739041\n",
      "Average loss at step 4600 for last 100 steps: 0.524407645762\n",
      "Average loss at step 4700 for last 100 steps: 0.512737208307\n",
      "Average loss at step 4800 for last 100 steps: 0.507669718862\n",
      "Average loss at step 4900 for last 100 steps: 0.5029662925\n",
      "Average loss at step 5000 for last 100 steps: 0.519361194372\n",
      "Average loss at step 5100 for last 100 steps: 0.529923945069\n",
      "Average loss at step 5200 for last 100 steps: 0.517595357001\n",
      "Average loss at step 5300 for last 100 steps: 0.520089063942\n",
      "Average loss at step 5400 for last 100 steps: 0.52300960958\n",
      "Average loss at step 5500 for last 100 steps: 0.5142472893\n",
      "Average loss at step 5600 for last 100 steps: 0.514282218516\n",
      "Average loss at step 5700 for last 100 steps: 0.51858225137\n",
      "Average loss at step 5800 for last 100 steps: 0.527864225805\n",
      "Average loss at step 5900 for last 100 steps: 0.523621515334\n",
      "Average loss at step 6000 for last 100 steps: 0.525545490682\n",
      "Average loss at step 6100 for last 100 steps: 0.50913503617\n",
      "Average loss at step 6200 for last 100 steps: 0.504412663877\n",
      "Average loss at step 6300 for last 100 steps: 0.524716421962\n",
      "Average loss at step 6400 for last 100 steps: 0.524649549127\n",
      "Average loss at step 6500 for last 100 steps: 0.516746329069\n",
      "Average loss at step 6600 for last 100 steps: 0.501005910337\n",
      "Average loss at step 6700 for last 100 steps: 0.524453837574\n",
      "Average loss at step 6800 for last 100 steps: 0.525716301799\n",
      "Average loss at step 6900 for last 100 steps: 0.516047860682\n",
      "Average loss at step 7000 for last 100 steps: 0.514024091363\n",
      "Average loss at step 7100 for last 100 steps: 0.515276255608\n",
      "Average loss at step 7200 for last 100 steps: 0.515728476048\n",
      "Average loss at step 7300 for last 100 steps: 0.522499360144\n",
      "Average loss at step 7400 for last 100 steps: 0.503557922542\n",
      "Average loss at step 7500 for last 100 steps: 0.520579247475\n",
      "Average loss at step 7600 for last 100 steps: 0.50403940469\n",
      "Average loss at step 7700 for last 100 steps: 0.50387910217\n",
      "Average loss at step 7800 for last 100 steps: 0.513373515308\n",
      "Average loss at step 7900 for last 100 steps: 0.513691130579\n",
      "Average loss at step 8000 for last 100 steps: 0.512129985988\n",
      "Average loss at step 8100 for last 100 steps: 0.521615062654\n",
      "Average loss at step 8200 for last 100 steps: 0.516158613861\n",
      "Average loss at step 8300 for last 100 steps: 0.504936356843\n",
      "Average loss at step 8400 for last 100 steps: 0.524093555212\n",
      "Average loss at step 8500 for last 100 steps: 0.520095401406\n",
      "Average loss at step 8600 for last 100 steps: 0.500849219263\n",
      "Average loss at step 8700 for last 100 steps: 0.517493283749\n",
      "Average loss at step 8800 for last 100 steps: 0.522517709732\n",
      "Average loss at step 8900 for last 100 steps: 0.52319335103\n",
      "Average loss at step 9000 for last 100 steps: 0.523349740505\n",
      "Average loss at step 9100 for last 100 steps: 0.52062045604\n",
      "Average loss at step 9200 for last 100 steps: 0.524220660329\n",
      "Average loss at step 9300 for last 100 steps: 0.51684579581\n",
      "Average loss at step 9400 for last 100 steps: 0.513641875684\n",
      "Average loss at step 9500 for last 100 steps: 0.516447304785\n",
      "Average loss at step 9600 for last 100 steps: 0.512951276004\n",
      "Average loss at step 9700 for last 100 steps: 0.525326438844\n",
      "Average loss at step 9800 for last 100 steps: 0.503435820043\n",
      "Average loss at step 9900 for last 100 steps: 0.526597486734\n",
      "Average loss at step 10000 for last 100 steps: 0.516560294628\n",
      "Average loss at step 10100 for last 100 steps: 0.514392562509\n",
      "Average loss at step 10200 for last 100 steps: 0.530734237432\n",
      "Average loss at step 10300 for last 100 steps: 0.51048050642\n",
      "Average loss at step 10400 for last 100 steps: 0.519334943593\n",
      "Average loss at step 10500 for last 100 steps: 0.51757607162\n",
      "Average loss at step 10600 for last 100 steps: 0.516923721731\n",
      "Average loss at step 10700 for last 100 steps: 0.508454729617\n",
      "Average loss at step 10800 for last 100 steps: 0.528487704098\n",
      "Average loss at step 10900 for last 100 steps: 0.509217047691\n",
      "Average loss at step 11000 for last 100 steps: 0.511471294463\n",
      "Average loss at step 11100 for last 100 steps: 0.507211082578\n",
      "Average loss at step 11200 for last 100 steps: 0.520138112903\n",
      "Average loss at step 11300 for last 100 steps: 0.523262702525\n",
      "Average loss at step 11400 for last 100 steps: 0.518594471216\n",
      "Average loss at step 11500 for last 100 steps: 0.514285795689\n",
      "Average loss at step 11600 for last 100 steps: 0.525537723601\n",
      "Average loss at step 11700 for last 100 steps: 0.51213793546\n",
      "Average loss at step 11800 for last 100 steps: 0.516619122624\n",
      "Average loss at step 11900 for last 100 steps: 0.521571534574\n",
      "Average loss at step 12000 for last 100 steps: 0.521174452305\n",
      "Average loss at step 12100 for last 100 steps: 0.525265445113\n",
      "Average loss at step 12200 for last 100 steps: 0.518826843798\n",
      "Average loss at step 12300 for last 100 steps: 0.522322049141\n",
      "Average loss at step 12400 for last 100 steps: 0.534407986104\n",
      "Average loss at step 12500 for last 100 steps: 0.521551977992\n",
      "Average loss at step 12600 for last 100 steps: 0.515328059494\n",
      "Average loss at step 12700 for last 100 steps: 0.513375307024\n",
      "Average loss at step 12800 for last 100 steps: 0.509194175303\n",
      "Average loss at step 12900 for last 100 steps: 0.507049228847\n",
      "Average loss at step 13000 for last 100 steps: 0.509180064499\n",
      "Average loss at step 13100 for last 100 steps: 0.517413226366\n",
      "Average loss at step 13200 for last 100 steps: 0.524594939947\n",
      "Average loss at step 13300 for last 100 steps: 0.519351791441\n",
      "Average loss at step 13400 for last 100 steps: 0.514207282364\n",
      "Average loss at step 13500 for last 100 steps: 0.518643033206\n",
      "Average loss at step 13600 for last 100 steps: 0.519693430066\n",
      "Average loss at step 13700 for last 100 steps: 0.508622986376\n",
      "Average loss at step 13800 for last 100 steps: 0.513391793966\n",
      "Average loss at step 13900 for last 100 steps: 0.516129086316\n",
      "Average loss at step 14000 for last 100 steps: 0.522668061256\n",
      "Average loss at step 14100 for last 100 steps: 0.520781403184\n",
      "Average loss at step 14200 for last 100 steps: 0.508410242796\n",
      "Average loss at step 14300 for last 100 steps: 0.520177929103\n",
      "Average loss at step 14400 for last 100 steps: 0.527961449623\n",
      "Average loss at step 14500 for last 100 steps: 0.500253678262\n",
      "Average loss at step 14600 for last 100 steps: 0.512452382743\n",
      "Average loss at step 14700 for last 100 steps: 0.520279524326\n",
      "Average loss at step 14800 for last 100 steps: 0.51891598314\n",
      "Average loss at step 14900 for last 100 steps: 0.526302749515\n",
      "Average loss at step 15000 for last 100 steps: 0.535347630382\n",
      "Average loss at step 15100 for last 100 steps: 0.522241913378\n",
      "Average loss at step 15200 for last 100 steps: 0.527171293199\n",
      "Average loss at step 15300 for last 100 steps: 0.518371871114\n",
      "Average loss at step 15400 for last 100 steps: 0.513239221573\n",
      "Average loss at step 15500 for last 100 steps: 0.515227545798\n",
      "Average loss at step 15600 for last 100 steps: 0.494515661895\n",
      "Average loss at step 15700 for last 100 steps: 0.508533824533\n",
      "Average loss at step 15800 for last 100 steps: 0.51348120749\n",
      "Average loss at step 15900 for last 100 steps: 0.537499339879\n",
      "Average loss at step 16000 for last 100 steps: 0.526064442396\n",
      "Average loss at step 16100 for last 100 steps: 0.519337310493\n",
      "Average loss at step 16200 for last 100 steps: 0.503277575076\n",
      "Average loss at step 16300 for last 100 steps: 0.52414970696\n",
      "Average loss at step 16400 for last 100 steps: 0.521765289307\n",
      "Average loss at step 16500 for last 100 steps: 0.519773362279\n",
      "Average loss at step 16600 for last 100 steps: 0.50465749234\n",
      "Average loss at step 16700 for last 100 steps: 0.519999231398\n",
      "Average loss at step 16800 for last 100 steps: 0.519155343771\n",
      "Average loss at step 16900 for last 100 steps: 0.525768562257\n",
      "Average loss at step 17000 for last 100 steps: 0.511079382896\n",
      "Average loss at step 17100 for last 100 steps: 0.512128869891\n",
      "Average loss at step 17200 for last 100 steps: 0.514261004925\n",
      "Average loss at step 17300 for last 100 steps: 0.512592877448\n",
      "Average loss at step 17400 for last 100 steps: 0.528117620945\n",
      "Average loss at step 17500 for last 100 steps: 0.507252937853\n",
      "Average loss at step 17600 for last 100 steps: 0.535240420699\n",
      "Average loss at step 17700 for last 100 steps: 0.52282474637\n",
      "Average loss at step 17800 for last 100 steps: 0.509826742113\n",
      "Average loss at step 17900 for last 100 steps: 0.516757738888\n",
      "Average loss at step 18000 for last 100 steps: 0.522410847843\n",
      "Average loss at step 18100 for last 100 steps: 0.524547966719\n",
      "Average loss at step 18200 for last 100 steps: 0.522513481975\n",
      "Average loss at step 18300 for last 100 steps: 0.512706947029\n",
      "Average loss at step 18400 for last 100 steps: 0.525144414902\n",
      "Average loss at step 18500 for last 100 steps: 0.518091574907\n",
      "Average loss at step 18600 for last 100 steps: 0.513298825324\n",
      "Average loss at step 18700 for last 100 steps: 0.535184328258\n",
      "Average loss at step 18800 for last 100 steps: 0.526413320899\n",
      "Average loss at step 18900 for last 100 steps: 0.520050524175\n",
      "Average loss at step 19000 for last 100 steps: 0.52409149617\n",
      "Average loss at step 19100 for last 100 steps: 0.52271669358\n",
      "Average loss at step 19200 for last 100 steps: 0.522279834151\n",
      "Average loss at step 19300 for last 100 steps: 0.522733696103\n",
      "Average loss at step 19400 for last 100 steps: 0.500225700736\n",
      "Average loss at step 19500 for last 100 steps: 0.502902184427\n",
      "Average loss at step 19600 for last 100 steps: 0.501167188883\n",
      "Average loss at step 19700 for last 100 steps: 0.525952898264\n",
      "Average loss at step 19800 for last 100 steps: 0.514496849179\n",
      "Average loss at step 19900 for last 100 steps: 0.513486772776\n",
      "Average loss at step 20000 for last 100 steps: 0.517937212288\n",
      "Average loss at step 20100 for last 100 steps: 0.522344259024\n",
      "Average loss at step 20200 for last 100 steps: 0.532680917084\n",
      "Average loss at step 20300 for last 100 steps: 0.512291485667\n",
      "Average loss at step 20400 for last 100 steps: 0.500216957033\n",
      "Average loss at step 20500 for last 100 steps: 0.522097625434\n",
      "Average loss at step 20600 for last 100 steps: 0.515501508713\n",
      "Average loss at step 20700 for last 100 steps: 0.53309215486\n",
      "Average loss at step 20800 for last 100 steps: 0.526242963374\n",
      "Average loss at step 20900 for last 100 steps: 0.509538871348\n",
      "Average loss at step 21000 for last 100 steps: 0.525491548479\n",
      "Average loss at step 21100 for last 100 steps: 0.510670605302\n",
      "Average loss at step 21200 for last 100 steps: 0.513927535117\n",
      "Average loss at step 21300 for last 100 steps: 0.529120531082\n",
      "Average loss at step 21400 for last 100 steps: 0.51565284133\n",
      "Average loss at step 21500 for last 100 steps: 0.513036144078\n",
      "Average loss at step 21600 for last 100 steps: 0.518393936157\n",
      "Average loss at step 21700 for last 100 steps: 0.510927327871\n",
      "Average loss at step 21800 for last 100 steps: 0.533242534697\n",
      "Average loss at step 21900 for last 100 steps: 0.521603239179\n",
      "Average loss at step 22000 for last 100 steps: 0.517279377878\n",
      "Average loss at step 22100 for last 100 steps: 0.513194419444\n",
      "Average loss at step 22200 for last 100 steps: 0.503806777596\n",
      "Average loss at step 22300 for last 100 steps: 0.5223595801\n",
      "Average loss at step 22400 for last 100 steps: 0.51107042104\n",
      "Average loss at step 22500 for last 100 steps: 0.525814367533\n",
      "Average loss at step 22600 for last 100 steps: 0.51463198483\n",
      "Average loss at step 22700 for last 100 steps: 0.525214637518\n",
      "Average loss at step 22800 for last 100 steps: 0.522454743385\n",
      "Average loss at step 22900 for last 100 steps: 0.508488881588\n",
      "Average loss at step 23000 for last 100 steps: 0.510406448245\n",
      "Average loss at step 23100 for last 100 steps: 0.50479627192\n",
      "Average loss at step 23200 for last 100 steps: 0.502323924601\n",
      "Average loss at step 23300 for last 100 steps: 0.514676012099\n",
      "Average loss at step 23400 for last 100 steps: 0.50953504324\n",
      "Average loss at step 23500 for last 100 steps: 0.521371131241\n",
      "Average loss at step 23600 for last 100 steps: 0.510852997899\n",
      "Average loss at step 23700 for last 100 steps: 0.519129653573\n",
      "Average loss at step 23800 for last 100 steps: 0.531626941562\n",
      "Average loss at step 23900 for last 100 steps: 0.51504286617\n",
      "Average loss at step 24000 for last 100 steps: 0.519549972415\n",
      "Average loss at step 24100 for last 100 steps: 0.523385393918\n",
      "Average loss at step 24200 for last 100 steps: 0.503709319234\n",
      "Average loss at step 24300 for last 100 steps: 0.523572019637\n",
      "Average loss at step 24400 for last 100 steps: 0.51057038933\n",
      "Average loss at step 24500 for last 100 steps: 0.541337008178\n",
      "Average loss at step 24600 for last 100 steps: 0.515065320134\n",
      "Average loss at step 24700 for last 100 steps: 0.525034749508\n",
      "Average loss at step 24800 for last 100 steps: 0.526574693918\n",
      "Average loss at step 24900 for last 100 steps: 0.51623021692\n",
      "Average loss at step 25000 for last 100 steps: 0.517586100101\n",
      "Average loss at step 25100 for last 100 steps: 0.507591213882\n",
      "Average loss at step 25200 for last 100 steps: 0.509811187387\n",
      "Average loss at step 25300 for last 100 steps: 0.528392604589\n",
      "Average loss at step 25400 for last 100 steps: 0.50706569612\n",
      "Average loss at step 25500 for last 100 steps: 0.520426943898\n",
      "Average loss at step 25600 for last 100 steps: 0.533327262104\n",
      "Average loss at step 25700 for last 100 steps: 0.519377346635\n",
      "Average loss at step 25800 for last 100 steps: 0.520886932611\n",
      "Average loss at step 25900 for last 100 steps: 0.516768484116\n",
      "Average loss at step 26000 for last 100 steps: 0.530386921465\n",
      "Average loss at step 26100 for last 100 steps: 0.521269230843\n",
      "Average loss at step 26200 for last 100 steps: 0.518153933287\n",
      "Average loss at step 26300 for last 100 steps: 0.506629240513\n",
      "Average loss at step 26400 for last 100 steps: 0.525381083786\n",
      "Average loss at step 26500 for last 100 steps: 0.531468741298\n",
      "Average loss at step 26600 for last 100 steps: 0.51358002305\n",
      "Average loss at step 26700 for last 100 steps: 0.517717989981\n",
      "Average loss at step 26800 for last 100 steps: 0.511770862341\n",
      "Average loss at step 26900 for last 100 steps: 0.502263171673\n",
      "Average loss at step 27000 for last 100 steps: 0.524775345325\n",
      "Average loss at step 27100 for last 100 steps: 0.510174392164\n",
      "Average loss at step 27200 for last 100 steps: 0.52096452415\n",
      "Average loss at step 27300 for last 100 steps: 0.522261937559\n",
      "Average loss at step 27400 for last 100 steps: 0.523655652702\n",
      "Average loss at step 27500 for last 100 steps: 0.5110743168\n",
      "Average loss at step 27600 for last 100 steps: 0.533957601488\n",
      "Average loss at step 27700 for last 100 steps: 0.503912503719\n",
      "Average loss at step 27800 for last 100 steps: 0.527703234255\n",
      "Average loss at step 27900 for last 100 steps: 0.526762022972\n",
      "Average loss at step 28000 for last 100 steps: 0.513669854105\n",
      "Average loss at step 28100 for last 100 steps: 0.52754722774\n",
      "Average loss at step 28200 for last 100 steps: 0.520244322419\n",
      "Average loss at step 28300 for last 100 steps: 0.498705645204\n",
      "Average loss at step 28400 for last 100 steps: 0.527153433263\n",
      "Average loss at step 28500 for last 100 steps: 0.509080888033\n",
      "Average loss at step 28600 for last 100 steps: 0.523543086648\n",
      "Average loss at step 28700 for last 100 steps: 0.527927344143\n",
      "Average loss at step 28800 for last 100 steps: 0.513131743968\n",
      "Average loss at step 28900 for last 100 steps: 0.506464575827\n",
      "Average loss at step 29000 for last 100 steps: 0.514889602661\n",
      "Average loss at step 29100 for last 100 steps: 0.522951909602\n",
      "Average loss at step 29200 for last 100 steps: 0.51865000993\n",
      "Average loss at step 29300 for last 100 steps: 0.522266148329\n",
      "Average loss at step 29400 for last 100 steps: 0.510585440695\n",
      "Average loss at step 29500 for last 100 steps: 0.520609425902\n",
      "Average loss at step 29600 for last 100 steps: 0.506869345903\n",
      "Average loss at step 29700 for last 100 steps: 0.525328123271\n",
      "Average loss at step 29800 for last 100 steps: 0.521981259584\n",
      "Average loss at step 29900 for last 100 steps: 0.526465974152\n",
      "Average loss at step 30000 for last 100 steps: 0.509933850467\n",
      "Average loss at step 30100 for last 100 steps: 0.505490261018\n",
      "Average loss at step 30200 for last 100 steps: 0.532001693547\n",
      "Average loss at step 30300 for last 100 steps: 0.513275440633\n",
      "Average loss at step 30400 for last 100 steps: 0.521773761213\n",
      "Average loss at step 30500 for last 100 steps: 0.521741927862\n",
      "Average loss at step 30600 for last 100 steps: 0.511619128883\n",
      "Average loss at step 30700 for last 100 steps: 0.50936791569\n",
      "Average loss at step 30800 for last 100 steps: 0.521029316783\n",
      "Average loss at step 30900 for last 100 steps: 0.522964453697\n",
      "Average loss at step 31000 for last 100 steps: 0.519125059545\n",
      "Average loss at step 31100 for last 100 steps: 0.502910514176\n",
      "Average loss at step 31200 for last 100 steps: 0.502839078605\n",
      "Average loss at step 31300 for last 100 steps: 0.533668427765\n",
      "Average loss at step 31400 for last 100 steps: 0.51142284587\n",
      "Average loss at step 31500 for last 100 steps: 0.519678409696\n",
      "Average loss at step 31600 for last 100 steps: 0.508812893927\n",
      "Average loss at step 31700 for last 100 steps: 0.526733789444\n",
      "Average loss at step 31800 for last 100 steps: 0.509451960921\n",
      "Average loss at step 31900 for last 100 steps: 0.489836374819\n",
      "Average loss at step 32000 for last 100 steps: 0.514728580117\n",
      "Average loss at step 32100 for last 100 steps: 0.513411294222\n",
      "Average loss at step 32200 for last 100 steps: 0.527639786005\n",
      "Average loss at step 32300 for last 100 steps: 0.511824789345\n",
      "Average loss at step 32400 for last 100 steps: 0.536693231165\n",
      "Average loss at step 32500 for last 100 steps: 0.506658901274\n",
      "Average loss at step 32600 for last 100 steps: 0.513294691443\n",
      "Average loss at step 32700 for last 100 steps: 0.52526788041\n",
      "Average loss at step 32800 for last 100 steps: 0.518728294373\n",
      "Average loss at step 32900 for last 100 steps: 0.516745735109\n",
      "Average loss at step 33000 for last 100 steps: 0.522364743948\n",
      "Average loss at step 33100 for last 100 steps: 0.495906170011\n",
      "Average loss at step 33200 for last 100 steps: 0.524003475904\n",
      "Average loss at step 33300 for last 100 steps: 0.520545088053\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhsAAAFkCAYAAACJu/k0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAIABJREFUeJztnXe8HUXZx39zbxJiKKFEEpASCBBAaQktFAFD74IQAiii\nNEFKEBHpRYryQuhVXkK9UhSEV5qgSEsoiYQOAkF6SAiEEkpy77x/zBnOnD2zuzO7O+fsOff3/Xzu\n556zZ3bKltnfPs8zM0JKCUIIIYSQUHQ0uwKEEEIIaW8oNgghhBASFIoNQgghhASFYoMQQgghQaHY\nIIQQQkhQKDYIIYQQEhSKDUIIIYQEhWKDEEIIIUGh2CCEEEJIUCg2CCGEEBIUig1CCCGEBIVigxBC\nCCFBodgghBBCSFAoNgghhBASlD7NrkBWhBCLAdgKwBsAvmxubQghhJCWoj+AoQDulVJ+GLqwlhUb\nUELjhmZXghBCCGlh9gJwY+hCWllsvAEA119/PVZZZZUmVyU/48aNw/jx45tdjcJge8pLO7UFYHvK\nTDu1BWiv9rz44ovYe++9gcqzNDStLDa+BIBVVlkFI0aMaHZdcjNw4MC2aIeG7Skv7dQWgO0pM+3U\nFqD92lOhIWEIDBAlhBBCSFAoNgghhBASFIoNQgghhASFYqMkjB07ttlVKBS2p7y0U1sAtqfMtFNb\ngPZrTyMRUspm1yETQogRACZPnjy5HQN2CCGEkGBMmTIFI0eOBICRUsopocujZYMQQgghQaHYIIQQ\nQkhQKDYIIYQQEhSKDUIIIYQEhWKDEEIIIUGh2CCEEEJIUCg2CCGEEBIUig1CCCGEBIVigxBCCCFB\nodgghBBCSFAoNgghhBASFIoNQgghhASFYoMQQgghQaHYIIQQQkhQKDYIIYQQEhSKDUIIIYQEhWKD\nEEIIIUGh2CCEEEJIUCg2CCGEEBIUig1CCCGEBIVigxBCCCFBodgghBBCSFAoNgghhBASFIoNQggh\nhASFYoMQQgghQaHYIIQQQkhQKDYIIYQQEhSKDUIIIYQEhWKDEEIIIUGh2CCEEEJIUCg2CCGEEBIU\nig1CCCGEBIVigxBCCCFBodgghBBCSFAoNgghhBASFIoNQgghhASFYoMQQgghQaHYIIQQQkhQKDYI\nIYQQEhSKDUIIIYQEhWKDEEIIIUGh2CCEEEJIUCg2CCGEEBIUig1CCCGEBIVigxBCCCFBodgghBBC\nSFAoNgghhBASFIoNQgghhASFYoMQQgghQckkNoQQhwghpgkhvhBCTBJCrJOQdhMhRE/kr1sIsXgk\n3UAhxMVCiHeFEF8KIV4SQmydpX6EEEIIKQ99fHcQQowBcA6AAwA8AWAcgHuFECtJKWfG7CYBrATg\n0282SPmBkWdfAPcDeB/ALgDeBbAsgI9960cIIYSQcuEtNqDExeVSymsBQAhxEIDtAPwMwB8S9psh\npfwk5refA1gYwPpSyu7Ktjcz1I0QQgghJcPLjVKxQIwE8IDeJqWUUFaJUUm7Ani64iK5TwixQeT3\nHQBMBHCJEOJ9IcSzQojfCiEYU0IIIYS0OL4P80EAOgFMj2yfDmBIzD7vATgQwK5QLpK3ADwohFjT\nSLM8gN0q9dkGwKkAfgXgOM/6EUIIIaRkZHGjeCGlfAXAK8amSUKIYVDumH0q2zqgBMsBFUvJv4UQ\nSwE4CsBpSfmPGzcOAwcOrNk2duxYjB07tqAWEEIIIa1LV1cXurq6arbNnj27oXXwFRszAXQDGBzZ\nPhgquNOVJwBsaHx/D8DXFaGheRHAECFEHynlvLiMxo8fjxEjRngUTQghhPQebC/gU6ZMwciRIxtW\nBy83ipRyLoDJAEbrbUIIUfn+mEdWa0IJDM2jAFaIpBkO4L0koUEIIYSQ8pPFjXIugAlCiMmoDn0d\nAGACAAghzgSwpJRyn8r3wwFMA/A8gP4A9gewGYAtjDwvBXCIEOICABdCDZP9LYDzMtSPEEIIISXC\nW2xIKW8WQgyCCuIcDOBpAFtJKWdUkgwBsLSxSz+oeTmWBDAHwDMARkspHzLyfFsIsRWA8QCmAnin\n8jlpKC0hhBBCWoBMAaJSyksAXBLz276R72cDONshz8cBRIfEEkIIIaTF4TwWhBBCCAkKxQYhhBBC\ngkKxQQghhJCgUGwQQgghJCgUG4QQQggJCsUGIYQQQoJCsUEIIYSQoFBsEEIIISQoFBuEEEIICQrF\nBiGEEEKCQrFBCCGEkKBQbBBCCCEkKBQbhBBCCAkKxQYhhBBCgkKxQQghhJCgUGwQQgghJCgUG4QQ\nQggJCsUGIYQQQoJCsUEIIYSQoFBsEEIIISQoFBuEEEIICQrFBiGEEEKCQrFBCCGEkKBQbBBCCCEk\nKBQbhBBCCAkKxQYhhBBCgkKxQQghhJCgUGwQQgghJCgUG4QQQggJCsUGIYQQQoJCsUEIIYSQoFBs\nEEIIISQoFBuEEEIICQrFBiGEEEKCQrFBCCGEkKBQbBBCCCEkKBQbhBBCCAkKxQYhhBBCgkKxQQgh\nhJCgUGwQQkgv44gjgPnma3YtSG+iT7MrQAghpLGcf36za0B6G7RsEEIIISQoFBukoRxwALDEEs2u\nBSGEkEZCNwppKFde2ewaEEIIaTS0bBBCCCEkKBQbhBBCCAkKxQYJzuuvA+ec0+xakFbnwgs5XJOQ\nVoVio5fzzjvA8OHAzJnhyvjhD4GjjgqXP+kdnHoq8PXXza4FISQLFBu9nFtuAV55BdhgA+Crr8KU\nMW9emHwJIYS0BhQbBADwn/8AEyY0uxaExCNluLxXWw24++5w+RPS26HYIN9ACwQpM1GxMXcuMGoU\n8Nxz+fN+7jngmGPy50MIsZNJbAghDhFCTBNCfCGEmCSEWCch7SZCiJ7IX7cQYvGY9HtU0vzFt17j\nxwPTpvnuRTSh3hxDvpGS3svbbwOTJgFnnllMfkIUkw9pbR57DJgzp9m1aD+8xYYQYgyAcwCcBGAt\nAFMB3CuEGJSwmwSwIoAhlb8lpJQfWPIeCuBsAA/51gsAjjwS2GmnLHv2XkwhQFFAejO8/klPD7Dh\nhsDBBze7Ju1HFsvGOACXSymvlVK+BOAgAHMA/CxlvxlSyg/0X/RHIUQHgOsBnAggs32C0ep+UGz0\nPp5/Hjj99GbXwp/o9cnrlRSNvqZee6259WhHvMSGEKIvgJEAHtDbpJQSwP0ARiXtCuBpIcS7Qoj7\nhBAbWNKcBGC6lPJqnzpFYQeUHR673sE22wDHH9/sWhRHUe4PulGIhn1h8fhaNgYB6AQwPbJ9OpR7\nxMZ7AA4EsCuAXQC8BeBBIcSaOoEQYiMA+wLYz7M+dfSmi6S7W5n9XJk1C7jppvjfffLyoTedE9I4\nirqueH0SDa+FcARfiE1K+QqAV4xNk4QQw6DcMfsIIRYAcC2A/aWUH/nmP27cOAwcOPCb7++8A3R1\njcXYsWNz1rz89Omj5sd49FG39D//OXD77cCuu6p9G8HGGwMvvaQ+Z72RX3sNuPFG4IQTiqtXb6ZV\nO9S4eue1SOh8s+Tz2WdKxC+zjPr+6acqaHWLLfLViTSHVr030ujq6kJXV1fNttmzZze0Dr6PnJkA\nugEMjmwfDOB9j3yeALBh5fMwAMsCuFOIb273DgAQQnwNYLiUMjaGY/z48RgxYgRUerV8eS/QGd/w\n2GPuaWfNUv/jLBghbrRHHsmf/+67A1OmUGz0dso4WmqLLZS40HkceCDQ1aWGkXd2FlM/0jjaVWyM\nHVv/Aj5lyhSMHDmyYXXwcqNIKecCmAxgtN5WEQijAXg89rAmlHsFAF4CsFpl2xqVvzsA/KPy+S2/\nOvqk7l10VM62KTbM4xXKjWIrqxH7ETvtcjzL4EaZNKn2+5tv5qsLIe1KFmP6uQAmCCEmQ1koxgEY\nAGACAAghzgSwpJRyn8r3w6FGlzwPoD+A/QFsBmALAJBSfgXgBbMAIcTH6if5omuldIfRLh1pCNLE\nRuhjF0JsTJ8OLLII0K9ftrx7I+12jzTTjZKWJ2kteN7C4T30VUp5M4CjAJwK4N8AVgewlZRyRiXJ\nEABLG7v0g5qX4xkAD0JZMUZLKR/MXGtrvWr/Z2XwYODSS/PXp4zozrS72/57WW+0pHoNGQLsv3/j\n6tIO2I7nHXeUfyKjUENfi7zu+dLT2vC8hSPTDKJSykuklEOllN+SUo6SUj5l/LavlPIHxvezpZQr\nSinnl1J+W0o5WkqZOGlXJY9d/OpU+z8rH3zQ/GGB++4LrLhi8fnaLBsmZbdsxO1///3Z8iWKmTPV\nZHi/+lWza5KNoiwbRcKHFiG1tM3aKPrmDh13kId33wXecohAmTABePXV4svXnXKriY2i9ieK6HHU\nlq73fUK8HbnhhtYRMUW4UWjZaG143sLRdmKjiIuliE7nr3+tn830O9+pDpFrBtqy0Sw3SqgA0UZM\nxvTpp6qcv/0tfFkTJgDDh4fLP3o800RoHvbeGzj33OLzBcK4Ubq7i1mQkA+tsHR3A9deW/xx5nkL\nB8WGhbwPrxdfBHbeubgFooqi2aNRnn02235p51YI4LbbgIsuypa/C/qtP2lStDffdB+K/PXXapXR\nL7+s/+3II4FXXqnfXhTR46ivi7J3tKHn2QDUUvOLLJIvv2iecfT0AIsuCtxzT/7yehtXXw3ssw9w\n111h8i/7vdCKUGwEQAfafVC3AkxzsVk2QrwZ/P73aiKvqHhZJ3Zt4PQ8zf82dtkFOPTQbPm74PJA\nW2UVtYiTCzffrI7TNdfU/9bRwLty2jQ1hwlQbhekjRCWjRdfVBN15c3LpW5ffQV89JG6Dogfn3yi\n/uc5VzbK8PxoVyg2ehGNiNl48031xr7KKsBZZ+XPDyjHOdXHLqkuSaM5Pv209nuSqT602DDbsPzy\nwFZbqc9lFxuh7nEGiLYeLvdjFkKct1AB/60GxYaFMi/INGECcNpp2fZNi9ko4mFjHv+HH86fn5ln\n1nO7/fb5YyDyxDW8/z6w0EIqWFKj22ITFs26/sr+gDSP2XPPhR36mlUoZ7lW4+5HEk8riY1QAf+t\nRq8VGzNmqAv2L3+p/62MQ+k0++4LnHhitn0bMfQ15IMyq8/+b38LGwORxvTKsoUPGQO+9Tmw1b2R\nlg2X7aH5+mv/oMy77y5ebJjn4re/rbdGZcnThYcfLu/D6JprlPWrbJT5hZDY6bVi49131f977y2u\nDnffrRZG05TthkiL2fDtvB95BHj66fz1SqMMo1GKfpNKsmw0S2z4Wm1mz1bHJTplt29d5psP2Ggj\n++8ffQS8/nr99p6eZMHmwowZqvwQ4st335dfzl6WC2+8oY7TxIl++x11lIrrKSutYNkgil4rNpLI\n2nkdfnjtzVy2C9fmCsgjNjbeGFhrLXsZRVKGeJyi21VGy4av2HjtNfXfFuTqyvnnq/+PP27/fe21\ngWHD6rcniQQX3n8fWHxx4KqrqvlERU0Wl5nPtepb/z//WdU3C3ok2D/+kW3/snDNNcq6axP/Dz1U\nPYcvvKACcH0pQ1/TrrSd2GhmkFveN63QNHqejaLORZk6gCx1sO1jM91rQvmj4+oQ9z0Nvaqpz3me\nPr3qVgKAZ55JTm8KgOgw7aT7rW9fNYQ4jhmVxRXMFVs/+qg2TU+P+m2VVfwf0r5iw6XP+NGPgP32\nq98+b56fG+qLL9zTlo2f/lTFQNiGa2+ySVWYfve7wCGHNLp2JIm2ExvNtGxkfbhOngz87nfZ9k3j\ngw+qsQKNjtkoWmyUgbi6fPxx+r7msXEJEA0lnOPulbTypkxRdXvjDfU9Sz2HDFF/rmXGkWbZmDcP\nGD++fvtbbwGXXFIr9uLy6e5Wfy+9BJx8snu9zP8uafMyeDAwdKhbWindrlVNWV+cXK69f//bP9+k\nlwCSD4oNC40SG9Onq5iHTTcFTjghW5lpbL65UvxA82cQzUrcuQ3dMcyaVX1jTDs2vhNBubhRQouN\nuDrF8fe/q/96Xo7Q9UxCWx182W039cbrcu10d2e3VjZSbMyaBbzzTnKaVnp4PvOMurZmzYpP42L9\ny+MGI8XT8mJDX1C+YiPERRWtSxojRqiYh5AX+H//W/3sGrOx++7AhRfmLztk8NZzz4WPbVhsMeCw\nw2rLLtqNkhQgGmpIZJp4izJvnrpmog+sImYe9dnXx40Sh3YhuFg25s3zF7RZLRvbbVfvxglF2YXH\nDTeoY5M043CS2CjCpZ50/m65JVkIETttIzY0vh2fzbSdty6unaAeEROSPn2qn11Ho9xyS/UhG4fL\nXB1Fj9ww8zPXKCm68/zss2oEvn6bL8JyZnMxNcOyoXEVG337AttsU63r/ferUVxFiCKfNkbFRhEi\np6Mj2Y3iK2iyiicA+L//c9/XJe+jjrKP5GkHQokNl/O3++7AT36SnOb114HLL/cvv51pebGhHwpZ\nLzDbwzapY9lzT+WasBEVG83g/POBceOq321iI65+PvUeM6b6+dJLq59DWGmSOpS8dHfXv6Vsu211\nboFXX1XrL/hardJwidlotGUj6fzfd1/186WXAltv7RcgOnu2fa6TrMdTymyWjWg/kRaz4VPGu++q\n+CuzHB/MMj7/PN9yB599BpxzDnDAAX77vfSSve7bbZe9LiFIOh8uz4JXXrGPVokK2vPPr11QU//+\n4YfJ9dt2W+Cgg5LT9DZaXmzssYc68b5vnraL1WXfri7ggQfsv0XFhpnfCy+41SsvRxwBnHde9Xvf\nvtW6FGme//Ofq58PPrj6uVGWjaI49ljlLjGJzny63XbF1yGPZWPyZGDq1Px18A0QjdbVJ0B0iy3s\ns7hmPZ5FWTaSxMbIkVVTfrTtM2bUT8RlzjeSN2Zjww1V4Gcjee01NfLGfCPX7TYFdxlwsWxEf9t3\nX+CKK9Tn4cOBX/wifl9AWZqOOAK48kr770m08oifULS82ADUW0ARMRt5Aw71Q9x2U265ZbY8XZg3\nT7052tCWjcsua8zaKCHjT+LyzuNGcV1xs+iONulaSxOFa68NrLlm/rJdt2uSYjYeeAC48cbq3BtR\nnnzSr45pNEJsfPxxddryaNuHD09e70Ln+fHHanG3pDQas4y8YjLL8OmZM9X/uBejohc9i8OnH3Zx\no0gJXHyxGjJ74IHVdE89lZy3XpHZXJmZAaTZaQuxAWR/83T1o7ug9zcfErNnq/H+c+fG75f3Aj7o\nIGDhhe2/acvGwQenx2wU8UANadlI25aGXinSxPXtvOgAUV1eUoDof/7jX5ZPfVxjNuIw3T2bbw7s\ntRewwgrZ6hKHeXyyBog++mjtkF0zrySxocuxlWEL5rS5ZEePBlZd1Z63TWw8+WStyyorWfqwtH3i\nXmhCkXRefGI2nnwS+OUv3fJPux7zvpD2ZtpCbEybVvVvNioyPsrUqVXfv9lBnX22Gu+f5H9NWi3U\nxJwA6cEHq59vvz1+Hy02gMbMs1G0eDHzzGvZuO22+H3TJkUqOkA0qdPS29ZfP3tZSWSJ2QDq6xrn\nG+/fH3j7bfsEVFHSyjTjQqIPc9dzoe8Vc/E2U+xlERtR/vnP+vqNGVMdJmzDVu6661ZX4S0CWxlm\nWz78UAkil7k3bGI9BC73c9JIqOg5NmMubOls27JY/+bOrfbFFCP1tIXY2HRTNWMcEG64Uxpm0Khp\nSi7S/G6Ope/qqn5OqrdNbORdG8U0K0Yp2rKxwgrV4btZ89NWnyWXjE+TFscSyo3iuzZKkQvKhRIb\nX32l4hdcptZOO6faDTh3brxlIw19D5jWRZcA0Wi6JH7wg9rvUgI335y8T5IbRXPnncl5uOYd99uf\n/qRmR73jDvvvZp3yLEzng4v1IMkiaXOjJJWjefvtdOtN0nH9wx+AH/4wechub6YtxIZJngecjxtF\nCBWcan7X7L9/fZ4hSWqzORolbZSD67FL6nSyiJck4mIAfNCzKybVJ01sFBUg+sknKg/btSal6qiS\nxIYtyNKHKVPq55owy08iTmzY9jNnCU0irUxt2TDnvABqLR1p92u/fuq/+Yarz7ev2Ljggvgp1n2v\nfZc0O+5YHeHiQ5KrLE5MpB3HUKOjspBn6GvcsVl66fhYqEMOUcIs6ZrT1uvPP4+vd2+GYsOyr6sJ\n7Kab0tPkERuff67cMGmm0CRsYiNrgKBL2SFiNtLycz3GScfR1bKRp01ffw0MHFh7Ts1jecUVwOqr\nF2u9iDJyZDFDn01sx04/4NPwsWyY9fMJENWWDdNV5io2oufp8MPd3Fu2PCdNAnbdNT5N3H2VJTAz\nKe+0WIW891kjcBn6mhabZGunzaUthJrifuzYYvrI3krbiQ3fk2wO88ryINEuhbhy89ygZ5wBHH10\n/IqYGlc3Slp6KZPjR7780t0CEIK4vNN8yS6BZo2I2dBl3HOPPUBUiwzXGB5z1MDMmcCvfuX39plm\n2Zg7t9Zl5upGseUVRbs0ktJ98kk1lkDPYmrm72qJtLliLrhA/U+bhdbm7spqtTjgAOAvf4lPU+QD\nKul6zRr7FOLevu024L33/PfzsWzknfjR1WJFgZFM24kNzdSp/hdVltEoevXIOPLcoLqjHzWqNpLe\npwxthgbsN6j5+Y03gPnnj8/rW99Knzkvq2XjjDPs497jMPN2jZLP40ZZa630PFzp7va3otl47LHq\n59/8Bjj3XDUpU5SLLqoPYATsMRvm9PZrraXOucZHbKSJbB1Hk3Q8zdVho6O5TMvGFVeoBdbisLlR\ndLBwmmXDtIBo4mKWkh5Kc+fWzxgc4i35iSeA99+Pz/uZZ2qPa5oYKXKGZRu77ALssIP/fiHcKHH8\n9a/1+7pYVkgtbSc2pFRD3dZcE7jmmvrf77lHXTw+ij+Nn/wkfrRJUQGrcWP1o+mimDeFDjaLEwQu\nQy1vvDGMG+W449RcIEnE5RcVCt3d9qmf84iNIjDftPIOswZq66wtAAsuWJ/u0ENVAGPaG95zz6n4\nFj2C4vnnk8vPY9nQ1hvXayRqeYq6UZJmytTWva++qj/PRQWIAsli48AD02edjCtDCHdXynrrAeus\nE/9AvfVW5UqLImV6G0O5UfT8HllwERs+bhQb5iR/Sfv6xL/0RtpObADARhup/1FrwP33qzUedt7Z\nvl9WsXHddfG/5blBzSWyOzvzj51/8031P66dSaNMzPxs9XjkEfVW7LN8tS+u5+fKK9XbkvnmH7e/\n79TgvtfIn/5UFXHmPCxFvP2Y15bLsMQ0y4YmLmbEx1Xo2j7XdDbLhllu0r2hxcZhh9Vbfooa+hol\nmqc563AWl9zdd7unffvtMG/Xb7yh1lux5f3+++oYPfpo/nJcrAc+lo28YoMUQ1uKDY3pQgDUlMk2\nbr5ZXXjmRT5nDrDyyrVDTLOQdaGpKJ2dxd0cOp/nnwdOOqm63UVsxNVj443VW/Fmm9WXkxWXwD0b\n2q2iHy5JHZNrzEZWxo6tHbUE1IqNPMcoOnkckHy9uV6LcdH0RcZs+KaLnh/zfrXVzcQWt2Tu53Kd\n5bVs2NL5xGzE5Rc3M6vr9eVz/R1wgFpvxfZC8fLL6n+WxeTi6pBUN595NooUGxQo2WlrsZEU/GXe\n2GPGKGVuvsX8+tfqBtpzz+QyijQ9utxcvvvZ6qfTm+ubAPnEhm+9NElv5NFZGufMcVuGe8AA9X/y\nZGCnndzqEcqyYdvXXOArzRw7caL6fuaZ9enMOut4BB+xEdcW16F7jRQbRVg2bDTKsmH7rQixEV3H\nx6X8tPRx+7ociyIexq5TD5h1stUhri7/+7/JvyfhKj7pRqmnrcVG1LJhEr3QouP4zQm0kki7YIu0\nbMTl63vTxHUatlUQo3R0uLcpLd1DD6mhoHGzLG6zTe33ESOARRdNL9cMcnUdRVCk2Dj9dHVso1Yx\nM2bD1fd74onq/7HHqsmXbPmZ+yQdc1c3Slx8QFTwJomNPMORbSQFiObBNWYjbdQKoIIzNS55RtOc\nfHJ6GVHiHmpZ3t6TZrMFiokzcik3tBvFXDgyjizWlrQA/N5OrxUbUVw6LzOK2ydfn7Rxb/p9+tTe\nfHGzgEZJsmxEf3N5oy3KsjFtmnooA/HBr9HFqMxAMlve+nybYiMtmj4as5HWthkz0mdSPP549f+0\n02q3mzEbrpYNk1tusedn4vKQS0sbJzZ8JvVyuUaEcHdfpQWIZn3bjr5kRPF5wJoxVlksG3EBuWmC\nKKkMMw/X9I1+QE6fDlx7bf12334tup/vDKJRdIybb50oNuxQbFSQMr5jmTFD+UZtsyImBUQK4W/Z\nOOgg+2/RthRhzs9CUbEj66xTXXAqLj/fOAptLtduFEAJGR0U6BKzkbY09JNPqmW4XYhObJXFsmES\ndSGZgtPFshH9Le74xm2Pq5fNKuR6jbhalPK4UWx10UN6BwwAxo2L37eRbpQsuFo2fFwTaZjHfd48\n9ZfH2vHll8A++9Rv93mw21ZmTet7047/r38dv49reyk2qvQKsTF1atVPF4cZcBa9kDbZRC2QZGP1\n1d3ytKHXc9H09NSPxdcU6UbxvWHM8jo73R8QSUs4m8MATfPzyy+rUUNAcjm2uuvJm6LHysU91N2t\nYkKS5hnRvPOOirpPG44YjRXIa9mIio28bpTFF7en8+1IH3kkvaw4XMV4VsvGcsvZxZNe7Kxfv9p1\nQeLqV6TY6OnxH5EUdy+kiQ1N3KJ4WYSPmXbwYBUYrrfNmqXii4p4yJp5zJ5tH1qq0xx1VP1+eS0b\nSft89ZW6H9dcU43+iZI2W3NvpFeIjTXXBH7+8+S0UWFgfk6a4yKNpM402oHkjdm4667aNLNn29/U\nffzQGt1hz5oFLLWU+34uXHhh9fPKK8ePGjKxHSv9cM/SuXR3+y2hvdFGwLbbKtdT3HC/OMvG3LlV\nl5WrKwCod7HltWz4EudGsXHJJW55Ruv04ouqHFOAAvWWDVOw6bq98IL6b7rf3nijuhqzrdy0Y5L1\nmCUdm5kzlTBOW6hNIwSw997xvyWVr//HvcRE00c/2/I3j8msWbXxbVdeqeKLXn01uby0OkTLiZsD\nSO9jm/QwpNh48kk1kd7UqckjFm3XXm+l7cSGeQEV5UbJg62zuvlm1THaxEbcDRAVBua4e73PdtvV\npll4YTV7OJynAAAgAElEQVSaIUoWy0aooaFF8tFHSlz5dC6+82yYPPywWnRPz+sSJSo2dPmvvloV\nWGmWDfOz7YEbTedj2fDFR2xERzrFEa2vFgrR6zZ6/X31Va37Q4hqoHHU0mK7dl3Xuglh2dDTc7vO\nneESKOlTfpZ0JrZrLFoPl/4iWnaS2IiWqa2KLgGiruWnEXUhX3lldXtcvmkT4/Um+qQnaV183tzT\nXB5ZsV3wY8aom3Po0PS0mmjdiph1r8xiI+uMniecoNxeNpKC5rK2L2nW1agbxTWQMu68ROvoGytR\npoW0NNE2jB2r/kfrGhVaX38NvP569bsQwHzzVX9LKgPwfyD59CXmfmb9NEWeh7yjUcx0rvu41N/l\n/k0TG0mBloccUrvdljaPZcP2W9wwae2+tb0kRK/F3kzbWTZMkiwb0XkLkmI2spIUICpl+kRFJi+/\nXD8U1NzPB592fvih8m/nETdZ+MMfkn+Pa/N//5vdjZLHrGojLmYjbX9XseEbs5F3OKqPZcOVqIiI\nyzvadlsnro939DebkNQPw7RjkrbQYhwuxyZkgGgWV2eRcTYuaf70p+Tykywb+ry4WDay9AdRNx4Q\nHxyvxYYZiK7Tbb11fBm9jbYWG7//ffzU5NFhhKYb5csvk4PGfEi66ZIeHlHi5qIA/DstH9PwzTer\nUSO33upXRl6yxsl89FG2N8esb5tJ+0WHzuWZ/Aqov170rI1A+dworkyaZN/e05PsQooKCiHsC64B\n9rds15gNPXV7XrFhrmzqalVxwWdIaxpmkOfJJ6vvSUPnk3CxbOy1V3K+LkHwjXSjxFk2OjvVS9nf\n/uaXX2+j7cSGeTG8+mrtin1JmNHtSStI+uIbIJqlA/c11flYNrRqd5ldtEhcRoXY+PjjbG8yIVwM\nzz2XXsaYMfV+3bjzEn3g3nIL8Pe/A6+9Vn0TGzkyPhbANrzQhxBiw8QMDI6ONrHFbETrFudGyROz\nYeaf5cGtMevk6q5wrVcRmH3PrbcCp5xSf/1qinKj2OoQ9z3pnp43r/beMI/vK68o12rcvnHrANlI\nEhvR4G3OIFpP24mNrDQyZkNjc6M0AlexMXt288RG//7Jv8cdq6++Ks5H64LPfnHXwu231353FRuA\nmmBshRVqt516qn3/e+5Jrp/m7LPTZ1ns27d+RtO86CHPgDpWl15a/e5i2TBXdzWxiQ39cHAVmVkm\n1orD17KR58GVVmczEDea9q677DMpu9T79deB0aPVitiufPVV7YJ1SW4UjZTAGmso0W1u0+y2m1q2\nwMa77wLDh7tbJJLEhuski70Zio0KphulUXmmzR0Qir32cpuI5913qx14o8VG3JoPgJptMG6Y6nPP\nxc+EGsKyUcR09EnBhCa2YcxjxriX74P5oNeY9Ro0qHZug6Lp6amdkdPFsqGPY/RatYkNbQnyeeAX\nca7N34q4130DV6PcdFP1c7Q+xxxj30cfh6S4kDFjlBiNWxH7gw/qtx1zDLD55sD//E9tOba6mdtf\neKF2mznDr4tQ8xEbNvr0qXfL23j2Wbdy2hWKjQohLBsTJybn2SzLBqDcDT5uFJeJsYok7m0EUO6A\nc8+N/z3rtPJFB4jaynDJw0dstMKQ5CxEj4mLZcOMuTJJMum7Coi0BduiJKVNGrbpi0vMhmuwqmt9\ndFrT6uFrfdHDRk2ic2W4ulGirLyyX71swtpGkmXj2GNrt9nKTZoAsjfQdmIj6w2sTWpFMmWKX8xG\n0gMvbuKorDz4IHDkkenptNho9BCuVVfNvu9559m3J3VeISZvck2bJDbMz422LkVp5NDZtKnVk8RG\n9HgWITYAv6DxpOtCjyprhNh44gn7Gh82XI9FdKr4LNjabjvn112XXF7aMSwydmLWLGDJJeu392nr\nCSSKo+0OU9ab4M9/DtOZJnV0PpaNosVGdLZRG0I0T2xkDRAF4lfsNY9v9Fw3040SxewgzQdFlqA7\n26RuWdl//+LySiNtno3o70liI8n643pOrr3WvlhYnnyL6G9cXA27756ej49lw/ZS9P3vu+0LKKuq\nLTAzmudNNyn3xHzzAYstZs8rbui0phGBmkWuW9XOtJ1lIyvRORE0eg2FrCStn1HUaJQsRGe3jEOL\njbSbumieeab4PJMCzrIe96i/2LX8pLLNDlKP3c/KBhvk279ZpFk2bNdj3PFNEhuhrDWurou8ebkE\nJs6Zk78ck6yWjV//Gth4Y3VN2mI5ouXrIN5PP42vW1wwtMa2bknR2MRGnMj54x/D16esUGxUiBMb\neR/+SavCRmmkmdp1Knd90zQ6ZiNEeXvuWY1at1k2Qgu9tPPL4XJVssRs6Aevj2Uj1D3nEjxbtNjI\nIrbMMkKLjf/5HzWVfNwcOnHl57k3bYGoReMTpNtI62DZoNioECc28nZGaUuWmxRp2UjLx/UG0Z1Z\n0rTcrYS2RNhiNkKLjTiXRtHl6pEZjbZGFUmaZcM2GiVLzEbWafHTMIdwxuHzcI9D1//b345P5zp1\nuI/4ufdet7Q+xLmKQ4wULBK6Tdyg2Khgcysst1z+zihtGXKTIm+oK65I/t3FsmG+wbTLsK1ll1X/\nm2HZiHvDjJabdzgjAJx+ururrIykxWwkiY0oSW/2zRzNU4Rlw5wJNY/Y8KlPT0/8zMx5iLp7zCHC\nZRYb5hwfADBjht9kYb2FtgsQzYrNstGnT/6L3MdnuMwy+coyeeqp5N9dxUZR07aXDZvYaFaH5jr0\n1Se/Rk8vXzTReVZCWTYa7R40KdKNImV8/IKroCrC0hKCnh7gqqsaW6YPF19c+90lILc3QstGBdvw\npc7O5nZGeUgLRHIRG93dwNVXF1OfshA3e2Mjg3MbgZ66u1WJLhPvMupAD3n2ERvNXJWzSDdKT0/9\n4pLRNGl18Rn66kNe8TxxohotGJJ2uvfLCsVGBdsN1KdP+y4RPG1aeppQ/uxmEjd7Y6Nmb7Whyy1y\nteFWdqHYSHs7FwL417/UZ58A0Wbe364P7aTRJC6r14ZwozQCXZ9G9EMUG+Gh2Khg65D69Gnfi9DF\nzF5mP2lW4iwbzRQbIWh1y0YUn2BXH7HRzCBa1+tt7Nj0PKILgZkUHSC6/vpu6UzyxMbYXNwLLpg9\nPxuNfLFqx5c4FzKJDSHEIUKIaUKIL4QQk4QQ6ySk3UQI0RP56xZCLG6k2U8I8ZAQYlbl7+9JeYbA\ndjN0dgL//ncja1Eu2vGm0B1q9LyWwbKhmTUrf56hLRuNHqbrYtnIsm8zxYZr0LW51gdQGwfmcs0W\nHbORRTiceKL/PkkWv6IDe/POZ+NDu1rL0/AWG0KIMQDOAXASgLUATAVwrxBiUMJuEsCKAIZU/paQ\nUpojoDcBcCOATQGsD+AtAPcJIZbwrV9WbA/WIjr9NOKG3JYBX7HRCvNE6A5s9Oja7WUKEM3LxInx\ny8y3KmmiIGkq+u5u9eIwbJh/vmVk6aWrrpVmiI0sPP64/z66PraHc9Fio5HrlrRqHGBeslg2xgG4\nXEp5rZTyJQAHAZgD4Gcp+82QUn6g/8wfpJQ/llJeJqV8Rkr5CoD9KnUbbc0pALYHaxFDENNwnVyr\nGfiKjR//OEw9imT33e0upGYGiLaT+yYUaccoaSbNefPUvbzDDvX7tepbpq63y7Xjch9/9lmyKyYv\nWV5EtPhvhNhoJK16zeXFa+irEKIvgJEAztDbpJRSCHE/gFFJuwJ4WgjRH8BzAE6WUj6WkH5+AH0B\nNMC2oLDdkI2wOpRZbPi+6bfKQ/O22+q3NdONMmVKc8ptJdLOTVqApBD2B16rd/xFrcHy29/mr0sS\nLpOcRdF9ss361Cp9jY1Wv+ay4vvuPghAJ4DoIt7TodwjNt4DcCCAXQHsAuUieVAIsWZCOb8H8A6A\n+z3rlxnb7HWNWM2vzGLD17IxJO4KKBk2ERknNvQIh5CEmI2x3SjCsmETG63oRgGqbSnSjVI29Llp\nt4dz0cGtrULwx2nFLWLOpzZJCDEMyh2zTzS9EOIYALsD2ERK6XCZjQMwMLJtbOXPHZtPsRFCoMxi\n44kn/NLbll8uIz5io8znpzeR9lCdPDk+7bx56uFsc4umTX5XVj79FFhoIbe0rSo2dGxDu1n+BkYf\nVw2gq6sLXV1dNdtmz57d0Dr4io2ZALoBDI5sHwzgfY98ngCwYXSjEOIoAEcDGC2lfN4tq/EARngU\n7U5vt2yENq02i9dfr98WJzYaEbcDAEcc0ZjrrSgabcZOcwW89lp82iQ3SqsO7156aeDKK1vbnZCG\ntmhMj9rRHdl33/ablDArY8eOxdjIGOopU6Zg5MiRDauDV1cqpZwLYDKMwE0hhKh8T4rBiLImlHvl\nG4QQRwM4DsBWUspSDDjt7ZaNduUf/6jfFjeDYqPOz/nnZ+9Um8F776WnMTniiHzl+TxUo+6/Bx9U\nozdaYbSUDw880DvERlYa9aJA3MjyLnUugAlCiMlQFopxAAYAmAAAQogzASwppdyn8v1wANMAPA+g\nP4D9AWwGYAudoRDiNwBOgfJ9vCmE0JaTz6SUn2eoYyE0onOi2CgHXV3AfffVb2/k+Wm3h6HJ1ltX\npxPPgs9DNc5t0G7Ht92m2I9CsdFeeIsNKeXNlTk1ToVynzwNZY2YUUkyBMDSxi79oOblWBJqiOwz\nUG6Sh4w0B0GNPokOSjylUo4TffoU65/s7UNfexMPPmjfzg7LjxVWAF59tX57EYvLuRLnGmm3c9nu\nYiNuPop113WLJ2u3893qZDodUspLpJRDpZTfklKOklI+Zfy2r5TyB8b3s6WUK0op55dSfltKGRUa\nkFIuJ6XstPw5C40QNOJNiDdELXoJ+LJAMejHj35k316E2Fh5Zbe0caOoiryfN9usuLxsbLJJepp2\nm2Jfs+KKqh+Is2y4Bliyby0XbXU6ir64GnGxtmNnkYfoMW92h0Gx4UfcA70IsfGf/7iljXvrda3D\nvvu6pWs2rWrZSLunOzvVX5zYcJ3/qNl9B6mlrU5H0ZaIRlg2ytZZLLdcc8uPdhDN9rO3W4e16qph\n84+7nvOex9tuc5/3ZcYM+3bXczk4OtaupJSt73Albd2ezs7kFbcpNloTno4EGnGxlm3oncuNvPzy\n9u1FdH7RYx5nWXAxMxdBu1k2LroIeOSRxpebV2wUsZ6Eax1c7vvQD3qXfqFVLRtpfUyaZcN1iDjF\nRrng6UigN1o2XG7kkMfFVWwce2y4OiSVH9JX34hrobOzOYv/NdpCZRPErnVwSRf6XLkEureq2Ejr\nY7RlI65tri8AFBvloq1OB90o+WnkxFJHHVW/zdWN0qiHV/R4hDxfjbBymcd3++3Dl2crF8g/70Ya\ntiDCIi0boXFxGbVqgKiL2EgSFK5io9kuWFJLCW6r4ij64uqNbhSXG9k8zraVNPPgesybJTZC0iix\noY9dIy0c0fMV2j21wAL122bOdNu3DJYNF7Fxxx3As8+GrUcIXGI2yiQ2hg8vJp/eTluJjaJphNgo\n26yRLm02b+IVVwxbfrMtG1kfiquv7r9PI8SGOW13I4VU9HwVdW8dfLB9u01I3XWXW55leCN2DYa9\n556w9QjBfPMl/67dKEm/u1CUIJx//mLy6e30SrGx6KJu6crQ6TQalzabaXyO0Z571n63dahF+tWL\nIKsbZbvt/MtqtGUjTWxsWLd6UXZCiY2LL679rgOHOzqAxRar/c11wr8yBIj6rrjcSriIjSIsG0Ud\nw974HAhBrxQbIYdOFX1h+iyGtuuuwPrr5ysvlHl71VWBfSJr/No6g7JZNrK+/Wepnyk2ttoqW7lp\nmMfX5mqIS+uK69DXENeZENV8Ozvr3SauYqMMD5feLjaKsLoVJd7LcD20A20lNop2e2S5yIquw+ab\nu6fdZBPg29/OV55L/bOYFSdOBIYMqd02d256+c0WG3EPxaFDi8nfdAOYD+pQ7evoqB73tPNY5LUc\nbU+I9pkuIlvdbWJj6aXrt7VKzEar4iI2ktL87/+6lVPUOaLYKIa2EhvNGo2y5Zbh6mDrNP/619rv\n+oEoRP4bzKX+gwb559vRUV83W+df9gBR3Yaf/jR5P5f6Lb44sMgi1e/mm1hIsfF5ZWnD+ecHvvwy\nOW2R5YbKWyNENV+bSLRdb7bjTLERFhexUUScBC0b5aKtxEbRuHaIa6/d2DrsuGPtd+0WKkJsuLTZ\nFBtppngz39VWq92Wx7JRJC+9FP9bnDk3LaLeNfbFbK8pIs39N900PS9XTLExYEByx1+kGzFUzEa0\nDJ2vq2WDYqPxpImNjo5ixEaec5RWR+IPxUYCWd6yi47wd6lDo8XGyScD3/8+8Oc/AyNGVLcnld3R\nof4OPbS6bd484Mgjk8tvhBslaWhbnBslLe7HVWy4pCvawqBnZlxoIb9yH3ssPf9mx2zoclwtG3H5\nZOVb38q+r0mrig2X0WmtYNlohEuzt9FWYiPvRbHGGtnyM9P175+vDlGiHf4pp9Sn0Q8+m6sib3k2\nVloJ+Ne/gF12cS9P52t2APPmVUcP6JEDZXOjROuj2xvCshH97fTT3fNypaNDWcaOPx7Yb7/0tCZ5\n4oHKYNlwfYD71m3ixOrnOXOS07quu1IGsbH33v77vPJKepC6j9iIWxrBhbLNYdTbaXmxsccexeSz\nwQbAlCm127KIjaLebDTRN7QTT6xPY1o2vvOdfOX5drSuN7RNbMydWx/Q16wAUZdObd11q5/TxIYL\naWJDd9pFi41+/YDTTksXxrrcAQOq+2al0WLDZtmIuu3iLFq+bhSfEWCuD09XK0xIiliQLjoCDfAT\nG3kmnmOAaLloebFhqu+4iyI6dbHtIrR1/Fnesov29blc6PrBJwRw3nnAGWeELc/E17Jhpp83r/7h\nUKTY+O5309Po8/X44+lpf/GL6ueiLBsu7QsZqOmSVo+88a1H0oidUGIjyY1y/PHVz6uvXmuR8K1b\n1geZKViTKMNbeRHnKEtMTGdnVQjbxMbKK7uVbbp480CxUQwtLzZc8J0VM2lb2r5Fx2zY1niIoh98\nOrBqm22ylxdq1lR9jKJulKyWDRemTk1Po0djRCeAsmE+YFxiNqRMXgMkybIRTVcUPg9JXTfz+vLh\ne9+rfs5j2fCZLjrJjbL//tXPK69cOxLIxOV4f/aZe50WX7z62dUiVgY3StbrzmXSv7iZX4HaxQJt\n/ekf/5heh803r7r98lpoKDaKoeXFhr4Qdtyx+jl6cWRdybTZk3otsYR9HoAoun1x7ffBt82uDy9d\np6hlQ3/X5brOx+DSRp8gRF/TuWveSebwNDdK9Nj4Ynuw+TzE9DHR+RQ5AsOnTQsuWL9twoT6+KU0\nN4pZZtL5c7kWll8euPfealxNEuaSBK73Zig3io/7L+t15xJcecIJ8fubYsMm6tPqtfrqwIUX5usH\nXQQT8aNtxAYQ/1bju7hYNL8s+xbBiBFueQ8bpv4vs4z6n8c6seyyful9H0Bm+mOOqX+gmnXfeef4\nfJrRAUhZ7axdxYZteK/G1Y2Sta22jtpHbESDYZPaEt0nStbRKOeea3dN7rNPffxSWoCoq2sq7f55\n+WXg+uvV/DrHHpuc1jdvTU8PcN99fnm74Hq/9ukT7h5LG4U1aFD12rVdJ2n1uvpqZbmyveCQ5tFW\nYuOf/1T/oxdoVrGRxY1SJOY6FklsvbVyGWyxRf76LLSQ383peyNrN8rzz6v5I5LEBhAffNustw0d\nuGZOpGZDt6sIy0azxYbOJ64tp50GHHRQcl5ZrI26Dq7xC0JUg1nTLBs2t170tziWX95ubXGtowvd\n3cBaa9VuW2WVbGUC1cBxl2P5xBPAq6+GdaPEbb/+euDMM/OJjTgraVZo2SiGthIbetKoLJYNvc+l\nl9rzdq2DlMDPfua2n0++aenMVUZD3RzPP18/14JvIFv0AeojNsw33GZ0AN3dVbGR5rJyERtAstiw\nffYhr9jQaMtZXPDzwQf7TasPVEVBGlL6ueqSRs6Y2/RnPedINJ8k8swR4mrZMOOZNHvtlb3cs88G\nPvzQ7Vius46ycIaK3wLij/Fee6lzmBQTRbHRmrS82DCJewD4WDayCIWo2LjqKuDTT4FrrnHPwxb0\n5GrZcA2qPP104Lnn3OsUZdVVgVGjardltWxEh8LaRqNE826WZUMHEvb0VB9mtoeUjayWDZOsnb6t\nw84yWuq445TVcLnl7L+nmcV1GhPXIeI9PX6WjajlycQWs2E7j2l9QCNiomyi8LDDspfbt69a7drn\nftXtLHp0m8v1oq/duJGDLmXmOU90vRRPy4sN2xtgHrHhGkQWVwfNAgv4TfA1dGj9tqjYuP12+74+\nw0WLfki7+ug1W2+t/usIfZ8A0bQH1AcfJP+elfPOU/+lrD7M9ORNaZaNpDiH731PmattuFo2rrii\nfn4YTVRsbLpp/ZTxcRxzTPVznz7JU6bHXVdJI3F8LBtZ3ChpMRv6c1RsXHKJ+xT8WXC9/3p63IOl\n4/jRj6qfbUPP08hqIcjjRtHoa9fVzWXOOFqE2Egrj/jTNmLDvIl83SijRtlna3T1K8ddjEnlXnpp\n7WJecTeVzvs73wF22smt/CTTvO8b6NtvJ6c3Mc/Bb35jT7PHHuptX1sL9D62Y1UWy4ZphfnZz1Rd\n08b6u7hRurqAGTPsv5kxG0nsv3+9b19jio099wQeeCA9P83667vHjMRdV6NHx8dyuIqNnh4/N4qr\nZSPJjRJHEZNcpVk2Ntyw+jmv2EgLiL3xRrf9i5zMDVDn01Vs2Cw8tn1vu636OU0kDRumRhIB8Za+\nRRdNLo/40zZiI2lbmth47DFgvfVq9/WxSviKjY4O1QlffXVyvqZlwyd6PlqfK6+sbve9cdJmJI17\nECTV1zwuWS0btna4jJbIghlMuMYaSkDozihrzMYaa6g36LhrxBQbRbhRhg71y8eMlXC5ZtKOQ/T3\njTeu7dCT6lGUZcNmtfS5Zt5/3034/OlPKr4pro5JzJwZnzaP2LBdZ2PHJrvV0oKgfevgsl3jKzaS\nhFU0/Z13pq/hYi5dQbFRDC0vNmxkcaNE983i24zuF2cZ0aNGTKSsH3ZqioM8YkOv0hrCjRL3INhl\nl+rn88+P3z864sHML/oGZL4Nx70xhcA21XracYy6Ua67Dvj97+tdHnHn1RR5Wc+Zef3lGaKcJlLM\n68rVrda/P3DrrW718LFs6IenvudNsWezbIRgzBgV32Qj7VwmzYPhex3kbW/WydxM0mI2outRaXzd\nKLaRRknXZJr1gwKjeCg2Uvb1JU1snHBCrclPM2QIsPDC9XXJKzbuuaf27dK1fQssAGy/fXq6uAfB\nyJHA2murzxttFL+/7ky0JSn6pplk2dhkE2CHHarbvvMdFZxbNPr6sYmNtOOpH3Y77AAcfXT92hhx\n+6+ySn7LRp5REz7CKgnb9fG3v7nn6+tGicapmMfANQ4GALbbzq1MX9LO5Z13Vj/7WjaiVgqX+LOk\nPM1lEH796+Sy4/LM+jDXZWcRGyutlFxGZ2f1eMRdW6ZFhcKjGNpSbCTFbOy1V7qvMo2f/7z2e9zF\naBsNsNRS9kBHc+iqxtWNktQpbbWVn9jQv3/6aW3HF0fSg8Dlgaz312LDfBONWjaiYuPBB4E77qjN\nr6hhxyamG0WT9mYUdaNE3xL1fnFulriHpA9JI3vS8LXipJ1rc/u227rlq/PWdRk/vnbqb1sZSW/E\ntjf9zTaz1/P//q+6baedatfFyUOa2DCtmz5i47PPqq5gW1lx5SZdF+Y1G41ROu20+P1cCOVGOeig\n5Gvx4ouVC8UWs3PTTdXPehkDl7oSN9pSbCRZNq6/XgWuZclHY0bqJ6VzmRr4xBPViIK48otwo5hi\no2hXg0t+LmJDv5UlWTZcgwqLJsmNEudG0u0aOlT9151nVGzExQwkmYVdyWvZcI3ZcAn4s+ErNn7w\ng3rrXzS/pIeU7U3/rrvS63D77WqUShH4HCcfsWGOxrClz3It6HtSiPo+xlzULqncuPsj7Tjolw9X\ny4brtoMPrm2P2X+Za/mYYsOXjz7Kvm870zZiI2k+/iw3WtJD1HWoqcvyyKecUrtAVFw5ZRUbJhts\nkFwPG1GxEbVsaJZdttai1Mi3DVvHlGaZ0Gkvugh4+OH69Ws0cWLDZtlIugaGDq2/jlzdL/feC7z4\nYu02n5gNoNY//swzwOTJ9flEccnXFD0u6V3Fhv7cv3/8kOpHH60GVxeF6zlZYQV/N0qUpKBJlzz1\ny5K5OrMLaaP4XCys+sXCVUQk3aM20gKIv/rKLR8beUR+O9PyYsPlYiz65Cc93H1WBvUpx8eNEqUR\nlo3LL1dDiDfZpL5ePm4U8+FrvjG/8UbtdM1mnj/5Sa216rrrgL//3asZiehj7+PH1e0aMKA2ZiVq\n2YgbetnZWf+QTbqOp02rt5C5PiC23LLeTJ42GiXqVjDTrLZadXnvpDx8LRudnfGBlzo//YA0z9VN\nNwH/+IdfzAagxPN++6Wn88Gl3JdfBh5/3H/f6O9FBYh+/bVfH+q7PMQBB9T/rsWGlGr6dB3/Fd1X\nkzavSlwdzYnqzDzyuFHodrHT8mLDRpbpyvPkHyc2bG6UJNdKVAi4do5Fig3fG0U/CPR+DzxQfStw\nDQAE7DEbUeKmK7/mGuD++6vf997bf/rsJPKMRokS3S96Pey+e22Z5j55ruOiYzbMwNwkN4p57b33\nXu28Iq5iw5yL5brrgKeesqeNi9nYfXcVmxH38HV9Gy4Cl/xXWkkNC85r2cgyQaFt/759s1s2XO6D\nyy+v/910ma6zDvDkk/Z9o3X1tWzYJgMDai0bvlBs2GlLsRE92Ulmvbjpu/OafwG7ZcNnfYOsbpQo\nIV0n0TfXzs76B6iLUNKm7HnzgHXXreZt7mv66/Pe0MOHA2+9Zf9Nv5VrbB1Z1mMe3e/KK4GTT1Zz\nrsyZU22jr2XDVn6e857WaZvnOymOwkw/ZEh1GDZQeyw23RR44QX1eamlqtujbpQFFlAjneLK0Pec\nTboAGqcAAB51SURBVNTHxTA0cmrqjg7g3/9Wo8TSKFJsZHGjfP65+j///H5iI+3YurhRkmYL9rVs\nJA19jbun86zDFHJYdSvjOEdma+HjRvnud+3bixAbNpHj41rR5sX990+Ohnd98IV0o2S1vOy2G/Ds\ns2pugssuU26Urq7q4l8ufucsDB1a+1AzmTSp9mFlCwbNKnaibpTBg4GTTqr+bptR1SVmw0aec73a\nasnnVtfl+OPdHh42zH0WWUStOAwoQaJnro26UdLycxUbWR6+RSAEsOaa7ml9SLKMugrVrbaqfjbF\nho/QNfs9WxtcAoqTgsFt+9rWVnJxo8SJjbvuUnEzafm41o+0qWVj441rv3d2qsWELrsse5477FDt\nJJI6fZdJvVzo6FBTRgPKFx83JTUQPmZj+nT1Z0O/AdhmXHW56fr1A846Cxg4UH03g9FchtWGICoI\nk9wocdOWu7pRopgL1el5RLTQbGTgWdoMitFjkuZGsRF1E9reRKWsxl+4BB7q+qSJ+jjXZ2h8BGMj\nLBsmZ59da3FZcEH1f8kli7VsAPVtueoq4NRTq9/1+Ysu/GjbF1Di+MQTq8tOxKXTpFk2hg3LPoMq\nxYadtrNsvPGGelu85Zbqts5O4Le/zZfvccdVO/1GiA2fDjCuPmPH1uaVNWYjaW6DsWOBWbPUmic+\neUbRb6Jz5ybHquy8c/yCdKFIEhtmxzpoUHW66bRONu3h3Nmp3u7uuEMFf0bLOuGE9HU9og/trNjq\nqoW3XuwryWQdl4fLw7Snx09s6GOSNuy8WabuomfjdE3vM6JIs/feKnZk9Gjgr391L9cnZkP/bpsf\n59VX7dbHOMvGKaekp9OYYqOjw77w3axZ6vfovEo29toLuOGG9HJ7My1v2Rg4UF0M48er78suW/+W\nXdTQ17gheNrcGMVXbNx8s3qIxJVv8tRT1bfPNJNwmtiIm+fDhT59gMMPtx/jtAeriTnPRpJl49vf\nzlbPPCSJDbOOP/5x9XNaJ2vGLpjYrrGosNl0U/UWeNZZ9fvrBe6KIOkaHDRI/a4noyuig7Vdx1JW\ng4ZdrBV6NFNa2mYNT/Q5TnmPqbZMAP4jRAB1PrbfXm3PatmIK8elbcOG2ddu8Y3ZsGG6UfS+0TwW\nWkg9Xy67TE0GFseCC6r5m+Lq0tmp+vbeTsuLjY4O4I9/rPrXbBTVscSJjQ8/LKbc4cNrTYlJjByp\n4hxs9YmSJjbi5vkoiiItG83AJjbSXD1JD+pbbqntnExs8Qmm2Pj44+TVWydNUqNz0urgg8s5cBmN\nkrSP6cfP40bRb8JJbkdb2Y0ijxvFFzPOyyVGxcUS4EIWN4oPLlaytDLM+9cW72EyaJCaDMzG1VcD\ns2cnl7vAAio2rbfTdm4UG1nFRrRD8hUb5ptFCNL85ZpGTeoVxfWNFLBPIOQypDIrWdxUaQGirm4L\n7XpIqldcgKiObYljpZWqa0PE1S2OxRcHPvggeZ+zz05eKTR6XFwCiDU2AWe6UVwsG2usoYJL01Yr\nLnro67Rpbg9kc+K7G25Q9dx0U3vavGKjo0ONrJoyJdkV6lKevh4XXNC+tpMtLdA4seGbLupGMbfl\nrU8ZXpDKSMtbNlwI7UbZc8/qb+Z+/furyWGShnHlIa0jj9ZJCLdOpyii64IkYYoNsz033AD89KfZ\n6/DLX+ZfL8X2xp0mNmwzWLoQd43FlZmE69LsmkmT7NvNco86Cjj00Pg00fsmS4BodP80y4Zew0fv\nnyY0gOKteUOHAsssk57OHImy5561k+BFyTsaZd68qoVn8OD0/V0ezr/6VfpyD2kxG1lHL5n7+6SL\nqwNQG6uRVyTola7jBHdvp+3Fxj77AGeemW3fqF/d9iAYO7a6sJSZRpP0FpgXc+RCEqbYWGKJeEtM\n0fhYNrQgjLpRRo1Spsoorh3DhReqSHdz3QNfkmYQjetI4qYhT6PIoa96ynBXzNkUgWo8hotYT3Mr\n+QSIRi1EadeRjl1yvSYWWkjN+WGW0W4ce6xaYfiSS9RIsrhj43rMbPdAHFniQ3wowrKhKcKyobnl\nlvr+i1Rpe7ExYUJ1MSxfTjml+paU5EZpFmliI64zX3TRcHUy8REb5j4uQ199efbZ7Pvalpi31dH8\nnFVspMVsNJKzzlKT3tmGNUfRojpthEwSaW6UtOvc98FZdvKMPjn9dPW9Xz93a2ZSedpCkuQC1MRZ\noFZbza2sNIoWG2kxG650dOQbgdjutMht1xz6968N7EkaKZBE0noOeYhzo+jJkcyZOG3pTM44Iz2N\nL/qB6+JG0bhM+FMEPkLG1Y1ikjTtugu2ayzPccki3Pr2jZ/0LooWJNFpnl3LjTvvpmUjjjwP5bx5\nJWHOdDp7dn0goWbnncPXRRNnaT388OQhntHRR0kssUT1s3n+zSXc8xA9LiefnD0vKav9Uxax4XJ9\nt6PlLAsUGzH4xGykcd99+etjI64+iy0GvPMOcNhh6vtmm6n/SeuFrLde8fXLYtkA3CwboW/g6dOr\nc2Zo65YWb0C8yV+TVWzoY2WzbPi65E4+ObtVzxctNqJLcx9+uBoh4hLPYB5THeR67LHppntfMRZ1\nF4W6lv71L+Ddd9XnhRaqvgREiQu47OgApk4ttk5vvGHfft559mXqs3DMMVWRGuLYRs+zOQOvLV3S\ndSFl9b6i+yMsFBseuIgN2wUbym2R5EZZcslqXZZdVtV9+eXD1CMOnwBRk0bc9GllLL64Em2AelB+\n+GF1krS4/YsUGzbLhq/YOOkk4Nprs9XDlzjLxogRag2atBlmzbkcpFQrn0qpAhuLFBv//Gf9uiRF\njEaxMf/8tW/5WXCxJPhgxqrk5fjj7dv79gV++MPiyolSpBtlrbWq95XrS2TasGpih2IjAx0dSr1H\nOeeccFYMG65DX10I8QaiHxI+sQZbbZVs2WjW20dUMKbVI2vMRpGWjUYSJzZ8SAu6TdvP5drYdNPm\nTAzXSPbd1z1tnvspbmI6Ex28+/DD9t+ziJ+ixMZzz6n5bkyxccstwCOPJO83ZUq+iRB7KwxniSHp\nzVUINXTurLNq0x15ZGPqFq1PkQFvIWI2XPP8+GM1TPiLL+LTlMX/mRYgmtWyoQPMbG/becSG63F7\n/nl/txdQjNjIeh1HF7fzpSzXVF50O7KOvisSfS5WW03dC52d1VV99W933lk774hv3tHPcenirknt\n6jFjNlwCYE0Ys+EOLRsWLrpITQcexWeCokagl0Jfeunm1iMO3wfuwIHqxm/EQmx5O4C0mI28lg3z\n2OURG77HadVVgRVX9C8nLmYjiaImQ8p7LWR54JUZn+NRZF+29db27XGWze23z+Zi1nX+7neT55LR\n6T75JDk/fe0WfdyWXdY9v94AxYaFQw6x++VcZ4hsFLvtpmZ8TJqqPY08K+GmkfXtvixiLglbHU2z\nct6YDdv+jbBsZEXXbeGFs+cR50ZJG82Vd7TOPfcAv/51tn3LiM9xyHNd6H2vugr47DPg7ruLzT8O\n1/PtehxCBIi+9lo18LcMz4oyQLHhQZliCDR5/c8HHqj+h7ghbCs2uuDiu2/2DWw778cdB1x3nfqc\n1bKh3Sjm/nruijLHbHR2An/5i1qnyJXoMYyzaD3yCPDii+n5ZL0XF1ywKtibfT8XQaPbMGBA/UiW\nNMtfHooMEAWAo49WIt/FfZg2e6pm+eUbNxKsVWDMhgfNfsAVTeio6gceqPppfQg1D8Khh6q3ZHOB\nqqzY6tGvH7DHHmr117yWDZtYcZmGu5n4jkBwdaMsskjyarZFzEPSTjTLjeKab94yXc+36z247bZu\nk9GZK1JrQrp6241Mlg0hxCFCiGlCiC+EEJOEEOskpN1ECNET+esWQiweSbebEOLFSp5ThRDbZKlb\nKMwZRPX3VubTT4GJE8OWscQS7m8CjeCCC9R6FED+8xf3Fq7903GrRKahJ3gyhzwutZSKks8S+NdK\nHV7WmWPbQWw0Msh7qaWyX5++NGKejTjyBCvb6NPHLxi5la/HEHhbNoQQYwCcA+AAAE8AGAfgXiHE\nSlLKmTG7SQArAfj0mw1SfrO+pBBiAwA3AvgNgL8B2AvA7UKItaSUGd6Ni8O8YHbdVa16Gfd7K7HA\nAvbtZWpPMzsq13ykVBMvzZpV3Z6n3musYd/fN0o+SiuI40YMfY2j2cfn44+LyyvtOLz1VnFlZalD\nUcHZae0sWmz4Uqa+tAxk0dPjAFwupbxWSvkSgIMAzAGQtrbmDCnlB/ov8tthAO6WUp4rpXxZSnki\ngCkAfpmhfkGQUg11nTOn2TUJQ7M7WxM9FG2nnep/K0s9zY5k9dXjlwlvNq3U4eUdjVJEW5t1vBZc\nUP01mlBz9OiRKeZ09zrmKMkl5oKrFUiLjR13zFdeVrKK53bFS2wIIfoCGAngAb1NSikB3A9gVNKu\nAJ4WQrwrhLivYskwGVXJw+TelDwbgnkzdnRUl4vnBRSOPn2UD/Xww4vPu9FBamWhjNdr3DHsjZaN\nIinDtbneeuqYmkHiK6ygJtH63e/y5e3rRvn+9/OVlwRjNtzxdaMMAtAJYHpk+3QAw2P2eQ/AgQCe\nAjAfgP0BPCiEWFdK+XQlzZCYPAucXDcMoS4oHVvQW8kysVQj4VtLOJoZs1HGB8TSS8evq2Kj0W3w\nKW+vvfKVtcwy7uXpETJ5p4y34VKHEKtXtzLBR6NIKV8B8IqxaZIQYhiUO2afvPmPGzcOAwcOrNk2\nduxYAGPtO1Q44AD7xF2u6CLNlR2TeO01d/Nf0kQ1oSljZ2uSt356aOmSS+bLhx1Jfoq2bLQrb77p\nl75dj8fLL6v1ilzF5TbbALffrtwo662n1jdqJGU6D11dXejq6qrZNjtuGeJA+IqNmQC6AQyObB8M\n4H2PfJ4AsKHx/f2seY4fPx4j9FSaBmmWgcsvT8s5mcUXB/77X/fZO30WQWvGRdpbHpoDBgB33QVs\nvHH9b3//u/v5LFNHkkSZ61l0sG6Z29pI2vU46JWAdTCty6ReOu5r2DD110jKdB7Gjh1beQmvMmXK\nFIx0fVsuAK+YDSnlXACTAXwzoFEIISrfH/PIak0o94pmoplnhS0q25uKfijZHkI+Jj1SHrbZxj4a\nZ/PNgeFxzsAIrXbeyy4m9eRyWaDYqKUM82yEpEx1ZsyGO1lGo5wLYH8hxE+EECsDuAzAAAATAEAI\ncaYQ4hqdWAhxuBBiRyHEMCHEd4UQ5wHYDMBFRp7nA9haCHGkEGK4EOJkqEBUM01T2Hln4PPPlbAg\nzUfHcpTlRi77Q7wsx8mGvqf+9S9gyy2r2xkgms5JJ9VvK9vaTaEog7jkPBv+eMdsSClvFkIMAnAq\nlKvjaQBbSSlnVJIMAWDaAfpBzcuxJNQQ2WcAjJZSPmTkOVEIsSeA0yt//wGwU7Pn2NAMGNDsGjSO\nst8gv/sdMHhw7ZC6ZtJqD6kyseCC9uPHANF0Tj4ZOOUU+2+NakOzrv0ynKPVVlP/zcn3ojCIvJZM\nAaJSyksAXBLz276R72cDONuWNpLuzwD+nKU+JD+tckMsvDBwwgnNrgUpE73RspFEo90ozRr90kzR\nsfbaavG56JowJgwir4ULsRHSC2ilDm+xxfzSl+HhUyba/TiUpX1JQgMoTz3LAhdiIyQHZX+It1qH\nd+utwAbRKf9SoNioxec43HQTcM016enKRKuc77LXr9FQbJAaeIO0J2UXRZpdd/XfpzfGbNjIUvct\nt6wNzm0FWuUctUo9GwXdKITkoFUe4u0MYzZqafeHXKtYNkgtFBuEkJamt1s29Gi5Rg997c2jUYg/\nFBuE5KDsb8S9oWPu7W+6775b+73Ma6MUWV6rnO+y9xGNgjEbBABviHannc+vfuiMjs5B7MFPfwq8\n8ALwox8VUqWGElkaqmUewllptfa1873nA8UGIW1Mq3XMWZkxA1hkkez7zz8/cPHFxdUnFOPGqWn1\nbWyxBfDII+1/zlvNsqH5xS+AVVdtdi2aB8UGITnI84BrJO3+djVoULNr0BjOPTf+t+OPBw491H11\n6aJo9LXVaiJDc4l1GszeA8UGqaFVb+RmcPPNwKhRza4FIYqOjsaK32b1Fa1q2ejtUGwQAO3/5huC\n3XZrdg1Ib2aPPdxXKQ4BR6MQHyg2CGlj2DG3L11dza6BolmjUXbcsbHlknxQbBDSC6DlirQTX3wB\nzDdfs2vhBu89BcUGqYFvwu0FzydpR/r3b3YN3KHYUHBSL0LaGHZ0hJAy0LaWjW22YQAfIYSEgkKW\n+NC2YuOuu5pdg9aCHUd7QjcKCQ2vMeIC3SikBnYc7QnFJCGkmVBsENLGUDwSQsoAxQYhhBASCFoV\nFRQbBABvCEKIH9ttp/5zyv5k2Lcq2jZAlGSDZvf2hB0eKZqVV+Z1RdyhZYOQNobikRBSBig2COkF\n8A2UENJMKDYIAD6M2hVaNgghZYBigxBCCAlE377NrkE5oNgghBBCAnDllcBTTzW7FuWAo1FIDTS7\ntyd0kxHSePbbr9k1KA+0bBAAfBi1K4ssov4PH97cehBCeje0bBDSxiy7LDB1KvC97zW7JoSQ3gzF\nBgFA90k7s/rqza4BIaS3QzcKAUA3CiGEkHBQbBAAQL9+6v+3vtXcehBCCGk/6EYhAIAttgAuuAA4\n8MBm14QQQki7QbFBAAAdHcChhza7FoQQQtoRulEIIYQQEhSKDUIIIYQEhWKDEEIIIUGh2CCEEEJI\nUCg2CCGEEBIUig1CCCGEBIVigxBCCCFBodgghBBCSFAoNgghhBASFIoNQgghhASFYoMQQgghQaHY\nIIQQQkhQKDYIIYQQEhSKDUIIIYQEhWKDEEIIIUGh2CgJXV1dza5CobA95aWd2gKwPWWmndoCtF97\nGkkmsSGEOEQIMU0I8YUQYpIQYh3H/TYUQswVQkyx/HaEEOIlIcQcIcSbQohzhRDzZalfK9JuFzHb\nU17aqS0A21Nm2qktQPu1p5F4iw0hxBgA5wA4CcBaAKYCuFcIMShlv4EArgFwv+W3PQGcWclzZQA/\nA7A7gNN960cIIYSQcpHFsjEOwOVSymullC8BOAjAHCiBkMRlAG4AMMny2ygAj0gpb5JSvimlvB/A\nnwCsm6F+hBBCCCkRXmJDCNEXwEgAD+htUkoJZa0YlbDfvgCWA3BKTJLHAIzU7hghxPIAtgXwN5/6\nEUIIIaR89PFMPwhAJ4Dpke3TAQy37SCEWBHAGQA2klL2CCHq0kgpuypumEeEStAJ4DIp5e8T6tIf\nAF588UXPJpST2bNnY8qUulCWloXtKS/t1BaA7Skz7dQWoL3aYzw7+zekQCml8x+AJQD0AFgvsv33\nACZa0ncAeALAAca2kwFMiaTbFMB7APYF8F0AOwH4L4DjE+qyJwDJP/7xj3/84x//Mv/t6aMDsv6J\nyoPbiYobZQ6AXaWUdxjbJwAYKKX8YST9QAAfAZgHQJs0Oiqf5wHYUkr5oBDiIQCTpJRHG/vuBRUb\nskBMXRYDsBWANwB86dwIQgghhPQHMBTAvVLKD0MX5uVGkVLOFUJMBjAawB0AUHF7jAZwgWWXTwB8\nL7LtEACbAdgVSigAwAAo8WHSo/OXFkVUOTg3+tSfEEIIId/wWKMK8o3ZAIBzAUyoiI4noEanDAAw\nAQCEEGcCWFJKuU9FJLxg7iyE+ADAl1JKM9jiTgDjhBBTATwOYEUApwK4wyY0CCGEENI6eIsNKeXN\nlWDOUwEMBvA0gK2klDMqSYYAWNoz29OgLBmnAfgOgBlQlpPjfetHCCGEkHLhFbNBCCGEEOIL10Yh\nhBBCSFAoNgghhBASlJYUG1kXgmskQojfCiGeEEJ8IoSYLoS4TQixkiXdqUKIdysL0P1dCLFC5Pf5\nhBAXCyFmCiE+FULcKoRYvHEtqUcIcYwQokcIcW5ke8u0RQixpBDiukpd5gghpgohRkTStER7hBAd\nQojThBCvV+r6qhCiLt6prO0RQmwshLhDCPFO5braMUTdhRCLCCFuEELMFkJ8JIT4oxBi/ka1RQjR\nRwjxeyHEM0KIzypprhFCLFHGtqS1x5L2skqaw1q5PUKIVYQQfxVCfFw5T48LIZYqW3vS2iKEmF8I\ncZEQ4q3KffO8EOLASJrGtaURk3kU+QdgDNS8Gj+BWrTtcgCzAAxqdt0i9bwLwI8BrAJgNQD/BzXU\n91tGmt9U6r491BDh2wG8BqCfkebSyn6bQC189xiAh5vYrnUAvA7g3wDObcW2AFgYwDQAf4Safn9Z\nAJsDWK5F23MsgA8AbA1gGQC7QA07/2UrtKdS71OhJvPrBrBj5PdC6g7gbgBTAKwNYAMArwC4vlFt\nAbAQgHuhhv2vCLX20yQAT0TyKEVbXM6Nke6HUH3CWwAOa9X2ABgGYCbUwqCrQy2zsT2M50tZ2uPQ\nlisq5W4M1S/sB2AugO2b0ZZCT2Qj/io35/nGdwHgbQBHN7tuKfUeBDXiZiNj27sAxhnfFwLwBYDd\nje9fAfihkWZ4JZ91m9CGBQC8DOAHAP6JWrHRMm0BcBaAf6WkaaX23Angysi2WwFc22rtqZQX7TRz\n1x1K9PcAWMtIsxXU/D5DGtUWS5q1oR4US5W5LUntgRpB+GalXtNgiI1Waw+ALgDXJOxTyvbEtOVZ\nAMdFtj0F4NRmtKWl3Cgi40JwJWFhqKlhZwGAEGI5qGHCZls+gZpnRLdlbajhyWaal6Fu7Ga092IA\nd0op/2FubMG27ADgKSHEzUK5uKYIIfbTP7Zgex4DMFqodYgghFgDwIZQ1rVWbM83FFj39QF8JKX8\nt5H9/VD35Hqh6u+A7hc+rnwfiRZqixBCALgWwB9k7dxJmpZpT6Ut2wH4jxDinkrfMEkIsZORrGXa\nA9Uv7CiEWBIAhBCbQVnU7q383tC2tJTYQPJCcEMaXx03KhfxeQAekVLqSc6GQJ2wpLYMBvB1pXON\nS9MQhBB7AFgTwG8tP7dUWwAsD+AXUFaaLaFMiRcIIX5c+b3V2nMWgJsAvCSE+BrAZADnSSn/VPm9\n1dpjUlTdh0C5mr5BStkNJf6b0j4hxHxQ5+5GKeVnlc1D0FptOQaqvhfF/N5K7Vkcynr7GyihvgWA\n2wD8RQixcSVNK7XnUAAvAni70i/cBeAQKeWjRj0b1pYsM4gSfy4BsCrU22bLUQmOOg/A5lLKuc2u\nTwF0QPnJT6h8nyqE+B6AgwBc17xqZWYM1MKEe0DN2LsmgPOFEO9KKVuxPW2PEKIPgFughNTBTa5O\nJoQQIwEcBuXrbwf0y/ftUkq9/MYzQogNoPqGh5tTrcwcBmV92B7KWvF9AJdU+oV/JO4ZgFazbMyE\n8m8OjmwfDOD9xlcnHSHERQC2BbCplPI946f3oeJNktryPoB+QoiFEtI0gpEAvg1gihBirhBiLlRA\n0eEVxTwdrdMWQK0wHDX5vggVRAW01rkBgD8AOEtKeYuU8nkp5Q0AxqNqhWq19pgUVff3od5cv0EI\n0QlgUTS4fYbQWBpqMcrPjJ9bqS0bQfULbxn9wrIAzhVCvG7UtVXaMxMqFiGtbyh9e4QQ/QGcDuBI\nKeVdUsrnpJSXQFlAjzLq2bC2tJTYqLxV64XgANQsBNewBWVcqQiNnQBsJqV80/xNSjkN6mSZbVkI\nSonqtkyGuvjNNMOhLvyJQStfy/1QI2rWBLBG5e8pANcDWENK+Tpapy0A8ChUIJTJcAD/BVru3ABq\nbaLuyLYeVO7vFmzPNxRY94kAFhZCmG/ho6GEzOOh6h/FEBrLAxgtpfwokqRl2gIVq7E6qn3CGlDB\nvH+ACiIEWqg9lefLk6jvG1ZCpW9A67Snb+Uv2i90o/rcb2xbQkTGhvwDsDvUMvfm0NcPAXy72XWL\n1PMSAB9BDTsabPz1N9IcXan7DlAP89sB/Ae1Q/ougYrw3hTKwvAomjj01ahXdDRKy7QFKqDwK6g3\n/2FQLohPAezRou25GspMui3Um+UPofysZ7RCewDMD/WgWhNKJB1R+b50kXWH8lk/BTV8e0OomJ3r\nGtUWKLf1X6EeXKuhtl/oW7a2uJwbS/qa0Sit1h4AO0NNrbAfVN/wSwBfAxhVtvY4tOWfAJ6BskIP\nBfBTqGfnAc1oS9BOJNQflI/zDajhbxMBrN3sOlnq2AOlIqN/P4mkOxnqbWAOVJTwCpHf5wNwIZSJ\n71Oot6LFS9C+f8AQG63WFqgH8zOVuj4P4GeWNC3Rnkqnc26l0/gc6kF8CoA+rdCeSmdou1/+t8i6\nQ438uB7AbKgXgSsBDGhUW6CEYPQ3/f37ZWuL67mJpH8d9WKjpdoD9VB+pXIvTYExL0WZ2pPWFij3\nx1VQc598DhXPdXiz2sKF2AghhBASlJaK2SCEEEJI60GxQQghhJCgUGwQQgghJCgUG4QQQggJCsUG\nIYQQQoJCsUEIIYSQoFBsEEIIISQoFBuEEEIICQrFBiGEEEKCQrFBCCGEkKBQbBBCCCEkKP8P564Y\n2WEGF84AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2335a50cc50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 模型测试\n",
    "training_losses = train_network(5,num_steps)\n",
    "plt.plot(training_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 模型改进\n",
    "'''\n",
    "#定义rnn_cell的权重参数，\n",
    "with tf.variable_scope('rnn_cell'):\n",
    "    W = tf.get_variable('W', [num_classes + state_size, state_size])\n",
    "    b = tf.get_variable('b', [state_size], initializer=tf.constant_initializer(0.0))\n",
    "#使之定义为reuse模式，循环使用，保持参数相同\n",
    "def rnn_cell(rnn_input, state):\n",
    "    with tf.variable_scope('rnn_cell', reuse=True):\n",
    "        W = tf.get_variable('W', [num_classes + state_size, state_size])\n",
    "        b = tf.get_variable('b', [state_size], initializer=tf.constant_initializer(0.0))\n",
    "    #定义rnn_cell具体的操作，这里使用的是最简单的rnn，不是LSTM\n",
    "    return tf.tanh(tf.matmul(tf.concat([rnn_input, state], 1), W) + b)\n",
    "\n",
    "state = init_state\n",
    "rnn_outputs = []\n",
    "#循环num_steps次，即将一个序列输入RNN模型\n",
    "for rnn_input in rnn_inputs:\n",
    "    state = rnn_cell(rnn_input, state)\n",
    "    rnn_outputs.append(state)\n",
    "final_state = rnn_outputs[-1]\n",
    "'''\n",
    "#----------------------上面是原始代码，定义了rnn_cell，然后使用循环的方式对其进行复用，简化之后我们可以直接调用BasicRNNCell和static_rnn两个函数实现------------------------\n",
    "\n",
    "\n",
    "cell = tf.contrib.rnn.BasicRNNCell(state_size)\n",
    "rnn_outputs, final_state = tf.contrib.rnn.static_rnn(cell, rnn_inputs, initial_state=init_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable rnn/basic_rnn_cell/kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-89936d884cb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mcell\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBasicRNNCell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[1;31m#使用dynamic_rnn函数，动态构建RNN模型\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mrnn_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdynamic_rnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrnn_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minit_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'softmax'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36mdynamic_rnn\u001b[0;34m(cell, inputs, sequence_length, initial_state, dtype, parallel_iterations, swap_memory, time_major, scope)\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0mswap_memory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mswap_memory\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0msequence_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m         dtype=dtype)\n\u001b[0m\u001b[1;32m    615\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m     \u001b[1;31m# Outputs of _dynamic_rnn_loop are always shaped [time, batch, depth].\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36m_dynamic_rnn_loop\u001b[0;34m(cell, inputs, initial_state, parallel_iterations, swap_memory, sequence_length, dtype)\u001b[0m\n\u001b[1;32m    775\u001b[0m       \u001b[0mloop_vars\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_ta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m       \u001b[0mparallel_iterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparallel_iterations\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m       swap_memory=swap_memory)\n\u001b[0m\u001b[1;32m    778\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m   \u001b[1;31m# Unpack final output if not using output tuples.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name)\u001b[0m\n\u001b[1;32m   2814\u001b[0m     \u001b[0mloop_context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWhileContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparallel_iterations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mback_prop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mswap_memory\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=redefined-outer-name\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2815\u001b[0m     \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_to_collection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWHILE_CONTEXT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloop_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2816\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloop_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBuildLoop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcond\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloop_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape_invariants\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2817\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36mBuildLoop\u001b[0;34m(self, pred, body, loop_vars, shape_invariants)\u001b[0m\n\u001b[1;32m   2638\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEnter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2639\u001b[0m       original_body_result, exit_vars = self._BuildLoop(\n\u001b[0;32m-> 2640\u001b[0;31m           pred, body, original_loop_vars, loop_vars, shape_invariants)\n\u001b[0m\u001b[1;32m   2641\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2642\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36m_BuildLoop\u001b[0;34m(self, pred, body, original_loop_vars, loop_vars, shape_invariants)\u001b[0m\n\u001b[1;32m   2588\u001b[0m         \u001b[0mstructure\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moriginal_loop_vars\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2589\u001b[0m         flat_sequence=vars_for_body_with_tensor_arrays)\n\u001b[0;32m-> 2590\u001b[0;31m     \u001b[0mbody_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mpacked_vars_for_body\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2591\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbody_result\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2592\u001b[0m       \u001b[0mbody_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbody_result\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36m_time_step\u001b[0;34m(time, output_ta_t, state)\u001b[0m\n\u001b[1;32m    760\u001b[0m           skip_conditionals=True)\n\u001b[1;32m    761\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m       \u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_cell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[1;31m# Pack state if using state tuples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[0minput_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflat_sequence\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m     \u001b[0mcall_cell\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    749\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msequence_length\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, state, scope)\u001b[0m\n\u001b[1;32m    181\u001b[0m       with vs.variable_scope(vs.get_variable_scope(),\n\u001b[1;32m    182\u001b[0m                              custom_getter=self._rnn_get_variable):\n\u001b[0;32m--> 183\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRNNCell\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_rnn_get_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgetter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0min_graph_mode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_assert_input_compatibility\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, state)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[1;34m\"\"\"Most basic RNN: output = new_state = act(W * input + U * state + B).\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_linear\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_linear\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Linear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_units\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_linear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, output_size, build_bias, bias_initializer, kernel_initializer)\u001b[0m\n\u001b[1;32m   1169\u001b[0m           \u001b[0m_WEIGHTS_VARIABLE_NAME\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtotal_arg_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m           \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1171\u001b[0;31m           initializer=kernel_initializer)\n\u001b[0m\u001b[1;32m   1172\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mbuild_bias\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mvs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouter_scope\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0minner_scope\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1201\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m       \u001b[0muse_resource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1203\u001b[0;31m       constraint=constraint)\n\u001b[0m\u001b[1;32m   1204\u001b[0m get_variable_or_local_docstring = (\n\u001b[1;32m   1205\u001b[0m     \"\"\"%s\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1090\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m           \u001b[0muse_resource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1092\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m   1093\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1094\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m    415\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;34m\"constraint\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mestimator_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0mcustom_getter_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"constraint\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconstraint\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mcustom_getter_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m       return _true_getter(\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\u001b[0m in \u001b[0;36m_rnn_get_variable\u001b[0;34m(self, getter, *args, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_rnn_get_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgetter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m     \u001b[0mvariable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0min_graph_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m       trainable = (variable in tf_variables.trainable_variables() or\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    392\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m           use_resource=use_resource, constraint=constraint)\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    740\u001b[0m                          \u001b[1;34m\"reuse=tf.AUTO_REUSE in VarScope? \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[0;32m--> 742\u001b[0;31m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    743\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable rnn/basic_rnn_cell/kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n"
     ]
    }
   ],
   "source": [
    "# 基于上面的修改\n",
    "x = tf.placeholder(tf.int32, [batch_size, num_steps], name='input_placeholder')\n",
    "y = tf.placeholder(tf.int32, [batch_size, num_steps], name='labels_placeholder')\n",
    "init_state = tf.zeros([batch_size, state_size])\n",
    "\n",
    "rnn_inputs = tf.one_hot(x, num_classes)\n",
    "#注意这里去掉了这行代码，因为我们不需要将其表示成列表的形式在使用循环去做。\n",
    "#rnn_inputs = tf.unstack(x_one_hot, axis=1)\n",
    "\n",
    "cell = tf.contrib.rnn.BasicRNNCell(state_size)\n",
    "#使用dynamic_rnn函数，动态构建RNN模型\n",
    "rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, initial_state=init_state)\n",
    "\n",
    "with tf.variable_scope('softmax'):\n",
    "    W = tf.get_variable('W', [state_size, num_classes])\n",
    "    b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "    logits = tf.reshape(\n",
    "            tf.matmul(tf.reshape(rnn_outputs, [-1, state_size]), W) + b,\n",
    "            [batch_size, num_steps, num_classes])\n",
    "    predictions = tf.nn.softmax(logits)\n",
    "\n",
    "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "train_step = tf.train.AdagradOptimizer(learning_rate).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
